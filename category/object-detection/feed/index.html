<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Object Detection &#8211; Machine Learning Space</title>
	<atom:link href="https://machinelearningspace.com/category/object-detection/feed/" rel="self" type="application/rss+xml" />
	<link>https://machinelearningspace.com</link>
	<description>A space for learning machine learning</description>
	<lastBuildDate>Sat, 29 Oct 2022 22:36:55 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.1.1</generator>
	<item>
		<title>Object Tracking: 2-D Object Tracking using Kalman Filter in Python</title>
		<link>https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=2d-object-tracking-using-kalman-filter</link>
					<comments>https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Wed, 26 Feb 2020 11:41:36 +0000</pubDate>
				<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Object Tracking]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[2D Kalman filter]]></category>
		<category><![CDATA[2d kalman filter python]]></category>
		<category><![CDATA[2D Object Tracking]]></category>
		<category><![CDATA[a kalman filter tutorial for undergraduate students]]></category>
		<category><![CDATA[a kalman filtering tutorial for undergraduate students]]></category>
		<category><![CDATA[how a kalman filter works]]></category>
		<category><![CDATA[implement a kalman filter]]></category>
		<category><![CDATA[implementing a kalman filter in python]]></category>
		<category><![CDATA[Kalman Filter 2D]]></category>
		<category><![CDATA[kalman filter implementation]]></category>
		<category><![CDATA[kalman filter in image processing]]></category>
		<category><![CDATA[Kalman Filter in Python]]></category>
		<category><![CDATA[Kalman Filter Python]]></category>
		<category><![CDATA[kalman filter python opencv]]></category>
		<category><![CDATA[kalman filter regression python]]></category>
		<category><![CDATA[kalman filter tracking]]></category>
		<category><![CDATA[KF]]></category>
		<category><![CDATA[Learn how to implement Kalman filter in Python]]></category>
		<category><![CDATA[Python]]></category>
		<category><![CDATA[two dimensional kalman filter python]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=1887</guid>

					<description><![CDATA[<p>In this tutorial, we're going to continue our discussion about the object tracking using Kalman Filter. Specifically in this part, we're going to discover 2-D object tracking.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/">Object Tracking: 2-D Object Tracking using Kalman Filter in Python</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<div class="responsive-embed-container">
<figure><iframe src="https://www.youtube.com/embed/niFzS2t1F-8" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" width="560" height="315" frameborder="0"></iframe></figure></div>



<p>In the <a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/">previous tutorial</a>, we&#8217;ve discussed the implementation of the Kalman filter in Python for tracking a moving object in 1-D direction. Now, we&#8217;re going to continue our discussion on object tracking, specifically in this part, we&#8217;re going to discover 2-D object tracking using the Kalman filter.</p>



<p>As I mentioned already in the previous tutorial, the 1-D Kalman filter concept is a prerequisite for the tutorials related to the object tracking algorithm that you will find in my blogs. So, I suppose that you&#8217;ve read it carefully. Otherwise, I strongly recommend reading it first before continuing this one.  So that you can follow this guide easily.</p>



<h3>State transition matrix A and control matrix B </h3>



<p>Remember, in 1-D Kalman Filter, we consider that both state and velocity are in one-direction only. Based on Kinematic  equation, the relation between the position <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and velocity <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-770d10fb0f6590bea84ce31bd1af6d0c_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#100;&#111;&#116;&#32;&#120;" title="Rendered by QuickLaTeX.com" height="12" width="10" style="vertical-align: 0px;"/> can be written as the following: </p>



<p> <a name="id3677551405"></a><p class="ql-center-displayed-equation" style="line-height: 44px;"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-66433c7ca819bd7b8f4dc9c8f3367476_l3.png" height="44" width="346" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#32;&#61;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#120;&#95;&#107;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#120;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#92;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#32;&#116;&#43;&#32;&#49;&#47;&#50;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#50;&#125;&#125;&#92;&#92;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>Then we can write eq.(<a href="#id3677551405">1</a>)  in the form of matrix multiplication as follows:</p>



<p>  <a name="id286783683"></a><p class="ql-center-displayed-equation" style="line-height: 44px;"><span class="ql-right-eqno"> (2) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-e6e2d565a04cf7a2595913099abbbba6_l3.png" height="44" width="358" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#32;&#61;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#95;&#107;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#120;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#92;&#92;&#48;&#32;&#38;&#32;&#49;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#43;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#92;&#92;&#32;&#32;&#32;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>Now, we&#8217;re going to focus on 2-D Kalman Filter. The equations of 2-D  Kalman Filter whose position and velocity must be considered in 2-dimensional direction, the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>&#8211; directions, can be created by modifying the 1-D Kalman Filter equations. Meaning that instead of considering only for the position and velocity in one direction, let&#8217;s say the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>-direction, we need to take into account the position and velocity in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-direction as well. </p>



<p>The state in time <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d42bc2203d6f76ad01b27ac9acc0bee1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#107;" title="Rendered by QuickLaTeX.com" height="13" width="9" style="vertical-align: 0px;"/> can be predicted by the previous state in time <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7dfca2445cd362ac42fb9032c9cf2367_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#107;&#45;&#49;" title="Rendered by QuickLaTeX.com" height="14" width="39" style="vertical-align: -1px;"/>. Let <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/> be the positions in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-directions, respectively, and also let <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-02f1f273123c3399663d27f86372ec28_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#100;&#111;&#116;&#32;&#32;&#120;" title="Rendered by QuickLaTeX.com" height="12" width="10" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-63e253a73a05b60ef47437af4e7cb120_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#100;&#111;&#116;&#32;&#32;&#121;" title="Rendered by QuickLaTeX.com" height="16" width="9" style="vertical-align: -4px;"/> be the velocities in <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-directions, respectively.  Then the 2-D Kinematic equation for state <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2c0c9cc1d15c8bbf6f8800e7c7d50482_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="18" style="vertical-align: -3px;"/> can be written as:</p>



<p><a name="id3871371663"></a><p class="ql-center-displayed-equation" style="line-height: 88px;"><span class="ql-right-eqno"> (3) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2b75345ed0c80f0debc89d3b0698cffe_l3.png" height="88" width="355" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#95;&#107;&#92;&#92;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#107;&#92;&#92;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#61;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#32;&#116;&#43;&#32;&#49;&#47;&#50;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#50;&#125;&#125;&#92;&#92;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#32;&#116;&#43;&#32;&#49;&#47;&#50;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#50;&#125;&#125;&#92;&#92;&#32;&#32;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#43;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>We can write eq.(<a href="#id3871371663">3</a>) into the form of matrix multiplication as follows:</p>



<p> <a name="id3826491201"></a><p class="ql-center-displayed-equation" style="line-height: 88px;"><span class="ql-right-eqno"> (4) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-17a06922ad76e9d65285cdbe93182da3_l3.png" height="88" width="525" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#95;&#107;&#92;&#92;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#107;&#92;&#92;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#38;&#32;&#48;&#32;&#92;&#92;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#32;&#92;&#92;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#32;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#43;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#92;&#92;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#92;&#100;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>The eq.(<a href="#id3826491201">4</a>) can be simplified as follows: </p>



<p><a name="id2248339877"></a><p class="ql-center-displayed-equation" style="line-height: 88px;"><span class="ql-right-eqno"> (5) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-813dddc7a28665369363f703102255c0_l3.png" height="88" width="420" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#38;&#32;&#48;&#92;&#92;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#43;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#92;&#92;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#125;&#95;&#123;&#107;&#45;&#49;&#125;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>Where <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2c0c9cc1d15c8bbf6f8800e7c7d50482_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="18" style="vertical-align: -3px;"/> is the current state, <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-648030bf53c2fbd3a6cf5095017b4653_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#123;&#107;&#45;&#49;&#125;" title="Rendered by QuickLaTeX.com" height="12" width="35" style="vertical-align: -4px;"/> is the previous state, and  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-aa6e9bd9f600bd8dac43309af9a22b2c_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#95;&#32;&#123;&#107;&#45;&#49;&#125;" title="Rendered by QuickLaTeX.com" height="12" width="34" style="vertical-align: -4px;"/> is a vector of the previous acceleration in <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-directions.</p>



<p>So, we the matrices A and B as follows:</p>



<p> <a name="id2300232522"></a><p class="ql-center-displayed-equation" style="line-height: 86px;"><span class="ql-right-eqno"> (6) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-17c37121fa29fffcde0437488177aa62_l3.png" height="86" width="167" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#65;&#125;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#38;&#32;&#48;&#92;&#92;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#49;&#32;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p><a name="id841436821"></a><p class="ql-center-displayed-equation" style="line-height: 88px;"><span class="ql-right-eqno"> (7) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-64478aa0536b92fe7893073ffc26aeba_l3.png" height="88" width="180" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#66;&#125;&#32;&#61;&#32;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#40;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#41;&#94;&#50;&#32;&#92;&#92;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#32;&#48;&#32;&#38;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#125;&#32;&#92;&#92;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>Notice: Pay attention to the notation: <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2c0c9cc1d15c8bbf6f8800e7c7d50482_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="18" style="vertical-align: -3px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-9f7a125c094d5cd793eb4a83bb8318ce_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="17" style="vertical-align: -3px;"/>,  they are  two different things. Where the  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2c0c9cc1d15c8bbf6f8800e7c7d50482_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="18" style="vertical-align: -3px;"/> (&#8220;x&#8221; in bold letter) is  the state at time <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d42bc2203d6f76ad01b27ac9acc0bee1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#107;" title="Rendered by QuickLaTeX.com" height="13" width="9" style="vertical-align: 0px;"/> that composes of the positions <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-9f7a125c094d5cd793eb4a83bb8318ce_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="11" width="17" style="vertical-align: -3px;"/> and  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-76fc8864fc0de59802f4fc7cacf86ce6_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="12" width="16" style="vertical-align: -4px;"/>  and the velocities  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-db308c5f7a921695942561b859d27f6f_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="15" width="17" style="vertical-align: -3px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6679a2f793dd4f66753e057644d467a1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="16" width="16" style="vertical-align: -4px;"/> at time <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d42bc2203d6f76ad01b27ac9acc0bee1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#107;" title="Rendered by QuickLaTeX.com" height="13" width="9" style="vertical-align: 0px;"/>. </p>



<h3>Transformation matrix H </h3>



<p>The measurement model for  2-D Kalman filter is the same as the 1-D Kalman filter model:</p>



<p><a name="id3920594246"></a><p class="ql-center-displayed-equation" style="line-height: 15px;"><span class="ql-right-eqno"> (8) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-272479a3dfe449c9597dca544da0dba4_l3.png" height="15" width="116" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#36;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#122;&#125;&#95;&#107;&#32;&#61;&#32;&#72;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#95;&#107;&#43;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#118;&#125;&#95;&#107;&#36;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>In deriving the observation model, we assume that we&#8217;re only measuring the position but not the velocity.  Whereas, there are cases where the velocity must be taken into account to incorporate Doppler&#8217;s effect, for example, airplane and satellite tracking systems.</p>



<p>We&#8217;re now considering the particular case where the velocity will not be considered in the measurement process. So, we can write the measurement model as follows:</p>



<p>  <a name="id1185811560"></a><p class="ql-center-displayed-equation" style="line-height: 86px;"><span class="ql-right-eqno"> (9) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-1e3dc2df8b7bbcce1a85430e583dc4b1_l3.png" height="86" width="228" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#122;&#125;&#95;&#107;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#120;&#95;&#107;&#92;&#92;&#32;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#95;&#107;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#95;&#107;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#43;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#118;&#125;&#95;&#123;&#107;&#125;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>So, we have the transformation matrix <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a7cedbc00aa5531f310166df85e3a9bb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#72;" title="Rendered by QuickLaTeX.com" height="12" width="16" style="vertical-align: 0px;"/> as:</p>



<p>  <a name="id1002015878"></a><p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno"> (10) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-3ff923d2d679dda1876441493c12f458_l3.png" height="43" width="138" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#72;&#125;&#32;&#61;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#92;&#92;&#32;&#48;&#32;&#38;&#32;&#49;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#48;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>  </p>



<h3>Process noise covariance matrix Q</h3>



<p>The process noise covariance matrix <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-dd440a7af28975f52f03607a49307d46_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#81;" title="Rendered by QuickLaTeX.com" height="16" width="14" style="vertical-align: -4px;"/> for 2-D Kalman filter can be written as:</p>



<p> <a name="id4182536129"></a><p class="ql-center-displayed-equation" style="line-height: 108px;"><span class="ql-right-eqno"> (11) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fa6e54fbdcf5eb6ccf58c615404e8736_l3.png" height="108" width="304" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#81;&#125;&#32;&#61;&#32;&#32;&#32;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#120;&#32;&#92;&#92;&#32;&#121;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#32;&#92;&#92;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#32;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#97;&#114;&#114;&#97;&#121;&#125;&#123;&#99;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#32;&#32;&#92;&#109;&#115;&#112;&#97;&#99;&#101;&#123;&#50;&#55;&#109;&#117;&#125;&#32;&#38;&#32;&#32;&#121;&#32;&#32;&#32;&#92;&#109;&#115;&#112;&#97;&#99;&#101;&#123;&#50;&#55;&#109;&#117;&#125;&#32;&#32;&#32;&#32;&#38;&#32;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#120;&#32;&#32;&#32;&#92;&#109;&#115;&#112;&#97;&#99;&#101;&#123;&#50;&#55;&#109;&#117;&#125;&#32;&#32;&#38;&#32;&#32;&#92;&#100;&#111;&#116;&#32;&#32;&#121;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#92;&#92;&#108;&#101;&#102;&#116;&#91;&#92;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#120;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#120;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#32;&#123;&#92;&#100;&#111;&#116;&#32;&#120;&#125;&#32;&#32;&#32;&#38;&#32;&#48;&#32;&#92;&#92;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#121;&#32;&#32;&#32;&#32;&#38;&#32;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#121;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#32;&#123;&#92;&#100;&#111;&#116;&#32;&#121;&#125;&#32;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#32;&#123;&#92;&#100;&#111;&#116;&#32;&#120;&#125;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#120;&#32;&#32;&#32;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#123;&#92;&#100;&#111;&#116;&#32;&#120;&#125;&#32;&#32;&#38;&#32;&#48;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#48;&#32;&#38;&#32;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#32;&#123;&#92;&#100;&#111;&#116;&#32;&#121;&#125;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#121;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#123;&#92;&#100;&#111;&#116;&#32;&#121;&#125;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#32;&#92;&#101;&#110;&#100;&#123;&#97;&#114;&#114;&#97;&#121;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>    </p>



<p>By using the same philosophy as what we&#8217;ve discussed earlier in my <a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/#noise-covariance">previous tutorial</a> for 1-D Kalman filter, we can rewrite the process noise covariance matrix for 2-D Kalman filter as: </p>



<p>  <a name="id1142397845"></a><p class="ql-center-displayed-equation" style="line-height: 97px;"><span class="ql-right-eqno"> (12) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-f1f8cc6adb0ae8be066f2bef3b6701af_l3.png" height="97" width="240" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#81;&#125;&#32;&#61;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#52;&#125;&#125;&#123;&#52;&#125;&#32;&#32;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#51;&#125;&#125;&#123;&#50;&#125;&#32;&#32;&#32;&#32;&#38;&#32;&#48;&#92;&#92;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#52;&#125;&#125;&#123;&#52;&#125;&#32;&#32;&#32;&#38;&#32;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#51;&#125;&#125;&#123;&#50;&#125;&#32;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#51;&#125;&#125;&#123;&#50;&#125;&#32;&#32;&#38;&#32;&#48;&#32;&#32;&#38;&#32;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#50;&#125;&#32;&#32;&#32;&#32;&#38;&#32;&#48;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#48;&#32;&#38;&#32;&#32;&#32;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#51;&#125;&#125;&#123;&#50;&#125;&#32;&#32;&#32;&#32;&#38;&#32;&#48;&#32;&#38;&#32;&#32;&#32;&#92;&#68;&#101;&#108;&#116;&#97;&#123;&#116;&#94;&#50;&#125;&#32;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#98;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#97;&#32;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>       </p>



<p>Where the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-08e8d9f62f13851b3b5512a55ed5b710_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#115;&#105;&#103;&#109;&#97;&#95;&#97;" title="Rendered by QuickLaTeX.com" height="11" width="17" style="vertical-align: -3px;"/> is the magnitude of the standard deviation of the acceleration that is basically the process noise effecting on the process noise covariance matrix. </p>



<h3>Measurement noise covariance matrix R</h3>



<p>In 2-D Kalman filter, we suppose that the measurement positions <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/> are both independent, so we can ignore any interaction between them so that the covariance <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/> is 0. We look at only the variance in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and the variance in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>. </p>



<p>Then, the measurement noise covariance  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d6abdd487c56e5efbb2c9522ed4b9360_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#82;" title="Rendered by QuickLaTeX.com" height="12" width="14" style="vertical-align: 0px;"/> can be written as follows:</p>



<p> <a name="id3446937855"></a><p class="ql-center-displayed-equation" style="line-height: 59px;"><span class="ql-right-eqno"> (13) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-22ccd16096187de4df59896e0778d033_l3.png" height="59" width="157" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#82;&#125;&#32;&#61;&#32;&#32;&#32;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#113;&#117;&#97;&#100;&#92;&#92;&#32;&#32;&#120;&#32;&#92;&#92;&#32;&#121;&#32;&#92;&#92;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#97;&#114;&#114;&#97;&#121;&#125;&#123;&#99;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#120;&#32;&#32;&#92;&#109;&#115;&#112;&#97;&#99;&#101;&#123;&#50;&#55;&#109;&#117;&#125;&#32;&#38;&#32;&#32;&#121;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#92;&#92;&#108;&#101;&#102;&#116;&#91;&#92;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#120;&#32;&#38;&#32;&#32;&#48;&#32;&#32;&#92;&#92;&#48;&#32;&#38;&#32;&#32;&#92;&#115;&#105;&#103;&#109;&#97;&#94;&#50;&#95;&#121;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#109;&#97;&#116;&#114;&#105;&#120;&#125;&#92;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#32;&#92;&#101;&#110;&#100;&#123;&#97;&#114;&#114;&#97;&#121;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p>   </p>



<p>Great! We have discussed the essential concepts of 2-D Kalman filter, and now it&#8217;s time to implement it in Python. Let&#8217;s do it! </p>



<h2>Python Implementation</h2>



<p>Now we’re ready to implement  2-D Kalman Filter in Python.  So, let&#8217;s do it..</p>



<p>This project has three files, namely: <code>KalmanFilter.py</code>, <code>Detector.py</code>, and <code>objTracking.py</code>.</p>



<h3>KalmanFilter.py  </h3>



<p>Let&#8217;s look at first the <code>KalmanFilter.py</code>. This file contains one class called <code>KalmanFilter</code> consisting of three functions, <code>__init__()</code>, <code>predict()</code>, and <code>update()</code>.</p>



<p>We&#8217;re going to discuss these functions one by one.</p>



<h4>Function <em>Initialization</em>()</h4>



<p>The  function <code>__init__</code>() is used to initialize the class parameters.  It has six parameters, <code>dt</code>, <code>u_x</code>, <code>u_y</code>, <code>std_acc</code>, <code>x_std_meas</code> and  <code>y_std_meas</code>.  The <code>dt</code> is the sampling time or the time for 1 cycle  used  to estimate the state, <code>u_x</code> and <code>u_y</code> are the accelerations in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-directions, respectively. The <code>std_acc</code> is the process noise magnitude, <code>x_std_meas </code>and  <code>x_std_meas</code> are the standard deviations of the measurement in the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/>&#8211; and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>-directions, respectively. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">class KalmanFilter(object):
    def __init__(self, dt, u_x,u_y, std_acc, x_std_meas, y_std_meas):
        """
        :param dt: sampling time (time for 1 cycle)
        :param u_x: acceleration in x-direction
        :param u_y: acceleration in y-direction
        :param std_acc: process noise magnitude
        :param x_std_meas: standard deviation of the measurement in x-direction
        :param y_std_meas: standard deviation of the measurement in y-direction
        """

        # Define sampling time
        self.dt = dt

        # Define the  control input variables
        self.u = np.matrix([[u_x],[u_y]])

        # Intial State
        self.x = np.matrix([[0], [0], [0], [0]])

        # Define the State Transition Matrix A
        self.A = np.matrix([[1, 0, self.dt, 0],
                            [0, 1, 0, self.dt],
                            [0, 0, 1, 0],
                            [0, 0, 0, 1]])

        # Define the Control Input Matrix B
        self.B = np.matrix([[(self.dt**2)/2, 0],
                            [0, (self.dt**2)/2],
                            [self.dt,0],
                            [0,self.dt]])

        # Define Measurement Mapping Matrix
        self.H = np.matrix([[1, 0, 0, 0],
                            [0, 1, 0, 0]])

        #Initial Process Noise Covariance
        self.Q = np.matrix([[(self.dt**4)/4, 0, (self.dt**3)/2, 0],
                            [0, (self.dt**4)/4, 0, (self.dt**3)/2],
                            [(self.dt**3)/2, 0, self.dt**2, 0],
                            [0, (self.dt**3)/2, 0, self.dt**2]]) * std_acc**2

        #Initial Measurement Noise Covariance
        self.R = np.matrix([[x_std_meas**2,0],
                           [0, y_std_meas**2]])

        #Initial Covariance Matrix
        self.P = np.eye(self.A.shape[1])
</pre>



<p>We define the matrices A, B, and H as exactly in eq.(<a href="#id2300232522">6</a>),  eq.(<a href="#id841436821">7</a>), and  (<a href="#id1002015878">10</a>), respectively.  We define also the matrices Q and R as in eq.(<a href="#id1142397845">12</a>) and  eq.(<a href="#id3446937855">13</a>) , respectively.</p>



<p>The covariance matrix <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fda1e51b12ba3624074fcbebad72b1fc_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#80;" title="Rendered by QuickLaTeX.com" height="12" width="14" style="vertical-align: 0px;"/> can be initialized as an identity matrix whose shape is the same as the shape of the matrix A.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="47" data-enlighter-title="" data-enlighter-group="">        #Initial Covariance Matrix
        self.P = np.eye(self.A.shape[1])</pre>



<h4>Function <em>predict</em>()</h4>



<p>The function<code>predict()</code> does the prediction of the state estimate <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fb40c7cb76549e15a8e280ea2320b405_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#32;&#123;&#92;&#94;&#123;&#120;&#125;&#125;&#94;&#123;&#45;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="19" width="21" style="vertical-align: -6px;"/> and the error covariance <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d1c004d0e4a3c0792b19fbe733318ffe_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#32;&#123;&#80;&#125;&#94;&#123;&#45;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="18" width="24" style="vertical-align: -6px;"/> . This task also called the time update process because it projects forward the current state to the next time step. We use the same equation for the time update process as we discussed earlier in our last tutorial. Please refer to Eq.(<a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python#time-update">9</a>) and Eq.(<a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python#time-update">10</a>) of the last tutorial.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="53" data-enlighter-title="" data-enlighter-group="">    def predict(self):
        # Refer to :Eq.(9) and Eq.(10)  in https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/?preview_id=1364&amp;preview_nonce=52f6f1262e&amp;preview=true&amp;_thumbnail_id=1795

        # Update time state
        #x_k =Ax_(k-1) + Bu_(k-1)     Eq.(9)
        self.x = np.dot(self.A, self.x) + np.dot(self.B, self.u)

        # Calculate error covariance
        # P= A*P*A' + Q               Eq.(10)
        self.P = np.dot(np.dot(self.A, self.P), self.A.T) + self.Q
        return self.x[0:2]</pre>



<h4>Function <em>update</em>()</h4>



<p>During the update stage, we compute the Kalman gain  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-da0435ff7424603dc93ebe340b304e75_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#32;&#123;&#75;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="16" width="23" style="vertical-align: -3px;"/>, then update the  predicted state estimate  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-e71829888637ed2430504666db673520_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#32;&#123;&#92;&#94;&#123;&#120;&#125;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="16" width="18" style="vertical-align: -3px;"/> and predicted error  covariance  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-55b078aa5e2775a7a81369879eec9e1f_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#32;&#123;&#80;&#125;&#95;&#107;" title="Rendered by QuickLaTeX.com" height="15" width="21" style="vertical-align: -3px;"/>.  Please refer to Eq.(<a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python#measurement-update">11</a>),  Eq.(<a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python#measurement-update">12</a>) and Eq.(<a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python#measurement-update">13</a>) of the last tutorial. </p>



<p>The following is our code of the function <code>update()</code> that returns the updated positions of the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7e5fbfa0bbbd9f3051cd156a0f1b5e31_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;" title="Rendered by QuickLaTeX.com" height="8" width="10" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-38461fc041e953482219abf5d4cce1cb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#121;" title="Rendered by QuickLaTeX.com" height="12" width="9" style="vertical-align: -4px;"/>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="65" data-enlighter-title="" data-enlighter-group="">    def update(self, z):

        # Refer to :Eq.(11), Eq.(12) and Eq.(13)  in https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/?preview_id=1364&amp;preview_nonce=52f6f1262e&amp;preview=true&amp;_thumbnail_id=1795
        # S = H*P*H'+R
        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R

        # Calculate the Kalman Gain
        # K = P * H'* inv(H*P*H'+R)
        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))  #Eq.(11)

        self.x = np.round(self.x + np.dot(K, (z - np.dot(self.H, self.x))))   #Eq.(12)

        I = np.eye(self.H.shape[1])

        # Update error covariance matrix
        self.P = (I - (K * self.H)) * self.P   #Eq.(13)
        return self.x[0:2]</pre>



<p>Now, we&#8217;re going to look at the <code>Detector.py</code></p>



<h3>Detector.py</h3>



<p>This file plays a rule as an object detector. Since we&#8217;re detecting a simple object like a moving circle, we no need an advance object detector using machine learning. It&#8217;s enough to do by only using classical image processing algorithms.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># Import python libraries
import numpy as np
import cv2

def detect(frame,debugMode):
    # Convert frame from BGR to GRAY
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    if (debugMode):
        cv2.imshow('gray', gray)

    # Edge detection using Canny function
    img_edges = cv2.Canny(gray,  50, 190, 3)
    if (debugMode):
        cv2.imshow('img_edges', img_edges)

    # Convert to black and white image
    ret, img_thresh = cv2.threshold(img_edges, 254, 255,cv2.THRESH_BINARY)
    if (debugMode):
        cv2.imshow('img_thresh', img_thresh)

    # Find contours
    contours, _ = cv2.findContours(img_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Set the accepted minimum &amp; maximum radius of a detected object
    min_radius_thresh= 3
    max_radius_thresh= 30   

    centers=[]
    for c in contours:
        # ref: https://docs.opencv.org/trunk/dd/d49/tutorial_py_contour_features.html
        (x, y), radius = cv2.minEnclosingCircle(c)
        radius = int(radius)

        #Take only the valid circles
        if (radius > min_radius_thresh) and (radius &lt; max_radius_thresh):
            centers.append(np.array([[x], [y]]))
    cv2.imshow('contours', img_thresh)
    return centers
</pre>



<p>The <code>Detector.py</code> only contains one function, that is the <code>detect()</code>. It has two arguments, the<code>frame</code> and <code>debugMode</code>.  When calling this function, it will convert a video frame passed through the <code>frame</code> argument to a Grayscale image using <code>cv2.cvtColor()</code>.  After that, it detects the edge of the object in the image using Canny edge detection.</p>



<p>Then, using <code>cv2.threshold()</code> function, the image is converted to a binary image. A threshold value is applied.  If a pixel value is greater than a threshold value, it is assigned a specific value (e.g. 255, white), else it will be converted to 0, or black.</p>



<p>From the binary image, then we find the contour representing an object in the image using <code>cv2.findContours()</code>. </p>



<p>Finally, in every detected contour,  we find the circumcircle of an object using the function <code>cv2.minEnclosingCircle()</code>. The circle candidate must meet the minimum and maximum radius values. </p>



<p>Since this function is purposed to detected multiple objects, it returns the centers of the detected objects. Meaning that we can use this function not only for detecting one object but also for detecting multi-object.</p>



<p>Every processing step can be displayed by setting the <code>debugMode</code>=1 when calling the function <code>detect()</code>.</p>



<p>Now we&#8217;re going to discuss the file <code>objTracking.py</code>.  </p>



<h2>objTracking.py</h2>



<p>This is the main file of this project that we&#8217;ll execute to track an object. At the beginning of this file, we import function <code>detect()</code> from the file <code>Detector.py</code>, and class <code>KalmanFilter</code> from the file <code>KalmanFilter.py</code>.</p>



<p>In the function <code>main()</code>, we create the object of the class<code>KalmanFilter</code>  as follows:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="21" data-enlighter-title="" data-enlighter-group="">    #Create KalmanFilter object KF
    #KalmanFilter(dt, u_x, u_y, std_acc, x_std_meas, y_std_meas)
    KF = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)</pre>



<p>We set the parameters values as: <code>dt</code> = 0.1 , <code>u_x</code> = 1, <code>u_y</code>=1, <code>std_acc</code> = 1, <code>y_std_meas</code> =1. You can try to set other values and observed the performance.</p>



<p>After that, we create the object of the cv2.VideoCapture():</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="25" data-enlighter-title="" data-enlighter-group="">    # Create opencv video capture object
    VideoCap = cv2.VideoCapture('video/OneBall.avi')</pre>



<p>Since the video is read in RGB format, it is necessary to convert it to Grayscale format. For this task, we perform it in the file <code>Detector.py</code> as we discussed already earlier.</p>



<p>In the while loop and in every looping, we read a video frame and detect the center of an object in that frame. If the center is detected, then call the Kalman filter prediction function <code>KF.predict()</code> and  the Kalman filter updating function <code>KF.upadate()</code> to update the position of the object given a new measurement value (a detected center) and a prediction position.</p>



<p>The rest of the cod is to draw a circle and two rectangles. The circle is for the measured position and the rectangles are for the predicted and updated positions.</p>



<p>Here is the complete code for <code>objTracking.py</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import cv2
from Detector import detect
from KalmanFilter import KalmanFilter

def main():

    HiSpeed= 100
    ControlSpeedVar = 30  #Lowest: 1 - Highest:100
    debugMode=1

    #Create KalmanFilter object KF
    #KalmanFilter(dt, u_x, u_y, std_acc, x_std_meas, y_std_meas)
    KF = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)

    # Create opencv video capture object
    VideoCap = cv2.VideoCapture('video/OneBall.avi')

    while(True):
        # Read frame
        ret, frame = VideoCap.read()

        # Detect object
        centers = detect(frame,debugMode)
        
        # If centroids are detected then track them
        if (len(centers) > 0):

            # Draw the detected circle
            cv2.circle(frame, (int(centers[0][0]), int(centers[0][1])), 10, (0, 191, 255), 2)

            # Predict
            (x, y) = KF.predict()
            # Draw a rectangle as the predicted object position
            cv2.rectangle(frame, (x - 15, y - 15), (x + 15, y + 15), (255, 0, 0), 2)

            # Update
            (x1, y1) = KF.update(centers[0])
            # Draw a rectangle as the estimated object position
            cv2.rectangle(frame, (x1 - 15, y1 - 15), (x1 + 15, y1 + 15), (0, 0, 255), 2)

            cv2.putText(frame, "Estimated Position", (x1 + 15, y1 + 10), 0, 0.5, (0, 0, 255), 2)
            cv2.putText(frame, "Predicted Position", (x + 15, y), 0, 0.5, (255, 0, 0), 2)
            cv2.putText(frame, "Measured Position", (centers[0][0] + 15, centers[0][1] - 15), 0, 0.5, (0,191,255), 2)

        cv2.imshow('image', frame)

        if cv2.waitKey(2) &amp; 0xFF == ord('q'):
            VideoCap.release()
            cv2.destroyAllWindows()
            break

        cv2.waitKey(HiSpeed-ControlSpeedVar+1)


if __name__ == "__main__":
    # execute main
    main()</pre>



<p>To run this program, I provide a video that you can download it <a href="https://github.com/RahmadSadli/2-D-Kalman-Filter/tree/master/video/randomball.avi">here</a>.</p>



<p>The complete code of this project can be found in the link below:</p>



<p><a href="https://github.com/RahmadSadli/2-D-Kalman-Filter">https://github.com/RahmadSadli/2-D-Kalman-Filter</a></p>



<p>This is the end of this tutorial and see you in the next tutorial.</p>



<h4>Recent posts:</h4>



<ul><li><a href="https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/">Object Tracking: Simple Implementation of Kalman</a></li><li><a href="https://machinelearningspace.com/nlp-sentiment-analysis-using-keras-embedding-layer-in-tensorflow-2-0/">Sentiment Analysis Using Keras Embedding Layer in TensorFlow 2.0</a></li><li><a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolov3-in-tensorflow-2-0-part-1/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0</a></li></ul>



<h2>Reference</h2>



<ol><li><a href="https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf">Greg Welch and Gary Bishop, &#8216;An Introduction to the Kalman Filter&#8217;, July 24, 2006</a> </li><li><a href="https://www.intechopen.com/books/introduction-and-implementations-of-the-kalman-filter/introduction-to-kalman-filter-and-its-applications#B5">Youngjoo Kim and Hyochoong Bang, Introduction to Kalman Filter and Its Applications, November 2018</a></li><li><a href="http://studentdavestutorials.weebly.com/">Student Dave, Kalman Filter With Matlab Code</a></li></ol>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/">Object Tracking: 2-D Object Tracking using Kalman Filter in Python</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/2d-object-tracking-using-kalman-filter/feed/</wfw:commentRss>
			<slash:comments>8</slash:comments>
		
		<enclosure url="https://github.com/RahmadSadli/2-D-Kalman-Filter/tree/master/video/randomball.avi" length="0" type="video/avi" />

			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-4</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Sun, 29 Dec 2019 18:06:53 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[object detection yolov3]]></category>
		<category><![CDATA[yolov3 object detection]]></category>
		<category><![CDATA[Yolov3 TensorFlow 2.0]]></category>
		<category><![CDATA[yolov3 tensorflow 2.0 python]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=81</guid>

					<description><![CDATA[<p>In part 3, we&#8217;ve created a python code to convert the file yolov3.weights into the TensorFlow 2.0 weights format. Now, we&#8217;re already in part 4, and this is our last part of this tutorial. In this part, we&#8217;re going to work on 3 files, utils.py, image.py and video.py. The file utils.py contains useful functions for [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">In part 3</a>,  we&#8217;ve created a python code to convert the file <code>yolov3.weights</code> into the TensorFlow 2.0 weights format.  Now, we&#8217;re already in part 4, and this is our last part of this tutorial.  </p>



<p>In this part, we&#8217;re going to work on 3 files, <code>utils.py</code>, <code>image.py</code> and <code>video.py</code>. The file <code>utils.py</code> contains useful functions for the implementation of YOLOv3. The files <code>image.py</code> and  <code>video.py</code> are the files that will be used for testing an image and a video/camera, respectively.</p>



<p>So, let&#8217;s get into it&#8230;.</p>



<h3>utils.py</h3>



<p>The file <code>utils.py</code> will contain the useful functions that we&#8217;ll be creating soon, they are: <code>non_maximum_suppression()</code>,  <code>resize_image()</code>,  <code>output_boxes()</code>, and <code>draw_output()</code>.  </p>



<p>Open the file <code>utils.py</code> and import the necessary packages as the following:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="2" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
import numpy as np
import cv2</pre>



<h4>non_max_suppression() </h4>



<p>Now, we&#8217;re going to create the function <code>non_max_suppression()</code>. If you forget about what the non-maximum suppression is, just go back to our <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">first part</a> of this tutorial and read it carefully. </p>



<p>Here, we&#8217;re not going to develop NMS algorithm from scratch. Instead, we leverage the TensorFlow&#8217;s built-in NMS function, <code>tf.image.combined_non_max_suppression</code>.  </p>



<p>Here is the code for the <code>non_max_suppression()</code> function:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="7" data-enlighter-title="" data-enlighter-group="">def non_max_suppression(inputs, model_size, max_output_size, 
                        max_output_size_per_class, iou_threshold, confidence_threshold):
    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)
    bbox=bbox/model_size[0]

    scores = confs * class_probs
    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),
        scores=tf.reshape(scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),
        max_output_size_per_class=max_output_size_per_class,
        max_total_size=max_output_size,
        iou_threshold=iou_threshold,
        score_threshold=confidence_threshold
    )
    return boxes, scores, classes, valid_detections</pre>



<h4>resize_image()</h4>



<p>We resize the image to fit with the model’s size.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="26" data-enlighter-title="" data-enlighter-group="">def resize_image(inputs, modelsize):
    inputs= tf.image.resize(inputs, modelsize)
    return inputs</pre>



<h4>load_class_names()</h4>



<p>The following is the code for function load_class_names().</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="32" data-enlighter-title="" data-enlighter-group="">def load_class_names(file_name):
    with open(file_name, 'r') as f:
        class_names = f.read().splitlines()
    return class_names</pre>



<h4>output_boxes() </h4>



<p>This function is used to convert the boxes into the format of (<code>top-left-corner</code>, <code>bottom-right-corner</code>), following by applying the NMS function and returning the proper bounding boxes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="38" data-enlighter-title="" data-enlighter-group="">def output_boxes(inputs,model_size, max_output_size, max_output_size_per_class, 
                 iou_threshold, confidence_threshold):

    center_x, center_y, width, height, confidence, classes = \
        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)

    top_left_x = center_x - width / 2.0
    top_left_y = center_y - height / 2.0
    bottom_right_x = center_x + width / 2.0
    bottom_right_y = center_y + height / 2.0

    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,
                        bottom_right_y, confidence, classes], axis=-1)

    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size, 
                                      max_output_size_per_class, iou_threshold, confidence_threshold)

    return boxes_dicts</pre>


<p><!--EndFragment--></p>


<h4>draw_outputs()</h4>



<p>Finally, we create a function to draw the output.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="57" data-enlighter-title="" data-enlighter-group="">def draw_outputs(img, boxes, objectness, classes, nums, class_names):
    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]
    boxes=np.array(boxes)

    for i in range(nums):
        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))
        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))

        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)

        img = cv2.putText(img, '{} {:.4f}'.format(
            class_names[int(classes[i])], objectness[i]),
                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)
        return img</pre>



<p>Here is the complete code of  <code>utils.py</code>:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
import numpy as np
import cv2
import time

def non_max_suppression(inputs, model_size, max_output_size,
                        max_output_size_per_class, iou_threshold,
                        confidence_threshold):
    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)
    bbox=bbox/model_size[0]

    scores = confs * class_probs
    boxes, scores, classes, valid_detections = \
        tf.image.combined_non_max_suppression(
        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),
        scores=tf.reshape(scores, (tf.shape(scores)[0], -1,
                                   tf.shape(scores)[-1])),
        max_output_size_per_class=max_output_size_per_class,
        max_total_size=max_output_size,
        iou_threshold=iou_threshold,
        score_threshold=confidence_threshold
    )
    return boxes, scores, classes, valid_detections


def resize_image(inputs, modelsize):
    inputs= tf.image.resize(inputs, modelsize)
    return inputs


def load_class_names(file_name):
    with open(file_name, 'r') as f:
        class_names = f.read().splitlines()
    return class_names


def output_boxes(inputs,model_size, max_output_size, max_output_size_per_class,
                 iou_threshold, confidence_threshold):

    center_x, center_y, width, height, confidence, classes = \
        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)

    top_left_x = center_x - width / 2.0
    top_left_y = center_y - height / 2.0
    bottom_right_x = center_x + width / 2.0
    bottom_right_y = center_y + height / 2.0

    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,
                        bottom_right_y, confidence, classes], axis=-1)

    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size,
                                      max_output_size_per_class, iou_threshold, confidence_threshold)

    return boxes_dicts

def draw_outputs(img, boxes, objectness, classes, nums, class_names):
    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]
    boxes=np.array(boxes)

    for i in range(nums):
        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))
        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))

        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)

        img = cv2.putText(img, '{} {:.4f}'.format(
            class_names[int(classes[i])], objectness[i]),
                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)
    return img</pre>



<p>That&#8217;s all for the file  <code>utils.py</code>. Now, let&#8217;s create the testing programs for image and video.  </p>



<h3>image.py</h3>



<p>Now, open <code>image.py</code> and write the following code: </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group=""># image.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
import cv2
import numpy as np
from yolov3 import YOLOv3Net

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 40
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'
img_path = "data/images/test.jpg"</pre>



<p>Put a testing image under the directory path: <code>data/image</code>.  Let&#8217;s say our image named <code>test.jpg</code>, then add the following line to <code>image.py</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="22" data-enlighter-title="" data-enlighter-group="">img_path= "data/images/test.jpg"</pre>



<p>You can download this image and save it as <code>test.jpg</code>.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1200" height="675" src="https://machinelearningspace.com/wp-content/uploads/2020/01/test-1.jpg" alt="" class="wp-image-643" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/test-1.jpg 1200w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-300x169.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-1024x576.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-768x432.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-533x300.jpg 533w" sizes="(max-width: 1200px) 100vw, 1200px" /><figcaption>Source: <a href="https://france3-regions.francetvinfo.fr/bourgogne-franche-comte/sites/regions_france3/files/styles/top_big/public/assets/images/2018/08/29/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE">https://france3-regions.francetvinfo.fr/bourgogne-franche-comte/sites/regions_france3/files/styles/top_big/public/assets/images/2018/08/29/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE</a></figcaption></figure></div>



<h4>Main Pipeline</h4>



<p>Here is the main pipeline function:  loading the model,  reading the input image, performing the detection, and drawing the outputs:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="24" data-enlighter-title="" data-enlighter-group="">def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)
    model.load_weights(weightfile)

    class_names = load_class_names(class_name)

    image = cv2.imread(img_path)
    image = np.array(image)
    image = tf.expand_dims(image, 0)

    resized_frame = resize_image(image, (model_size[0],model_size[1]))
    pred = model.predict(resized_frame)

    boxes, scores, classes, nums = output_boxes( \
        pred, model_size,
        max_output_size=max_output_size,
        max_output_size_per_class=max_output_size_per_class,
        iou_threshold=iou_threshold,
        confidence_threshold=confidence_threshold)

    image = np.squeeze(image)
    img = draw_outputs(image, boxes, scores, classes, nums, class_names)

    win_name = 'Image detection'
    cv2.imshow(win_name, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    #If you want to save the result, uncommnent the line below:
    #cv2.imwrite('test.jpg', img)</pre>



<p>Here is the complete code of the  <code>image.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># image.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
import cv2
import numpy as np
from yolov3 import YOLOv3Net

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 40
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'
img_path = "data/images/test.jpg"

def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)
    model.load_weights(weightfile)

    class_names = load_class_names(class_name)

    image = cv2.imread(img_path)
    image = np.array(image)
    image = tf.expand_dims(image, 0)

    resized_frame = resize_image(image, (model_size[0],model_size[1]))
    pred = model.predict(resized_frame)

    boxes, scores, classes, nums = output_boxes( \
        pred, model_size,
        max_output_size=max_output_size,
        max_output_size_per_class=max_output_size_per_class,
        iou_threshold=iou_threshold,
        confidence_threshold=confidence_threshold)

    image = np.squeeze(image)
    img = draw_outputs(image, boxes, scores, classes, nums, class_names)

    win_name = 'Image detection'
    cv2.imshow(win_name, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    #If you want to save the result, uncommnent the line below:
    #cv2.imwrite('test.jpg', img)


if __name__ == '__main__':
    main()</pre>



<h4>Testing image</h4>



<p>Finally, we&#8217;re now ready to execute our first implementation. </p>



<p>In Anaconda prompt or in PyCharm Terminal, type the following command and press Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python image.py</pre>



<p>Here is the result. Bravo..! Finally, you did it.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1200" height="675" src="https://machinelearningspace.com/wp-content/uploads/2020/01/test-2.jpg" alt="" class="wp-image-645" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/test-2.jpg 1200w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-300x169.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-1024x576.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-768x432.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-533x300.jpg 533w" sizes="(max-width: 1200px) 100vw, 1200px" /></figure></div>


<p><!--StartFragment--></p>


<h3>video.py</h3>



<p>Open the file <code>video.py</code> then import the necessary packages as follow:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#video.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
from yolov3 import YOLOv3Net
import cv2
import time

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 100
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'</pre>



<p>Then create the main function.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)

    model.load_weights(weightfile)

    class_names = load_class_names(class_name)



    win_name = 'Yolov3 detection'
    cv2.namedWindow(win_name)

    #specify the vidoe input.
    # 0 means input from cam 0.
    # For vidio, just change the 0 to video path
    cap = cv2.VideoCapture(0)
    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),
                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    try:
        while True:
            start = time.time()
            ret, frame = cap.read()
            if not ret:
                break

            resized_frame = tf.expand_dims(frame, 0)
            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))

            pred = model.predict(resized_frame)

            boxes, scores, classes, nums = output_boxes( \
                pred, model_size,
                max_output_size=max_output_size,
                max_output_size_per_class=max_output_size_per_class,
                iou_threshold=iou_threshold,
                confidence_threshold=confidence_threshold)

            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)
            cv2.imshow(win_name, img)

            stop = time.time()

            seconds = stop - start
            # print("Time taken : {0} seconds".format(seconds))

            # Calculate frames per second
            fps = 1 / seconds
            print("Estimated frames per second : {0}".format(fps))

            key = cv2.waitKey(1) &amp; 0xFF

            if key == ord('q'):
                break</pre>



<p>The function above read the input from a camera. If you want to test with the video just change this code (line 17):</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">    cap = cv2.VideoCapture(0)</pre>



<p>to:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">    cap = cv2.VideoCapture(videopath)</pre>



<p>Where, <code>video path</code> is the path of your video.</p>



<p>Here is the complete code of the <code>video.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
from yolov3 import YOLOv3Net
import cv2
import time

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 100
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5


cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'

def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)

    model.load_weights(weightfile)

    class_names = load_class_names(class_name)



    win_name = 'Yolov3 detection'
    cv2.namedWindow(win_name)

    #specify the vidoe input.
    # 0 means input from cam 0.
    # For vidio, just change the 0 to video path
    cap = cv2.VideoCapture(0)
    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),
                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    try:
        while True:
            start = time.time()
            ret, frame = cap.read()
            if not ret:
                break

            resized_frame = tf.expand_dims(frame, 0)
            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))

            pred = model.predict(resized_frame)

            boxes, scores, classes, nums = output_boxes( \
                pred, model_size,
                max_output_size=max_output_size,
                max_output_size_per_class=max_output_size_per_class,
                iou_threshold=iou_threshold,
                confidence_threshold=confidence_threshold)

            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)
            cv2.imshow(win_name, img)

            stop = time.time()

            seconds = stop - start
            # print("Time taken : {0} seconds".format(seconds))

            # Calculate frames per second
            fps = 1 / seconds
            print("Estimated frames per second : {0}".format(fps))

            key = cv2.waitKey(1) &amp; 0xFF

            if key == ord('q'):
                break

    finally:
        cv2.destroyAllWindows()
        cap.release()
        print('Detections have been performed successfully.')

if __name__ == '__main__':
    main()</pre>



<p>Execute the code with this command:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python video.py</pre>



<p>This is the end of our tutorial series of &#8220;The beginner’s guide to implementing YOLO (v3) in TensorFlow 2.0&#8221;. I hope you enjoy it.</p>



<h2>Conclusion</h2>



<p>In this tutorial series, we have implemented the YOLOv3 object detection algorithm in TensorFlow 2.0 from scratch. I made this tutorial simple and presented the code in a simple way so that every beginner just getting started learning object detection algorithms can learn it easily.<br>I hope this tutorial series will help you and will be useful for your skills as a deep learning practitioner. Don&#8217;t forget to share it and see you in another tutorial.</p>



<h2>Parts</h2>



<ul><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights  into TensorFlow 2.0 weights format.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p> </li></ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/feed/</wfw:commentRss>
			<slash:comments>67</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-3</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Sun, 29 Dec 2019 18:05:45 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[yolov3 object detection]]></category>
		<category><![CDATA[Yolov3 Tensorflow]]></category>
		<category><![CDATA[YoloV3 TensorFlow 2]]></category>
		<category><![CDATA[Yolov3 TensorFlow 2.0]]></category>
		<category><![CDATA[Yolov3 tutorial]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=79</guid>

					<description><![CDATA[<p>In part 2, we&#8217;ve discovered how to construct the YOLOv3 network. In this part 3, we&#8217;ll focus on the file yolov3.weights. So, what we&#8217;re going to do in part is to load the weights parameters from the file yolov3.weights, then convert them into the TensorFlow 2.0 weights format. Just to remain you that, the file [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">In part 2</a>, we&#8217;ve discovered how to construct the YOLOv3 network.  In this part 3, we&#8217;ll focus on the file <code>yolov3.weights</code>.</p>



<p>So, what we&#8217;re going to do in part is to load the weights parameters from the file <code>yolov3.weights</code>, then convert them into the TensorFlow 2.0 weights format.  Just to remain you that, the file <code>yolov3.weights</code> contains the pre-trained CNN&#8217;s parameters of YOLOv3.</p>



<p>To begin with, let&#8217;s take a look at how the YOLOv3 weights are stored.</p>



<h3 id="understandingtheweightsfile">How YOLOv3&#8217;s weights are stored?</h3>



<p>The original YOLOv3 weights file <code>yolov3.weights</code>  is a binary file and the weights are stored in the float data type. </p>



<p>One thing that we need to know that the weights only belong to convolutional layers. As we know, in YOLOv3, there are 2  convolutional layer types, with and without a batch normalization layer. So,  the weights are applied differently for different types of convolutional layers.  Since we&#8217;re reading the only float data, there&#8217;s no clue which one belongs to which layer.  If we incorrectly associate these weights with their layers properly, we&#8217;ll screw up everything, and the weights won&#8217;t be converted. So, that&#8217;s why understanding how the weights are stored is crucially important.  </p>



<p>Here, I tried to make a simple flowchart in order to describe how the weights are stored. </p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/weights.jpg" alt="" class="wp-image-529" width="444" height="436" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/weights.jpg 562w, https://machinelearningspace.com/wp-content/uploads/2020/01/weights-300x295.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/weights-305x300.jpg 305w" sizes="(max-width: 444px) 100vw, 444px" /></figure></div>


<p>When we re-write these weights to TensorFlow&#8217;s format  for a convolutional with a batch normalization layer, we need to switch the position of <code>beta</code> and <code>gamma</code>. So, they&#8217;re ordered like this: <code>beta</code>, <code>gamma</code>, <code>means</code>, <code>variance </code>and <code>conv weights</code>.  However,  the weights&#8217; order remains the same for the convolutional without a batch normalization layer.</p>



<p>All right!!, Now we&#8217;re ready to code the weights converter. </p>



<p>Without further ado, let&#8217;s do it&#8230;</p>



<h3>Working with the file <code>convert_weights.py</code></h3>



<p>Open the file <code>convert_weights.py</code>, then copy and paste the following code to the top of it. Here, we import NumPy library and the two functions that we&#8217;ve created previously in part 2, <code>YOLOv3Net</code> and <code>parse_cfg</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group="">#convert_weights.py
import numpy as np
from yolov3 import YOLOv3Net
from yolov3 import parse_cfg</pre>



<p>Now, let&#8217;s create a function called <code>load_weights()</code>. This function has 3 parameters, <code>model</code>, <code>cfgfile</code>, and <code>weightfile</code>. The parameter <code>model</code> is a returning parameters of the network&#8217;s model after calling the function <code>YOLOv3Net</code>.  The<code>cfgfile</code> and <code>weightfile</code> are respectively refer to the files <code>yolov3.cfg</code> and <code>yolov3.weights</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="6" data-enlighter-title="" data-enlighter-group="">def load_weights(model,cfgfile,weightfile):</pre>



<p>Open the file  <code>yolov3.weights</code> and read the first 5 values. These values are the header information. So, we can skip them all.<br></p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="7" data-enlighter-title="" data-enlighter-group="">    # Open the weights file
    fp = open(weightfile, "rb")

    # The first 5 values are header information
    np.fromfile(fp, dtype=np.int32, count=5)</pre>



<p>Then call <code>parse_cfg()</code> function.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="14" data-enlighter-title="" data-enlighter-group="">    blocks = parse_cfg(cfgfile)</pre>



<p>As we did when building the YOLOv3 network, we need to loop over the <code>blocks</code> and search for the convolutional layer. Don&#8217;t forget to check whether the convolutional is with batch normalization or not. If it is true, go get the relevant values (<code>gamma</code>, <code>beta</code>, <code>means</code>, and <code>variance</code>), and re-arrange them to the TensorFlow weights order. Otherwise, take the bias values. After that take the convolutional weights and set these weights to the convolutional layer depending on the convolutional type.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="16" data-enlighter-title="" data-enlighter-group="">    for i, block in enumerate(blocks[1:]):

        if (block["type"] == "convolutional"):
            conv_layer = model.get_layer('conv_' + str(i))
            print("layer: ",i+1,conv_layer)

            filters = conv_layer.filters
            k_size = conv_layer.kernel_size[0]
            in_dim = conv_layer.input_shape[-1]

            if "batch_normalize" in block:

                norm_layer = model.get_layer('bnorm_' + str(i))
                print("layer: ",i+1,norm_layer)
                size = np.prod(norm_layer.get_weights()[0].shape)

                bn_weights = np.fromfile(fp, dtype=np.float32, count=4 * filters)
                # tf [gamma, beta, mean, variance]
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]

            else:
                conv_bias = np.fromfile(fp, dtype=np.float32, count=filters)

            # darknet shape (out_dim, in_dim, height, width)
            conv_shape = (filters, in_dim, k_size, k_size)
            conv_weights = np.fromfile(
                fp, dtype=np.float32, count=np.product(conv_shape))
            # tf shape (height, width, in_dim, out_dim)
            conv_weights = conv_weights.reshape(
                conv_shape).transpose([2, 3, 1, 0])

            if "batch_normalize" in block:
                norm_layer.set_weights(bn_weights)
                conv_layer.set_weights([conv_weights])
            else:
                conv_layer.set_weights([conv_weights, conv_bias])</pre>



<p>Alert if the reading has failed. Then, close the file whether the reading was successful or not.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="53" data-enlighter-title="" data-enlighter-group="">    assert len(fp.read()) == 0, 'failed to read all data'
    fp.close()</pre>



<p>The last part of this code is the main function. Copy and paste the following code of the main function just right after the function <code>load_weights()</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="57" data-enlighter-title="" data-enlighter-group="">def main():

    weightfile = "weights/yolov3.weights"
    cfgfile = "cfg/yolov3.cfg"

    model_size = (416, 416, 3)
    num_classes = 80

    model=YOLOv3Net(cfgfile,model_size,num_classes)
    load_weights(model,cfgfile,weightfile)

    try:
        model.save_weights('weights/yolov3_weights.tf')
        print('\nThe file \'yolov3_weights.tf\' has been saved successfully.')
    except IOError:
        print("Couldn't write the file \'yolov3_weights.tf\'.")</pre>



<p>Here is the complete code for the  <code>convert_weights.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#convert_weights.py
import numpy as np
from yolov3 import YOLOv3Net
from yolov3 import parse_cfg

def load_weights(model,cfgfile,weightfile):
    # Open the weights file
    fp = open(weightfile, "rb")

    # Skip 5 header values
    np.fromfile(fp, dtype=np.int32, count=5)

    # The rest of the values are the weights
    blocks = parse_cfg(cfgfile)

    for i, block in enumerate(blocks[1:]):

        if (block["type"] == "convolutional"):
            conv_layer = model.get_layer('conv_' + str(i))
            print("layer: ",i+1,conv_layer)

            filters = conv_layer.filters
            k_size = conv_layer.kernel_size[0]
            in_dim = conv_layer.input_shape[-1]

            if "batch_normalize" in block:

                norm_layer = model.get_layer('bnorm_' + str(i))
                print("layer: ",i+1,norm_layer)
                size = np.prod(norm_layer.get_weights()[0].shape)

                bn_weights = np.fromfile(fp, dtype=np.float32, count=4 * filters)
                # tf [gamma, beta, mean, variance]
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]

            else:
                conv_bias = np.fromfile(fp, dtype=np.float32, count=filters)

            # darknet shape (out_dim, in_dim, height, width)
            conv_shape = (filters, in_dim, k_size, k_size)
            conv_weights = np.fromfile(
                fp, dtype=np.float32, count=np.product(conv_shape))
            # tf shape (height, width, in_dim, out_dim)
            conv_weights = conv_weights.reshape(
                conv_shape).transpose([2, 3, 1, 0])

            if "batch_normalize" in block:
                norm_layer.set_weights(bn_weights)
                conv_layer.set_weights([conv_weights])
            else:
                conv_layer.set_weights([conv_weights, conv_bias])

    assert len(fp.read()) == 0, 'failed to read all data'
    fp.close()


def main():

    weightfile = "weights/yolov3.weights"
    cfgfile = "cfg/yolov3.cfg"

    model_size = (416, 416, 3)
    num_classes = 80

    model=YOLOv3Net(cfgfile,model_size,num_classes)
    load_weights(model,cfgfile,weightfile)

    try:
        model.save_weights('weights/yolov3_weights.tf')
        print('\nThe file \'yolov3_weights.tf\' has been saved successfully.')
    except IOError:
        print("Couldn't write the file \'yolov3_weights.tf\'.")


if __name__ == '__main__':
    main()</pre>



<p>Finally, we can now execute the  <code>convert_weights.py</code> file . Open  Anaconda prompt or terminal in Pycharm, type the following command and press Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python convert_weights.py</pre>



<p>Here is the output, I printed all the convolutional layers just to make sure that the weights are loaded correctly until the last convolutional layer.</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1089" height="937" src="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3.png" alt="" class="wp-image-592" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3.png 1089w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-300x258.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-1024x881.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-768x661.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-349x300.png 349w" sizes="(max-width: 1089px) 100vw, 1089px" /></figure></div>


<p>If you use PyCharm, look at the Project Navigation on the left side as I pointed with the red boxes, you have 4 new files:</p>



<ul><li>checkpoint</li><li>yolov3_weights.tf.data-00000-of-00002</li><li>yolov3_weights.tf.data-00001-of-00002</li><li>yolov3_weights.tf.index</li></ul>



<p>Those files are the TensorFlow 2.0 weights format. So, anytime we want to use them, just simply call them like the only one file,  <code>yolov3_weights.tf</code>. We&#8217;ll see how to do this in the last part of this tutorial. </p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1357" height="819" src="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2.png" alt="" class="wp-image-594" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2.png 1357w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-300x181.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-1024x618.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-768x464.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-497x300.png 497w" sizes="(max-width: 1357px) 100vw, 1357px" /></figure></div>


<p>This is the end of part 3, and we still have several things to do, those are: </p>



<ul><li>creating a pipeline to read the input image or video/camera, </li><li>computing the prediction, </li><li>and drawing the bounding boxes prediction over the input image/video/camera. </li></ul>



<p>Those are what we will be doing soon in the <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">last part</a>. Let&#8217;s go into it&#8230;</p>



<h2>Parts</h2>



<ul>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part--2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p>
</li>
</ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/feed/</wfw:commentRss>
			<slash:comments>21</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-2</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 17:46:29 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[Belajar Deteksi Objek]]></category>
		<category><![CDATA[Belajar Yolo]]></category>
		<category><![CDATA[cara mudah Yolo]]></category>
		<category><![CDATA[Pendeteksi objek]]></category>
		<category><![CDATA[simple yolov3]]></category>
		<category><![CDATA[Simple Yolov3 tutorial]]></category>
		<category><![CDATA[TensorFlow 2 yolov3 tutorial]]></category>
		<category><![CDATA[tensorflow 2.0 yolov3]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=44</guid>

					<description><![CDATA[<p>In part 1, we've discussed the YOLOv3 algorithm. Now, it's time to dive into the technical details of the Yolov3 implementation in Tensorflow 2.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">In part 1</a>, we&#8217;ve discussed the YOLOv3 algorithm. Now, it&#8217;s time to dive into the technical details for the implementation of YOLOv3 in Tensorflow 2.</p>



<p>The code for this tutorial designed to run on Python 3.7 and TensorFlow 2.0 can be found in my <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/RahmadSadli/Deep-Learning/tree/master/YOLOv3_TF2" target="_blank">Github repo</a>.</p>



<p>This tutorial was inspired by Ayoosh Kathuria, from one of his great articles about the implementation of YOLOv3 in Pytorch published in paperspaces&#8217;s blog (credit link at the end of this tutorial). </p>



<p>YOLOv3 has 2 important files: <code>yolov3.cfg</code> and <code>yolov3.weights</code>.  The file <code>yolov3.cfg</code> contains all information related to the YOLOv3 architecture and its parameters, whereas the file <code>yolov3.weights</code> contains the convolutional neural network (CNN) parameters of the YOLOv3 pre-trained weights. </p>



<p>Specifically, in this part, we&#8217;ll focus only on the file <code>yolov3.cfg</code>, while the file <code>yolov3.weights</code> will be discussed in the next <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">part</a>. </p>



<p>So, what we&#8217;re going to do now is to parse the parameters from the file <code>yolov3.cfg</code>, read them all, and based on that we&#8217;ll construct the YOLOv3 network.<br></p>



<p>Take your hot drink and let&#8217;s get into it&#8230;</p>



<h2>Preparation</h2>



<h3>Creating a Project Directory and Files</h3>



<p>The first thing that we need to do is to create a project directory, I personally name it <code>PROJECTS</code> because I have several projects under it. However, feel free to give another name as you want, but I suggest you do the same thing as I did so that you can follow this tutorial easily. </p>



<p>Under <code>PROJECTS</code>, create a directory named <code>YOLOv3_TF2</code>.  This is the directory where we&#8217;ll be working. <br><br>Now, under the <code>YOLOv3_TF2</code> directory, let&#8217;s create 4 subdirectories, namely: <code><strong>img</strong></code>, <code><strong>cfg</strong></code>, <code><strong>data</strong></code>,and <code><strong>weights</strong></code>. </p>



<p>And still under <code>PROJECTS</code>, now create 5 python files, they are: </p>



<ul><li><code>yolov3.py</code>,</li><li><code>convert_weights.py</code>, </li><li><code>utils.py</code>, </li><li><code>image.py</code>, and </li><li><code>video.py</code>. </li></ul>



<p>Specifically, in this part, we&#8217;ll only work on the file <code>yolov3.py</code> and leave the others all empty for the moment.</p>



<h3>Downloading files <strong><code>yolov3.cfg</code></strong>, <code>yolov3.weights</code>, and <code>coco.names</code></h3>



<p>Here are the links to download the files <code>yolov3.cfg</code>, <code>yolov3.weights</code>, and <code>coco.names</code>:</p>



<ul><li><a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg">yolov3.cfg</a></li><li><a href="https://pjreddie.com/media/files/yolov3.weights">yolov3.weights</a></li><li><a href="https://github.com/pjreddie/darknet/blob/master/data/coco.names">coco.names</a></li></ul>



<p>Save the files <code>yolov3.cfg</code>, <code>yolov3.weights</code>, and <code>coco.names</code> to the subdirectories <code><strong>cfg</strong></code>, <code><strong>weights</strong></code>, and <code><strong>data</strong></code>, respectively.</p>



<h2>yolov3.py </h2>



<h3>Importing the necessary packages</h3>



<p>Open  the <code>yolov3.py</code> and import TensorFlow and Keras Model. We also import the layers from Keras, they are <code>Conv2D</code>, <code>Input</code>, <code>ZeroPadding2D</code>, <code>LeakyReLU</code>, and <code>UpSampling2D</code><em>. </em>We&#8217;ll use them all when we build the YOLOv3 network. </p>



<p>Copy the following lines to the top of the file <code>yolov3.py</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="1,2" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group="">#yolov3.py
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import BatchNormalization, Conv2D, \
    Input, ZeroPadding2D, LeakyReLU, UpSampling2D</pre>



<h3>Parsing the configuration file</h3>



<p>The code below is a function  called <code>parse_cfg()</code> with a parameter named <code>cfgfile</code> used to parse the YOLOv3 configuration file<code>yolov3.cfg</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="10" data-enlighter-title="" data-enlighter-group="">def parse_cfg(cfgfile):
    with open(cfgfile, 'r') as file:
        lines = [line.rstrip('\n') for line in file if line != '\n' and line[0] != '#']
    holder = {}
    blocks = []
    for line in lines:
        if line[0] == '[':
            line = 'type=' + line[1:-1].rstrip()
            if len(holder) != 0:
                blocks.append(holder)
                holder = {}
        key, value = line.split("=")
        holder[key.rstrip()] = value.lstrip()
    blocks.append(holder)
    return blocks</pre>



<p>Let&#8217;s explain this code. </p>



<p>Lines 11-12, we open the <code>cfgfile</code> and read it, then remove unnecessary characters like &#8216;\n&#8217; and &#8216;#&#8217;. </p>



<p>The variable <code>lines</code> in line 12 is now holding all the lines of the file <code>yolov3.cfg</code>. So, we need to loop over it in order to read every single line from it.</p>



<p>Lines 15-23,  loop over the variable <code>lines</code> and read every single attribute from it and store them all in the list <code>blocks</code>.  This process is performed by reading the attributes block per block.  The block&#8217;s attributes and their values are firstly stored as the key-value pairs in a dictionary <code>holder</code>. After reading each block, all attributes are then appended to the list <code>blocks</code> and the <code>holder</code> is then made empty and ready to read another block.  Loop until all blocks are read before returning the content of the list <code>blocks</code>. </p>



<p>All right!..we just finished a small piece of code. The next step is to create the YOLOv3 network function. Let&#8217;s do it..</p>



<h3>Building the YOLOv3 Network</h3>



<p>We&#8217;re still working on the file <code>yolov3.py</code>, the following is the code for the YOLOv3 network function, called the <code>YOLOv3Net</code>. We pass a parameter named <code>cfgfile</code>. So, Just copy and paste the following lines under the previous function <code>parse_cfg()</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="27" data-enlighter-title="" data-enlighter-group="">def YOLOv3Net(cfgfile, model_size, num_classes):

    blocks = parse_cfg(cfgfile)

    outputs = {}
    output_filters = []
    filters = []
    out_pred = []
    scale = 0

    inputs = input_image = Input(shape=model_size)
    inputs = inputs / 255.0</pre>



<p>Let&#8217;s look at it&#8230;</p>



<p>Line 27, we first call the  function <code>parse_cfg()</code> and store all the return attributes in a variable <code>blocks</code>. Here, the variable <code>blocks</code> contains all the attributes read from the file <code>yolov3.cfg</code>. </p>



<p>Lines 37-38,  we define the input model using Keras function and divided by 255 to normalize it to the range of 0–1.</p>



<p>Next&#8230;  </p>



<p>YOLOv3 has 5 layers types in general, they are: &#8220;<em>convolutional</em> layer&#8221;, &#8220;<em>upsample</em> layer&#8221;, &#8220;<em>route</em> layer&#8221;, &#8220;<em>shortcut</em> layer&#8221;, and &#8220;<em>yolo</em> layer&#8221;. </p>



<p>The following code performs an iteration over the list <code>blocks</code>. For every iteration, we check the type of the block which corresponds to the type of layer.   </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="40" data-enlighter-title="" data-enlighter-group="">    for i, block in enumerate(blocks[1:]):</pre>



<h4>Convolutional Layer</h4>



<p>In YOLOv3, there are 2 convolutional layer types, i.e with and without batch normalization layer. The convolutional layer followed by <em>a batch normalization layer</em> uses <em>a leaky ReLU activation layer</em>, otherwise, it uses the linear activation.  So, we must handle them for every single iteration we perform.  </p>



<p>This is the code to perform the convolutional layer.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="41" data-enlighter-title="" data-enlighter-group="">        # If it is a convolutional layer
        if (block["type"] == "convolutional"):

            activation = block["activation"]
            filters = int(block["filters"])
            kernel_size = int(block["size"])
            strides = int(block["stride"])

            if strides > 1:
                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)

            inputs = Conv2D(filters,
                            kernel_size,
                            strides=strides,
                            padding='valid' if strides > 1 else 'same',
                            name='conv_' + str(i),
                            use_bias=False if ("batch_normalize" in block) else True)(inputs)

            if "batch_normalize" in block:
                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)
                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)</pre>



<p>Line 42,  we check whether the type of the block is a convolutional block, if it is true then read the attributes associated with it, otherwise, go check for another type ( we&#8217;ll be explaining after this).  In the convolutional block, you&#8217;ll find the following attributes: <em>batch_normalize</em>, <em>activation</em>, <em>filters</em>, <em>pad</em>, <em>size</em>, and <em>stride</em>. For more details, what attributes are in the convolutional blocks, you can open the file <code>yolov3.cfg</code>. </p>



<p>Lines 49-50, verify whether the <code>stride</code>is greater than 1, if it is true, then downsampling is performed, so we need to adjust the padding.</p>



<p> Lines 59-61, if we find <code>batch_normalize</code>in a block,  then add layers <em>BatchNormalization </em>and  <em>LeakyReLU</em>,  otherwise, do nothing.  </p>



<h4>Upsample Layer</h4>



<p>Now, we&#8217;re going to continue <code>if..else</code> case above. Here, we&#8217;re going to check for the <code>upsample layer</code>.  The upsample layer performs upsampling of the previous feature map by a factor of <code>stride</code>. To do this, YOLOv3 uses bilinear upsampling method.<br>So, if we find upsample block, retrieve the <code>stride </code>value and add a layer <code>UpSampling2D</code> by specifying the stride value.</p>



<p>The following is the code for that.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="64" data-enlighter-title="" data-enlighter-group="">        elif (block["type"] == "upsample"):
            stride = int(block["stride"])
            inputs = UpSampling2D(stride)(inputs)</pre>



<h4>Route Layer</h4>



<p>The route block contains an attribute <code>layers</code> which holds one or two values.  For more details, please look at the file <code>yolov3.cfg</code> and point to lines 619-634. There, you will find the following lines.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="619" data-enlighter-title="" data-enlighter-group="">[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 61</pre>



<p>I&#8217;ll explain a little bit about the above lines of <code>yolov3.cfg</code>.</p>



<p>In the line 620 above, the attribute <code>layers</code> holds a value of -4 which means that if we are in this route block, we need to backward 4 layers and then output the feature map from that layer. However, for the case of the route block whose attribute <code>layers</code> has 2 values like in lines 633-634, <code>layers</code> contains -1 and 61, we need to concatenate the feature map from a previous layer (-1) and the feature map from layer 61. So, the following is the code for the <code>route</code> layer.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="68" data-enlighter-title="" data-enlighter-group="">        # If it is a route layer
        elif (block["type"] == "route"):
            block["layers"] = block["layers"].split(',')
            start = int(block["layers"][0])

            if len(block["layers"]) > 1:
                end = int(block["layers"][1]) - i
                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index
                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)
            else:
                filters = output_filters[i + start]
                inputs = outputs[i + start]</pre>



<h4>Shortcut Layer</h4>



<p>In this layer, we perform skip connection. If we look at the file <code>yolov3.cfg</code>, this block contains an attribute <code>from</code> as shown below.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="60" data-enlighter-title="" data-enlighter-group="">[shortcut]
from=-3
activation=linear</pre>



<p>What we&#8217;re going to do in this layer block is to backward  3 layers (-3) as indicated in <code>from</code> value, then take the feature map from that layer, and add it with the feature map from the previous layer. Here is the code for that.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="81" data-enlighter-title="" data-enlighter-group="">        elif block["type"] == "shortcut":
            from_ = int(block["from"])
            inputs = outputs[i - 1] + outputs[i + from_]</pre>



<h4>Yolo Layer</h4>



<p>Here, we perform our detection and do some refining to the bounding boxes. If you have any difficulty understanding or have a problem with this part, just check out my previous <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">post </a>(part-1 of this tutorial). </p>



<p>As we did to other layers, just check whether we&#8217;re in the yolo layer. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="85" data-enlighter-title="" data-enlighter-group="">        # Yolo detection layer
        elif block["type"] == "yolo":</pre>



<p>If it is true, then take all the necessary attributes associated with it. In this case, we just need <code>mask</code> and <code>anchors</code> attributes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="88" data-enlighter-title="" data-enlighter-group="">            mask = block["mask"].split(",")
            mask = [int(x) for x in mask]
            anchors = block["anchors"].split(",")
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]
            n_anchors = len(anchors)</pre>



<p>Then we need to reshape the YOLOv3 output to the form of  [<code>None</code>, B  * <code>grid size</code> * <code>grid size</code>, 5 + <code>C</code>]. The <em>B</em> is the number of anchors and <em>C</em> is the number of classes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="96" data-enlighter-title="" data-enlighter-group="">            out_shape = inputs.get_shape().as_list()

            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \
										 5 + num_classes])</pre>



<p>Then access all boxes attributes by this way:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="101" data-enlighter-title="" data-enlighter-group="">            box_centers = inputs[:, :, 0:2]
            box_shapes = inputs[:, :, 2:4]
            confidence = inputs[:, :, 4:5]
            classes = inputs[:, :, 5:num_classes + 5]</pre>



<h4>Refine Bounding Boxes</h4>



<p>As I mentioned in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#unique-identifier2" target="_blank">part 1</a> that after the YOLOv3 network outputs the bounding boxes prediction, we need to refine them in order to the have the right positions and shapes. </p>



<p>Use the sigmoid function to convert <code>box_centers</code>, <code>confidence</code>, and <code>classes</code> values into range of 0 &#8211; 1.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="106" data-enlighter-title="" data-enlighter-group="">            box_centers = tf.sigmoid(box_centers)
            confidence = tf.sigmoid(confidence)
            classes = tf.sigmoid(classes)</pre>



<p>Then convert <code>box_shapes</code> as the following:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="110" data-enlighter-title="" data-enlighter-group="">            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])
            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)</pre>



<p>Use a <code>meshgrid </code>to convert the relative positions of the center boxes into the real positions.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="113" data-enlighter-title="" data-enlighter-group="">            x = tf.range(out_shape[1], dtype=tf.float32)
            y = tf.range(out_shape[2], dtype=tf.float32)

            cx, cy = tf.meshgrid(x, y)
            cx = tf.reshape(cx, (-1, 1))
            cy = tf.reshape(cy, (-1, 1))
            cxy = tf.concat([cx, cy], axis=-1)
            cxy = tf.tile(cxy, [1, n_anchors])
            cxy = tf.reshape(cxy, [1, -1, 2])

            strides = (input_image.shape[1] // out_shape[1], \
                       input_image.shape[2] // out_shape[2])
            box_centers = (box_centers + cxy) * strides</pre>



<p>Then, concatenate them all together.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="127" data-enlighter-title="" data-enlighter-group="">            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)</pre>



<p><strong><code>Big note:</code> </strong>Just to remain you that YOLOv3 does 3 predictions across the scale. We do as it is. </p>



<p>Take the prediction result for each scale and concatenate it with the others.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="129" data-enlighter-title="" data-enlighter-group="">            if scale:
                out_pred = tf.concat([out_pred, prediction], axis=1)
            else:
                out_pred = prediction
                scale = 1</pre>



<p>Since the <em>route</em> and <em>shortcut</em> layers need output feature maps from previous layers, so for every iteration, we always keep the track of the feature maps and output filters.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="135" data-enlighter-title="" data-enlighter-group="">        outputs[i] = inputs
        output_filters.append(filters)</pre>



<p>Finally, we can return our model.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="138" data-enlighter-title="" data-enlighter-group="">    model = Model(input_image, out_pred)
    model.summary()
    return model</pre>



<h3>The Complete Code of the <code>yolov3.py</code></h3>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#yolov3.py
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import BatchNormalization, Conv2D, \
    Input, ZeroPadding2D, LeakyReLU, UpSampling2D




def parse_cfg(cfgfile):
    with open(cfgfile, 'r') as file:
        lines = [line.rstrip('\n') for line in file if line != '\n' and line[0] != '#']
    holder = {}
    blocks = []
    for line in lines:
        if line[0] == '[':
            line = 'type=' + line[1:-1].rstrip()
            if len(holder) != 0:
                blocks.append(holder)
                holder = {}
        key, value = line.split("=")
        holder[key.rstrip()] = value.lstrip()
    blocks.append(holder)
    return blocks


def YOLOv3Net(cfgfile, model_size, num_classes):

    blocks = parse_cfg(cfgfile)

    outputs = {}
    output_filters = []
    filters = []
    out_pred = []
    scale = 0

    inputs = input_image = Input(shape=model_size)
    inputs = inputs / 255.0

    for i, block in enumerate(blocks[1:]):
        # If it is a convolutional layer
        if (block["type"] == "convolutional"):

            activation = block["activation"]
            filters = int(block["filters"])
            kernel_size = int(block["size"])
            strides = int(block["stride"])

            if strides > 1:
                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)

            inputs = Conv2D(filters,
                            kernel_size,
                            strides=strides,
                            padding='valid' if strides > 1 else 'same',
                            name='conv_' + str(i),
                            use_bias=False if ("batch_normalize" in block) else True)(inputs)

            if "batch_normalize" in block:
                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)
            #if activation == "leaky":
                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)

        elif (block["type"] == "upsample"):
            stride = int(block["stride"])
            inputs = UpSampling2D(stride)(inputs)

        # If it is a route layer
        elif (block["type"] == "route"):
            block["layers"] = block["layers"].split(',')
            start = int(block["layers"][0])

            if len(block["layers"]) > 1:
                end = int(block["layers"][1]) - i
                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index
                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)
            else:
                filters = output_filters[i + start]
                inputs = outputs[i + start]

        elif block["type"] == "shortcut":
            from_ = int(block["from"])
            inputs = outputs[i - 1] + outputs[i + from_]

        # Yolo detection layer
        elif block["type"] == "yolo":

            mask = block["mask"].split(",")
            mask = [int(x) for x in mask]
            anchors = block["anchors"].split(",")
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]
            n_anchors = len(anchors)

            out_shape = inputs.get_shape().as_list()

            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \
										 5 + num_classes])

            box_centers = inputs[:, :, 0:2]
            box_shapes = inputs[:, :, 2:4]
            confidence = inputs[:, :, 4:5]
            classes = inputs[:, :, 5:num_classes + 5]

            box_centers = tf.sigmoid(box_centers)
            confidence = tf.sigmoid(confidence)
            classes = tf.sigmoid(classes)

            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])
            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)

            x = tf.range(out_shape[1], dtype=tf.float32)
            y = tf.range(out_shape[2], dtype=tf.float32)

            cx, cy = tf.meshgrid(x, y)
            cx = tf.reshape(cx, (-1, 1))
            cy = tf.reshape(cy, (-1, 1))
            cxy = tf.concat([cx, cy], axis=-1)
            cxy = tf.tile(cxy, [1, n_anchors])
            cxy = tf.reshape(cxy, [1, -1, 2])

            strides = (input_image.shape[1] // out_shape[1], \
                       input_image.shape[2] // out_shape[2])
            box_centers = (box_centers + cxy) * strides

            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)

            if scale:
                out_pred = tf.concat([out_pred, prediction], axis=1)
            else:
                out_pred = prediction
                scale = 1

        outputs[i] = inputs
        output_filters.append(filters)

    model = Model(input_image, out_pred)
    model.summary()
    return model</pre>



<p>That&#8217;s it for part 2 and see you in <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">part 3</a>.</p>



<h2>Parts:</h2>



<ul>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part--2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p>
</li>
</ul>



<p>Credit link:<br><a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a><br></p>



<p> </p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/feed/</wfw:commentRss>
			<slash:comments>31</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-1</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 13:18:14 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[implementation yolov3]]></category>
		<category><![CDATA[implementation yolov3 python]]></category>
		<category><![CDATA[Yolo TensorFlow]]></category>
		<category><![CDATA[Yolo TensorFlow 2]]></category>
		<category><![CDATA[Yolo TensorFlow 2.0]]></category>
		<category><![CDATA[yolov3 implementation]]></category>
		<category><![CDATA[yolov3 python]]></category>
		<category><![CDATA[yolov3 python tensorflow 2]]></category>
		<category><![CDATA[yolov3 tutorial python]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=5</guid>

					<description><![CDATA[<p>In this tutorial, I'll be sharing how to implement the YOLOv3 object detector using TensorFlow 2 in the simplest way. Without over complicating things, you will discover how easy is to build a YOLOv3 object detector in TensorFlow 2.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<h2>Tutorial Overview</h2>



<p></p>



<h3>What is this post about? </h3>



<p>Over the past few years in Machine learning, we&#8217;ve seen dramatic progress in the field of object detection. Although there are several different models of object detection, in this post, we&#8217;re going to discuss specifically one model called &#8220;You Only Look Once&#8221; or in short YOLO.  </p>



<p>Invented by  Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi (2015), YOLO has already 3 different versions so far. But in this post, we&#8217;are going to focus on the latest version only, that is YOLOv3. So here, you&#8217;ll be discovering how to implement the YOLOv3 algorithm in TensorFlow 2.0 in the simplest way. </p>



<p>For more details about how to install TensorFlow 2.0, you can follow my previous tutorial <a href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">here</a>.</p>



<h3>Who is this tutorial for?</h3>



<p>When I got started learning YOLO a few years ago, I found that it was really difficult for me to understand both the concept and implementation.  Even though there are tons of blog posts and GitHub repos about it, most of them are presented in complex architectures.</p>



<p>I pushed myself to learn them one after another and it ended me up to debug every single code, step by step, in order to grasp the core of the YOLO concept. Fortunately, I didn’t give up. After spending a lot of time, I finally made it works. </p>



<p>Based on that experience, I tried to make this tutorial easy and useful for many beginners who just got started learning object detection.  Without over-complicating things, this tutorial can be a simple explanation of  YOLOv3’s implementation in TensorFlow 2.0.</p>



<h3 id="prerequisites">Prerequisites</h3>



<ul><li>Familiar with Python 3</li><li>Understand object detection and Convolutional Neural Networks (CNNs).  </li><li>Basic TensorFlow usage.</li></ul>



<h3>What will you get after completing this tutorial?</h3>



<p>After completing this tutorial, you will understand the principle of YOLOv3 and know how to implement it in TensorFlow 2.0. I believe this tutorial will be useful for a beginner who just got started learning object detection.</p>



<p>This tutorial is broken into 4 parts, they are: </p>



<ol><li><a href="https://machinelearningspace.com//yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></li><li><a rel="noreferrer noopener" href="https://medium.com/@rahmadsadli/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2-eb2551eef3d6" target="_blank">P</a><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">art-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></li><li><a rel="noreferrer noopener" href="https://medium.com/@rahmadsadli/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-3-2a48f6a06f0a" target="_blank">Pa</a><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">rt-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></li><li><a href="https://machinelearningspace.com/yolo-v3-in-tensorflow-2-0-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></li></ol>



<p>Now, it’s time to get started on this tutorial with a brief overview of everything that we’ll be seeing in this post.</p>



<h2>YOLO: YOLOv3</h2>



<p>Initially, for those of you who don’t have a lot of prior experience with this topic, I’m going to do a brief introduction about YOLOv3 and how the algorithm actually works.</p>



<p>As its name suggests, YOLO – You Only Look Once, it applies a single forward pass neural network to the whole image and predicts the bounding boxes and their class probabilities as well. This technique makes YOLO a super-fast real-time object detection algorithm. </p>



<p>As mentioned in the original paper (<em>the link is provided at the end of this part</em>), YOLOv3 has 53 convolutional layers called Darknet-53 as you can see in the following figure. </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2.png" alt="YoloV3 Darknet 53" class="wp-image-191" width="393" height="480" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2.png 685w, https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2-246x300.png 246w" sizes="(max-width: 393px) 100vw, 393px" /></figure></div>



<h3>How YOLOv3 works?</h3>



<p>The YOLOv3 network divides an input image into <em>S</em> x <em>S</em> grid of cells and predicts bounding boxes as well as class probabilities for each grid. Each grid cell is responsible for predicting <em>B</em> bounding boxes and <em>C</em> class probabilities of objects whose centers fall inside the grid cell. Bounding boxes are the regions of interest (ROI) of the candidate objects. The &#8220;<em>B</em>&#8221; is associated with the number of using anchors. Each bounding box has (<em>5</em> + <em>C</em>) attributes. The value of &#8220;<em>5</em>&#8221; is related to <em>5</em> bounding box attributes, those are<em> center coordinates</em> (b<sub>x</sub>, b<sub>y</sub>) and <em>shape </em>(b<sub>h</sub>, b<sub>w</sub>) of the bounding box, and one <em>confidence score</em>.  The &#8220;<em>C</em>&#8221; is the number of classes.   The confidence score reflects how confidence a box contains an object. The confidence score is in the range of 0 &#8211; 1. We&#8217;ll be talking this confidence score in more detail in the section <a href="#nms-unique"><em>Non-Maximum Suppression</em> (<em>NMS</em>)</a>. </p>



<p>Since we have <em>S</em> x <em>S</em> grid of cells, after running a single forward pass convolutional neural network to the whole image, YOLOv3 produces a 3-D tensor with the shape of [<em>S</em>, <em>S</em>, <em>B</em> * (5 + <em>C</em>]. </p>



<p>The following figure illustrates the basic principle of YOLOv3  where the input image is divided into the 13 x 13 grid of cells (<code>13 x 13 grid of cells is used for the first scale,  whereas YOLOv3 actually uses 3 different scales and we're going to discuss it in the section <a href="#unique-identifier"><em>prediction across scale</em></a></code>).  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" loading="lazy" width="868" height="1024" src="https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png" alt="yolov3 tensorflow 2" class="wp-image-177" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png 868w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-254x300.png 254w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-768x906.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2.png 1233w" sizes="(max-width: 868px) 100vw, 868px" /></figure></div>



<p>YOLOv3 was trained on the COCO dataset with <em>C</em>=80 and <em>B</em>=3.   So, for the first prediction scale, after a single forward pass of CNN, the YOLOv3 outputs a tensor with the shape of [(13, 13, 3 * (5 + 80)].   </p>



<h3>Anchor Box Algorithm </h3>



<p>Basically, one grid cell can detect only one object whose mid-point of the object falls inside the cell, but what about if a grid cell contains more than one mid-point of the objects?.  That means there are multiple objects overlapping. In order to overcome this condition, YOLOv3 uses 3 different anchor boxes for every detection scale. </p>



<p>The anchor boxes are a set of pre-defined bounding boxes of a certain height and width that are used to capture the scale and different aspect ratio of specific object classes that we want to detect.  </p>



<p>While there are 3 predictions across scale, so the total anchor boxes are 9, they are: (10×13), (16×30), (33×23) for the first scale, (30×61), (62×45), (59×119)  for the second scale, and (116×90), (156×198), (373×326)  for the third scale. </p>



<p>A clear explanation of the anchor box&#8217;s concept can be found in Andrew NG&#8217;s video <a href="https://www.youtube.com/watch?v=RTlwl2bv0Tg">here</a>.</p>



<h3 id="unique-identifier">Prediction Across Scale</h3>



<p>YOLOv3 makes detection in 3 different scales in order to accommodate different objects size by using strides of 32, 16 and 8. This means, if we feed an input image of size 416 x 416, YOLOv3 will make detection on the scale of 13 x 13, 26 x 26, and 52 x 52.  </p>



<p>For the first scale, YOLOv3 downsamples the input image into 13 x 13 and makes a prediction at the 82nd layer.  The 1st detection scale yields a 3-D tensor of size 13 x 13 x 255. </p>



<p>After that, YOLOv3 takes the feature map from layer 79 and applies one convolutional layer before upsampling it by a factor of 2 to have a size of 26 x 26. This upsampled feature map is then concatenated with the feature map from layer 61. The concatenated feature map is then subjected to a few more convolutional layers until the 2nd detection scale is performed at layer 94.  The second prediction scale produces a 3-D tensor of size 26 x 26 x 255.</p>



<p>The same design is again performed one more time to predict the 3rd scale.   The feature map from layer 91 is added one convolutional layer and is then concatenated with a feature map from layer 36.   The final prediction layer is done at layer 106 yielding a 3-D tensor of size 52 x 52 x 255. </p>



<figure class="wp-block-image"><img decoding="async" loading="lazy" width="1024" height="569" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1024x569.png" alt="yolov3 tensorflow 2" class="wp-image-27" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1024x569.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-300x167.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-768x427.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1536x854.png 1536w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-540x300.png 540w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct.png 1901w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption> Source:<a rel="noreferrer noopener" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" target="_blank"> https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b</a> </figcaption></figure>



<p>Once again, YOLOv3 predicts over 3 different scales detection, so if we feed an image of size 416x 416, it produces 3  different output shape tensor,  13 x 13 x 255,  26 x 26 x 255, and  52 x 52 x 255. </p>



<h3 id="unique-identifier2">Bounding box Prediction </h3>



<p>For each bounding box, YOLO predicts 4 coordinates, <em>t</em><sub>x</sub>, <em>t</em><sub>y</sub>, <em>t</em><sub>w</sub>, <em>t</em><sub>h</sub>. The <em>t</em><sub>x</sub> and <em>t</em><sub>y</sub> are the bounding box&#8217;s center coordinate relative to the grid cell whose center falls inside, and the <em>t</em><sub>w</sub> and <em>t</em><sub>h</sub> are the bounding box&#8217;s shape, width and height, respectively. </p>



<p id="unique-identifier3">The final output of the bounding box predictions need to be refined based on this formula: </p>



<div class="wp-block-image"><figure class="aligncenter size-medium is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-300x246.png" alt="yolov3 tensorflow 2" class="wp-image-29" width="211" height="173" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-300x246.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-366x300.png 366w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1.png 582w" sizes="(max-width: 211px) 100vw, 211px" /></figure></div>



<p>The <em>p</em><sub>w</sub> and <em>p</em><sub>h</sub> are the anchor&#8217;s width and height, respectively. The figure below describes this transformation in more detail. </p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-300x225.png" alt="yolov3 tensorflow 2" class="wp-image-31" width="468" height="351" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-300x225.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-400x300.png 400w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression.png 720w" sizes="(max-width: 468px) 100vw, 468px" /><figcaption> Source:<a rel="noreferrer noopener" href="https://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html" target="_blank"> https://christopher5106.github.io [Bounding box object detectors: understanding YOLO, You Look Only Once]</a> </figcaption></figure></div>



<p>The YOLO algorithm returns bounding boxes in the form of (b<sub>x</sub>, b<sub>y</sub>, b<sub>w</sub>, b<sub>h</sub>). The b<sub>x</sub> and b<sub>y</sub> are the center coordinates of the boxes and b<sub>w</sub> and b<sub>h</sub> are the box shape (width and height). Generally, to draw boxes, we use the top-left coordinate (x<sub>1</sub>, y<sub>1</sub>) and the box shape (width and height). To do this just simply convert them using this simple relation: </p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox_formula2.png" alt="" class="wp-image-62" width="140" height="134"/></figure></div>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1.png" alt="yolov3 tensorflow 2" class="wp-image-57" width="289" height="182" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1.png 694w, https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1-300x189.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1-475x300.png 475w" sizes="(max-width: 289px) 100vw, 289px" /></figure></div>



<h3>Total Class Prediction </h3>



<p>Using the COCO dataset, YOLOv3 predicts 80 different classes. YOLO outputs bounding boxes and class prediction as well. If we split an image into a 13 x 13 grid of cells and use 3 anchors box, the total output prediction is 13 x 13 x 3 or 169 x 3. However, YOLOv3 uses 3 different prediction scales which splits an image into (13 x 13), (26 x 26) and (52 x 52) grid of cells and with 3 anchors for each scale. So, the total output prediction will be ([(13 x13) + (26&#215;26)+(52&#215;52)] x3) =10,647. </p>



<h3 id="nms-unique">Non-Maximum Suppression</h3>



<p>Actually, after single forward pass CNN, what&#8217;s going to happen is the YOLO network is trying to suggest multiple bounding boxes for the same detected object. The problem is how do we decide which one of these bounding boxes is the right one. </p>



<p>Fortunately, to overcome this problem, a method called non-maximum suppression (NMS) can be applied.  Basically, what NMS does is to clean up these detections.  The first step of NMS is to suppress all the predictions boxes where the confidence score is under a certain threshold value. Let&#8217;s say the confidence threshold is set to 0.5, so every bounding box where the confidence score is less than or equal to 0.5 will be discarded. </p>



<p>Yet, this method is still not sufficient to choose the proper bounding boxes, because not all unnecessary bounding boxes can be eliminated by this step, so then the second step of NMS is applied. The rest of the higher confidence scores are sorted from the highest to the lowest one, then highlight the bounding box with the highest score as the proper bounding box, and after that find all the other bounding boxes that have a high IOU (<em>intersection over union</em>) with this highlighted box.  Let&#8217;s say we&#8217;ve set the IOU threshold to 0.5, so every bounding box that has IOU greater than 0.5 must be removed because it has a high IOU that corresponds to the same highlighted object. This method allows us to output only one proper bounding box for a detected object. Repeat this process for the remaining bounding boxes and always highlight the highest score as an appropriate bounding box. Do the same step until all bounding boxes are selected properly.</p>



<h2>End Notes</h2>



<p> Here’s a brief summary of what we have covered in this part: </p>



<ul><li>YOLO applies a single neural network to the whole image and predicts the bounding boxes and class probabilities as well. This makes YOLO a super-fast real-time object detection algorithm.</li><li>YOLO divides an image into SxS grid cells. Every cell is responsible for detecting an object whose center falls inside.</li><li>To overcome the overlapping objects whose centers fall in the same grid cell, YOLOv3 uses anchor boxes.</li><li>To facilitate the prediction across scale, YOLOv3 uses three different numbers of grid cell sizes (13&#215;13), (26&#215;26), and (52&#215;52).</li><li>A Non-Max Suppression is used to eliminate the overlapping boxes and keep only the accurate one.</li></ul>



<p>If I missed something or you have any questions, please don&#8217;t hesitate to let me know in the comments section. </p>



<p>After a brief introduction, now it&#8217;s time for us to jump into the technical details. So, let&#8217;s go get <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2/"><strong>part-2</strong></a>. </p>



<h2>Parts</h2>



<ul><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos.</a></p></li></ul>



<h3>Links to the original YOLO&#8217;s papers:</h3>



<ul><li>v1, You Only Look Once: Unified, Real-Time Object Detection <a rel="noreferrer noopener" href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></li><li>v2, YOLO9000: Better, Faster, Stronger <a rel="noreferrer noopener" href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a> </li><li>v3, YOLOv3: An Incremental Improvement <a rel="noreferrer noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li></ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/feed/</wfw:commentRss>
			<slash:comments>29</slash:comments>
		
		
			</item>
		<item>
		<title>Installing TensorFlow 2.0 in Anaconda Environment</title>
		<link>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=installing-tensorflow-2-0-in-anaconda-environment</link>
					<comments>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 07:49:02 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[Anaconda]]></category>
		<category><![CDATA[Easy Install Anaconda]]></category>
		<category><![CDATA[How to Install Anacoda]]></category>
		<category><![CDATA[Python]]></category>
		<category><![CDATA[TensorFlow 2]]></category>
		<category><![CDATA[TensorFlow 2.0]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=1324</guid>

					<description><![CDATA[<p>TensorFlow is still one of the popular Deep learning frameworks. It has been used in many different fields of applications including handwritten digit classification, image recognition, object detection, word embeddings, and natural language processing (NLP). In September last year, 2019, Google finally announced the availability of the final release of TensorFlow 2.0. With eager execution [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">Installing TensorFlow 2.0 in Anaconda Environment</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="640" src="https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-1024x640.jpg" alt="TensorFlow 2 in Anaconda" class="wp-image-1326" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-1024x640.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-300x188.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-768x480.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-480x300.jpg 480w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image.jpg 1080w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>TensorFlow is still one of the popular Deep learning frameworks. It has been used in many different fields of applications including handwritten digit classification, image recognition, object detection, word embeddings, and natural language processing (NLP).</p>



<p>In September last year, 2019, Google finally announced the availability of the final release of TensorFlow 2.0. With eager execution by default and tight integration with Keras, now TensorFlow 2.0 makes the development of machine learning applications much easier than before. </p>



<p>We can now easily debug TensorFlow&#8217;s variables and print their values just like in the standard Python. That&#8217;s way, TensorFlow 2.0 is more friendly than the older version 1.x. </p>



<p>For those of you who don&#8217;t have prior experience with this topic, this post is special for you. Here, I&#8217;m going to show you how to install TensorFlow 2.0 in Anaconda.</p>



<h2>What is Anaconda and why I recommend it?   </h2>



<p>Anaconda is a Python-based data processing built for data science. It comes with many useful built-in third-party libraries.  Installing Anaconda meaning installing Python with some commonly used libraries such as Numpy, Pandas, Scrip, and Matplotlib.  </p>



<p>For a Python developer or a data science researcher, using Anaconda has a lot of advantages, such as independently installing/updating packages without ruining the system.  So, we no need to worry about the system library or anything like that. This can save time and energy for other things. </p>



<p>Anaconda can be used across different platforms, Windows, macOS, and Linux. If we want to use a different Python version or package libraries, just create a different environment and play around without any risk of crashing the system library. </p>



<p>Now, let&#8217;s install Anaconda first.</p>



<h2><strong>Installing Anaconda</strong></h2>



<p>Anaconda is available for Windows, Mac OS X, and Linux, you can find the installation file in <a href="https://www.anaconda.com/distribution/">the anaconda official site</a>.  I suggest you choose the Python version 3.7 64-bit installer if you have a 64-bit machine, otherwise choose the 32-bit installer, instead. If you need, you can easily install Python 2.7 versions later.  </p>



<p>In case you have already installed Python on your computer, don&#8217;t worry, it won&#8217;t ruin anything. Instead, the default Python used by your programs will be the one that comes with Anaconda. Go ahead and choose the appropriate version,  follow the instructions and install it. </p>



<p>I will let you explore it, but anyhow, if you have any problem, you can simply post a comment in the comment section and I will try to do my best for you.<br>(Note: For more details on how to use Anaconda, you can visit the Anaconda user guide <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands">here</a>).</p>



<p>Now, we&#8217;re going to create our first environment, but be sure that you&#8217;ve installed Anaconda on your computer.</p>



<h3>Creating an Environment</h3>



<p>Open Anaconda prompt, and create a new environment called <strong>yolov3_tf2</strong> ( I gave this name because it relates to my next article about <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-1/">the implementation of YOLOv3 in TensorFlow 2.0</a>).  You can name it whatever you want. Just type or copy the following command to your Anaconda prompt and hit Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda create -n yolov3_tf2 python=3.7</pre>



<p>After that, you will be prompted something like this, just type &#8216;<strong>y</strong>&#8216; and then hit the Enter. <br>Note: you might be prompted a bit different to this, it doesn&#8217;t matter just hit Enter, Anaconda will do the best for you. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="769" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-1024x769.png" alt="Installing Anaconda" class="wp-image-11" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-1024x769.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-300x225.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-768x577.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-399x300.png 399w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1.png 1085w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>Wait until all packages installed successfully, and then you can activate your new Anaconda environment. <br>Copy and paste this command to your Anaconda prompt and hit Enter. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda activate yolov3_tf2</pre>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="452" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-1024x452.png" alt="Activating Anaconda environment" class="wp-image-12" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-1024x452.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-300x132.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-768x339.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-550x243.png 550w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2.png 1085w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p> Now, your Conda&#8217;s environment is ready to use. Let&#8217;s install TensorFlow 2.0.   </p>



<h2>Installing TensorFlow 2.0</h2>



<p>When you are in the <strong>yolov3_tf2</strong> environment, now you can install any package you want. To install TensorFlow 2.0, type this command and hit Enter. </p>



<p>GPU: </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda install -c conda-forge tensorflow-gpu=2.0</pre>



<p>CPU:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda install -c conda-forge tensorflow=2.0</pre>



<p>Verify the Cuda toolkit and <code>cudnn</code> that will be installed, it must come with Cudatoolkit 10 and <code>cudnn 7.6</code>. If everything goes right, just type &#8216;y&#8217; and hit Enter. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="805" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-1024x805.png" alt="TensorFlow GPU" class="wp-image-13" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-1024x805.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-300x236.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-768x603.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-382x300.png 382w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3.png 1083w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>Basically, your TensorFlow has been installed now.  Let&#8217;s check whether it&#8217;s installed correctly or not.<br>Type <code><strong>python</strong></code> in Anaconda command prompt and hit Enter, your Python must be version <strong>3.7</strong>, then type <strong><code>import tensorflow as tf</code></strong>  and hit Enter, followed by typing <strong><code>tf.__version__</code></strong> and hit Enter. If you have TensorFlow installed on your environment, you&#8217;ll get no errors, otherwise, you&#8217;ll need to re-install it. </p>



<p>If everything has been installed correctly, you&#8217;ll get the result as shown in the figure below. Your TF version must be &#8216;2.0.0&#8217;. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="203" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-1024x203.png" alt="Verify TensorFlow installation" class="wp-image-14" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-1024x203.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-300x60.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-768x152.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-550x109.png 550w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4.png 1084w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>See you and check <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolov3-in-tensorflow-2-0-part-1/">this out</a>, my tutorial about  YOLOv3 object detection.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">Installing TensorFlow 2.0 in Anaconda Environment</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/feed/</wfw:commentRss>
			<slash:comments>71</slash:comments>
		
		
			</item>
	</channel>
</rss>
