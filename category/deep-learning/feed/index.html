<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Deep Learning &#8211; Machine Learning Space</title>
	<atom:link href="https://machinelearningspace.com/category/deep-learning/feed/" rel="self" type="application/rss+xml" />
	<link>https://machinelearningspace.com</link>
	<description>A space for learning machine learning</description>
	<lastBuildDate>Sat, 29 Oct 2022 22:36:55 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.1.1</generator>
	<item>
		<title>Introduction to Artiﬁcial Neural Networks (ANNs)</title>
		<link>https://machinelearningspace.com/introduction-to-arti%ef%ac%81cial-neural-networks-anns/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=introduction-to-arti%25ef%25ac%2581cial-neural-networks-anns</link>
					<comments>https://machinelearningspace.com/introduction-to-arti%ef%ac%81cial-neural-networks-anns/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 07 Feb 2020 10:28:41 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ANN]]></category>
		<category><![CDATA[Artificial Neural Network]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=1234</guid>

					<description><![CDATA[<p>Artiﬁcial Neural Networks (ANNs), inspired by the human brain system, are based on a collection of units of neurons that are connected one to another to process and send information. A very basic or a simplest neural network composes of only a single neuron, some inputs and a bias b as illustrated in the following [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/introduction-to-arti%ef%ac%81cial-neural-networks-anns/">Introduction to Artiﬁcial Neural Networks (ANNs)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="683" src="https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-1024x683.jpg" alt="ANN" class="wp-image-1333" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-1024x683.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-300x200.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-768x512.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-1536x1024.jpg 1536w, https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-2048x1365.jpg 2048w, https://machinelearningspace.com/wp-content/uploads/2020/02/ANN-450x300.jpg 450w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>Artiﬁcial Neural Networks (ANNs), inspired by the human brain system, are based on a collection of units of neurons that are connected one to another to process and send information. </p>



<p>A very basic or a simplest neural network composes of only a single neuron, some inputs<strong> </strong><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-be75946bd8cfa6391ac02de4df4824dd_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#32;&#32;&#61;&#32;&#40;&#120;&#95;&#49;&#44;&#32;&#120;&#95;&#50;&#44;&#46;&#46;&#44;&#120;&#95;&#110;&#41;" title="Rendered by QuickLaTeX.com" height="18" width="136" style="vertical-align: -4px;"/>  and a bias <em>b</em> as illustrated in the following ﬁgure.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs-1024x413.jpg" alt="" class="wp-image-1241" width="403" height="162" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs-1024x413.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs-300x121.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs-768x310.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs-550x222.jpg 550w, https://machinelearningspace.com/wp-content/uploads/2020/02/simplest-NNs.jpg 1319w" sizes="(max-width: 403px) 100vw, 403px" /></figure></div>



<p>All the inputs and the bias are connected to this neuron. These connections are called the synapses where every synapse has the weight <em>W</em>. </p>



<p>The hypothesis output of this simplest neural network is written as:</p>



<p> <p class="ql-center-displayed-equation" style="line-height: 21px;"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-04bae29ddbfd8047c296ddd414f66cc8_l3.png" height="21" width="217" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#36;&#104;&#95;&#123;&#87;&#44;&#98;&#125;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#120;&#125;&#41;&#32;&#61;&#32;&#102;&#40;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#123;&#110;&#125;&#32;&#87;&#95;&#105;&#120;&#95;&#105;&#32;&#43;&#98;&#41;&#36;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>The function of <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-f5844370b6482674a233a3063f762555_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#102;" title="Rendered by QuickLaTeX.com" height="16" width="10" style="vertical-align: -4px;"/> is called the activation function. </p>



<p>There are many kinds of activation functions used in NNs implementation, the most commonly used are step function, sigmoid function, <code>tanh</code> and Rectifier Linear Unit (ReLu). </p>



<h2>Activation Function</h2>



<p>In the above description of the simplest neural network, it uses a sigmoid function as the activation function.</p>



<p>The sigmoid function is one of the popular activation functions used in the neural network systems. It is written by:</p>



<p> <p class="ql-center-displayed-equation" style="line-height: 38px;"><span class="ql-right-eqno"> (2) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-48c78ed60d6e81de1948723d9805a225_l3.png" height="38" width="117" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#102;&#40;&#122;&#41;&#61;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#49;&#43;&#101;&#94;&#123;&#45;&#122;&#125;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>It is important to be noticed that there are other common choices of the activation functions, they are hyperbolic tangent or <code>tanh</code> and rectiﬁed linear unit (ReLU).</p>



<p>The <code>tanh</code> function is written as:</p>



<p> <p class="ql-center-displayed-equation" style="line-height: 39px;"><span class="ql-right-eqno"> (3) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-794c14482dd826d8889aa70c15751a1e_l3.png" height="39" width="207" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#102;&#40;&#122;&#41;&#61;&#116;&#97;&#110;&#104;&#40;&#122;&#41;&#61;&#92;&#102;&#114;&#97;&#99;&#123;&#101;&#94;&#123;&#122;&#125;&#45;&#101;&#94;&#123;&#45;&#122;&#125;&#125;&#123;&#101;&#94;&#123;&#122;&#125;&#43;&#101;&#94;&#123;&#45;&#122;&#125;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>The rectiﬁed linear activation function is given by:</p>



<p> <p class="ql-center-displayed-equation" style="line-height: 18px;"><span class="ql-right-eqno"> (4) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-de856b7b1307819b99e5dbf8999ecf8d_l3.png" height="18" width="132" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#102;&#40;&#122;&#41;&#61;&#109;&#97;&#120;&#40;&#48;&#44;&#120;&#41;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>In practice, for deep Neural Networks,  rectiﬁed linear function often works better than the<code>sigmoid </code>and the <code>tanh </code>functions. </p>



<p>The following figure shows the plots of the sigmoid, tanh and rectiﬁed linear functions (ReLU).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/02/Activation_functions.jpg" alt="" class="wp-image-1246" width="440" height="330" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/Activation_functions.jpg 700w, https://machinelearningspace.com/wp-content/uploads/2020/02/Activation_functions-300x225.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/Activation_functions-400x300.jpg 400w" sizes="(max-width: 440px) 100vw, 440px" /></figure></div>



<h2>Multi-Layer Neural Network</h2>



<p>The simplest neural network described above is a very limited model. To form a multi-layer neural network, we can hook together the simple neurons. The output of a neuron can be the input of another.  </p>



<p>The following figure shows a simple multi-layer neural network with two hidden layers. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-1024x570.jpg" alt="" class="wp-image-1248" width="485" height="269" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-1024x570.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-300x167.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-768x428.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-1536x856.jpg 1536w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-2048x1141.jpg 2048w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN-539x300.jpg 539w" sizes="(max-width: 485px) 100vw, 485px" /></figure></div>



<p>This network has four layers with two inputs <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-d7aa45c8899989487fb32dab51a8f7d7_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;&#95;&#49;" title="Rendered by QuickLaTeX.com" height="12" width="16" style="vertical-align: -4px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-de02251c2c969c17b8633e299d9a2244_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#120;&#95;&#50;" title="Rendered by QuickLaTeX.com" height="11" width="17" style="vertical-align: -3px;"/>  in the input layer (layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-ce3c62f3486a988a529a52bedaec2bc9_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#49;" title="Rendered by QuickLaTeX.com" height="16" width="18" style="vertical-align: -4px;"/>) and one output in the output layer (layer  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a28f1bdddb512df7a5b0ae2d61acca6b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#52;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> ). It has two hidden layers, layer  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a84cb3ce6c36a1bd1f3c19c9a488afeb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#50;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> and layer  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-61b145dc3f94523f1a7f0c630925068a_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#51;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/>. The circles labeled &#8220;+1&#8221; are called bias units that correspond to the intercept term.</p>



<h2>Feed Forward Propagation</h2>



<p>Now, we&#8217;re going to analyze this multi-layer neural network.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok-1024x602.jpg" alt="" class="wp-image-1251" width="456" height="267" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok-1024x602.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok-300x176.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok-768x451.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok-510x300.jpg 510w, https://machinelearningspace.com/wp-content/uploads/2020/02/Multi_NN2_ok.jpg 1446w" sizes="(max-width: 456px) 100vw, 456px" /></figure></div>



<p>We write  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-24f739a819bef683f2a4d477381fe1b1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="28" width="33" style="vertical-align: -8px;"/> to denote the parameter (or weight) associated with the connection between unit    <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-b09880662630fc49b25d42badb906d51_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#106;" title="Rendered by QuickLaTeX.com" height="16" width="9" style="vertical-align: -4px;"/> in layer    <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-b6280786295cb6f54ae2e0ac2b803e5e_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;" title="Rendered by QuickLaTeX.com" height="13" width="5" style="vertical-align: 0px;"/>, and unit <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-c62021ff05f50664cb2d22c418c3d46c_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;&#43;&#49;" title="Rendered by QuickLaTeX.com" height="15" width="35" style="vertical-align: -2px;"/>. The number of layers in the network is denoted by <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e295917775cc894394ddc47bf84772b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#110;&#95;&#108;" title="Rendered by QuickLaTeX.com" height="11" width="15" style="vertical-align: -3px;"/>. So, the above multi-layer neural network model has <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e295917775cc894394ddc47bf84772b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#110;&#95;&#108;" title="Rendered by QuickLaTeX.com" height="11" width="15" style="vertical-align: -3px;"/>=4.</p>



<p>The activation of unit <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-b6280786295cb6f54ae2e0ac2b803e5e_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;" title="Rendered by QuickLaTeX.com" height="13" width="5" style="vertical-align: 0px;"/> is denoted by <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-07c44ee40d2395a54cb3cc0e76964fb8_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#97;&#95;&#105;&#94;&#32;&#123;&#40;&#108;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="25" width="23" style="vertical-align: -5px;"/>. For example: for <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-563f7c661c83403255313760fa0a113e_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;&#61;&#49;" title="Rendered by QuickLaTeX.com" height="14" width="37" style="vertical-align: -1px;"/>, we denote <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-424bd4b959619b67d0094eb7cc0f58e1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#97;&#95;&#105;&#94;&#123;&#40;&#49;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="25" width="25" style="vertical-align: -5px;"/>  for activation of unit <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer 1. </p>



<p>We also use <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-58725d473f47be3e9afaf60a7591797b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#97;&#95;&#105;&#94;&#123;&#40;&#49;&#41;&#125;&#61;&#32;&#120;&#95;&#105;" title="Rendered by QuickLaTeX.com" height="25" width="66" style="vertical-align: -5px;"/> to denote the <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/>th input. </p>



<p>Given a fixed setting of the parameters <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-183777ab9133546b80b6f342c6ec9919_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#87;" title="Rendered by QuickLaTeX.com" height="12" width="19" style="vertical-align: 0px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-ad69adf868bc701e561aa555db995f1f_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#98;" title="Rendered by QuickLaTeX.com" height="13" width="8" style="vertical-align: 0px;"/>, our neural network defines a hypothesis <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6fe7422fcddd84af75cef1391198b9dd_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#104;&#95;&#123;&#87;&#44;&#98;&#125;&#40;&#120;&#41;" title="Rendered by QuickLaTeX.com" height="20" width="56" style="vertical-align: -6px;"/>. It outputs a real number. </p>



<p>Now, we are going to look particularly at the last two layers, <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-61b145dc3f94523f1a7f0c630925068a_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#51;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a28f1bdddb512df7a5b0ae2d61acca6b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#52;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/>. </p>



<p>The layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a28f1bdddb512df7a5b0ae2d61acca6b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#52;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> produces the output hypothesis and the layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-61b145dc3f94523f1a7f0c630925068a_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#51;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> is the last hidden layer. Specifically, the computation of the activation function in the layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-61b145dc3f94523f1a7f0c630925068a_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#51;" title="Rendered by QuickLaTeX.com" height="15" width="19" style="vertical-align: -3px;"/> can be derived as the following: </p>



<p> <p class="ql-center-displayed-equation" style="line-height: 58px;"><span class="ql-right-eqno"> (5) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-f90ba3f1130b538e02a07ebce373dd88_l3.png" height="58" width="362" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#97;&#95;&#49;&#94;&#123;&#40;&#51;&#41;&#125;&#61;&#102;&#40;&#87;&#95;&#123;&#49;&#49;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#49;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#87;&#95;&#123;&#49;&#50;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#50;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#32;&#87;&#95;&#123;&#49;&#51;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#51;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#32;&#98;&#95;&#49;&#94;&#123;&#40;&#50;&#41;&#125;&#41;&#92;&#92;&#32;&#97;&#95;&#50;&#94;&#123;&#40;&#51;&#41;&#125;&#61;&#102;&#40;&#87;&#95;&#123;&#50;&#49;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#49;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#87;&#95;&#123;&#50;&#50;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#50;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#32;&#87;&#95;&#123;&#50;&#51;&#125;&#94;&#123;&#40;&#50;&#41;&#125;&#97;&#95;&#51;&#94;&#123;&#40;&#50;&#41;&#125;&#43;&#32;&#98;&#95;&#50;&#94;&#123;&#40;&#50;&#41;&#125;&#41;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p> </p>



<p>The hypothesis of the output of this neural network can be written as:<br><a name="id3914530745"></a><p class="ql-center-displayed-equation" style="line-height: 27px;"><span class="ql-right-eqno"> (6) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7ad0013939cede680a1d613083301027_l3.png" height="27" width="302" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#104;&#95;&#123;&#119;&#98;&#125;&#32;&#40;&#120;&#41;&#61;&#32;&#97;&#95;&#49;&#94;&#123;&#40;&#52;&#41;&#125;&#32;&#61;&#102;&#40;&#87;&#95;&#123;&#49;&#49;&#125;&#94;&#123;&#40;&#51;&#41;&#125;&#97;&#95;&#49;&#94;&#123;&#40;&#51;&#41;&#125;&#32;&#43;&#32;&#87;&#95;&#123;&#49;&#50;&#125;&#94;&#123;&#40;&#51;&#41;&#125;&#32;&#97;&#95;&#50;&#94;&#123;&#40;&#51;&#41;&#125;&#41;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>If we let <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-0ab7d7c7c12e0937609f74fe2325eeec_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#122;&#95;&#123;&#105;&#125;&#94;&#123;&#40;&#108;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="25" width="23" style="vertical-align: -5px;"/> denoted as the total weighted sum of inputs to unit <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-b6280786295cb6f54ae2e0ac2b803e5e_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;" title="Rendered by QuickLaTeX.com" height="13" width="5" style="vertical-align: 0px;"/>, including the bias term, we will have:</p>



<p><a name="id1842349123"></a><p class="ql-center-displayed-equation" style="line-height: 53px;"><span class="ql-right-eqno"> (7) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-41433c78d2f61f6db30af210f92caba1_l3.png" height="53" width="234" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125; &#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125; &#122;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#61;&#92;&#115;&#117;&#109;&#95;&#123;&#106;&#61;&#49;&#125;&#94;&#110;&#32;&#32;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#45;&#49;&#41;&#125;&#97;&#95;&#106;&#94;&#123;&#40;&#108;&#45;&#49;&#41;&#125;&#43;&#32;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#45;&#49;&#41;&#125;&#92; &#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125; &#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>so that: <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-671627173f71adc2a8c30411094351bc_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#97;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#61;&#102;&#40;&#122;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#41;" title="Rendered by QuickLaTeX.com" height="25" width="97" style="vertical-align: -5px;"/></p>



<p>or, for the computation of the next layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-c62021ff05f50664cb2d22c418c3d46c_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;&#43;&#49;" title="Rendered by QuickLaTeX.com" height="15" width="35" style="vertical-align: -2px;"/> is:</p>



<p><a name="id11188519"></a><p class="ql-center-displayed-equation" style="line-height: 53px;"><span class="ql-right-eqno"> (8) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-ddf6a98a1b486b54a1ebf96b4b862c8e_l3.png" height="53" width="198" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#122;&#95;&#105;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#61;&#92;&#115;&#117;&#109;&#95;&#123;&#106;&#61;&#49;&#125;&#94;&#110;&#32;&#32;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#97;&#95;&#106;&#94;&#123;&#40;&#108;&#41;&#125;&#43;&#32;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>Using matrix-vectorial notation, the above equation <a href="#id11188519">8</a> can be written as follows:</p>



<p><a name="id3491373024"></a><p class="ql-center-displayed-equation" style="line-height: 21px;"><span class="ql-right-eqno"> (9) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-663bac64fec56309a15defdeeb961731_l3.png" height="21" width="175" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125; &#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125; &#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#122;&#125;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#61;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#87;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#43;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#98;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#92; &#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125; &#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>so that: </p>



<p><a name="id1456495395"></a><p class="ql-center-displayed-equation" style="line-height: 52px;"><span class="ql-right-eqno"> (10) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-9ebad15c965d52bafe774e329aef4378_l3.png" height="52" width="201" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#32;&#38;&#32;&#61;&#102;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#122;&#125;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#41;&#32;&#92;&#92;&#32;&#38;&#32;&#61;&#102;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#87;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#43;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#98;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>The output of <a href="#id3914530745">6</a> can be written as:<br> <a name="id3339912159"></a><p class="ql-center-displayed-equation" style="line-height: 82px;"><span class="ql-right-eqno"> (11) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-8f5ecca856f53da51bb548f9a05ba6ea_l3.png" height="82" width="217" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#104;&#95;&#123;&#119;&#98;&#125;&#32;&#40;&#120;&#41;&#32;&#38;&#32;&#61;&#32;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#94;&#123;&#40;&#52;&#41;&#125;&#32;&#92;&#92;&#32;&#38;&#32;&#61;&#102;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#122;&#125;&#94;&#123;&#40;&#52;&#41;&#125;&#41;&#32;&#92;&#92;&#32;&#38;&#32;&#61;&#32;&#102;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#87;&#125;&#94;&#123;&#40;&#51;&#41;&#125;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#97;&#125;&#94;&#123;&#40;&#51;&#41;&#125;&#43;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#98;&#125;&#94;&#123;&#40;&#51;&#41;&#125;&#41;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<h2>Back Propagation</h2>



<p>Suppose we have a fixed training set <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-dce77ccc8bdead37b7c4c488c3a6fa04_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#108;&#98;&#114;&#97;&#99;&#101;&#32;&#40;&#120;&#94;&#123;&#40;&#49;&#41;&#125;&#44;&#32;&#121;&#94;&#123;&#40;&#49;&#41;&#125;&#41;&#44;&#8230;&#46;&#44;&#40;&#120;&#94;&#123;&#40;&#109;&#41;&#125;&#44;&#32;&#121;&#94;&#123;&#40;&#109;&#41;&#125;&#41;&#32;&#92;&#114;&#98;&#114;&#97;&#99;&#101;" title="Rendered by QuickLaTeX.com" height="22" width="203" style="vertical-align: -5px;"/> of <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fdc40b8ad1cdad0aab9d632215459d28_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#109;" title="Rendered by QuickLaTeX.com" height="8" width="15" style="vertical-align: 0px;"/> training examples. We can train our neural network using batch gradient descent. For a single training example, the cost function with respect to single example is written as one-half square error as follows:</p>



<p><p class="ql-center-displayed-equation" style="line-height: 36px;"><span class="ql-right-eqno"> (12) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-978b41d90608022df414a33a0fd12bd9_l3.png" height="36" width="240" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125; &#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#32;&#92;&#86;&#101;&#114;&#116;&#32;&#104;&#95;&#123;&#87;&#44;&#98;&#125;&#32;&#40;&#120;&#41;&#45;&#121;&#32;&#92;&#86;&#101;&#114;&#116;&#94;&#50; &#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>Given a training set of <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fdc40b8ad1cdad0aab9d632215459d28_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#109;" title="Rendered by QuickLaTeX.com" height="8" width="15" style="vertical-align: 0px;"/> examples, we then define the overall cost function to be:</p>



<p><p class="ql-center-displayed-equation" style="line-height: 121px;"><span class="ql-right-eqno"> (13) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-dd2b19d1605e765e57aceaf206206f8b_l3.png" height="121" width="528" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#32;&#38;&#32;&#61;&#32;&#91;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#109;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#94;&#123;&#40;&#105;&#41;&#125;&#44;&#121;&#94;&#123;&#40;&#105;&#41;&#125;&#41;&#93;&#32;&#43;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#108;&#97;&#109;&#98;&#100;&#97;&#125;&#123;&#50;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#108;&#61;&#49;&#125;&#94;&#123;&#110;&#95;&#108;&#45;&#49;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#123;&#115;&#95;&#108;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#106;&#61;&#49;&#125;&#94;&#123;&#115;&#95;&#123;&#108;&#43;&#49;&#125;&#125;&#32;&#40;&#87;&#95;&#123;&#106;&#105;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#94;&#50;&#92;&#92;&#32;&#38;&#32;&#61;&#32;&#91;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#109;&#32;&#40;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#32;&#92;&#86;&#101;&#114;&#116;&#32;&#104;&#95;&#123;&#87;&#44;&#98;&#125;&#40;&#32;&#120;&#94;&#123;&#40;&#105;&#41;&#125;&#41;&#45;&#121;&#94;&#123;&#40;&#105;&#41;&#125;&#32;&#92;&#86;&#101;&#114;&#116;&#32;&#94;&#50;&#32;&#41;&#93;&#32;&#43;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#108;&#97;&#109;&#98;&#100;&#97;&#125;&#123;&#50;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#108;&#61;&#49;&#125;&#94;&#123;&#110;&#95;&#108;&#45;&#49;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#123;&#115;&#95;&#108;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#106;&#61;&#49;&#125;&#94;&#123;&#115;&#95;&#123;&#108;&#43;&#49;&#125;&#125;&#32;&#40;&#87;&#95;&#123;&#106;&#105;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#94;&#50;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>The first term is an average sum-of-squares error term. The second term is a regularization term (also called a weight decay term) that tends to decrease the magnitude of the weights, and helps prevent over-fitting. The weight decay parameter <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-8c37d2f1acb1d49f3e5e655797880475_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#108;&#97;&#109;&#98;&#100;&#97;" title="Rendered by QuickLaTeX.com" height="12" width="10" style="vertical-align: 0px;"/>  controls the relative importance of the two terms.</p>



<h3>Gradient Descent </h3>



<p>When we train the NNs, the goal is to minimize <em>J</em>(<em>W</em>,<em>b</em>) as a function of <em>W</em> and <em>b</em>. So basically, we initialize all parameters  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-24f739a819bef683f2a4d477381fe1b1_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="28" width="33" style="vertical-align: -8px;"/> and  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-2ecf6d2af655501494504f277cb37ef9_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#98;&#95;&#123;&#105;&#125;&#94;&#123;&#40;&#108;&#41;&#125;" title="Rendered by QuickLaTeX.com" height="25" width="21" style="vertical-align: -5px;"/>   to small random values and then apply the Gradient Descent algorithm to optimize these parameters.</p>



<p><br>To implement gradient descent algorithm, the parameters of the gradient  <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-cf20da74dbc51a3cf2245bbe75ea7e59_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#87;&#44;&#98;" title="Rendered by QuickLaTeX.com" height="17" width="32" style="vertical-align: -4px;"/> must be updated as follows:</p>



<p><p class="ql-center-displayed-equation" style="line-height: 104px;"><span class="ql-right-eqno"> (14) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-199e9f06f0588594e5e072781e54a8c8_l3.png" height="104" width="222" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#38;&#32;&#61;&#32;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#45;&#32;&#92;&#97;&#108;&#112;&#104;&#97;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#41;&#92;&#92;&#32;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#38;&#32;&#61;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#45;&#92;&#97;&#108;&#112;&#104;&#97;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#41;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>where <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-5f44d9bbc8046069be4aa2989bff19aa_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#97;&#108;&#112;&#104;&#97;" title="Rendered by QuickLaTeX.com" height="8" width="11" style="vertical-align: 0px;"/> is the learning rate. </p>



<p>To do back propagation completely,  the derivative of the overall cost function J(W,b) can be computed as:</p>



<p><p class="ql-center-displayed-equation" style="line-height: 116px;"><span class="ql-right-eqno"> (15) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-454ba1926375d9113b22ad6899828d47_l3.png" height="116" width="418" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#41;&#32;&#38;&#32;&#61;&#32;&#92;&#108;&#101;&#102;&#116;&#91;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#109;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#94;&#123;&#40;&#105;&#41;&#125;&#44;&#121;&#94;&#123;&#40;&#105;&#41;&#125;&#41;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#32;&#43;&#92;&#108;&#97;&#109;&#98;&#100;&#97;&#32;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#92;&#92;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#41;&#32;&#38;&#32;&#61;&#32;&#92;&#108;&#101;&#102;&#116;&#91;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#105;&#61;&#49;&#125;&#94;&#109;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#94;&#123;&#40;&#105;&#41;&#125;&#44;&#121;&#94;&#123;&#40;&#105;&#41;&#125;&#41;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></p>



<p>Detail of back propagation algorithm is:</p>



<ol><li>Perform a feedforward pass, computing the activations for layers <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-75206f65cdbe85990703ddcb6fea5f84_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#50;&#44;&#32;&#76;&#95;&#51;&#44;" title="Rendered by QuickLaTeX.com" height="16" width="51" style="vertical-align: -4px;"/> and so on up to the output layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e777d132d20d5bba887a66ec564b859_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#123;&#110;&#95;&#108;&#125;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: -5px;"/>.</li><li>For each output unit <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e295917775cc894394ddc47bf84772b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#110;&#95;&#108;" title="Rendered by QuickLaTeX.com" height="11" width="15" style="vertical-align: -3px;"/> (the output layer), set: <p class="ql-center-displayed-equation" style="line-height: 48px;"><span class="ql-right-eqno"> (16) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a374279bdd6fd96b0d4374cac415016d_l3.png" height="48" width="411" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#92;&#100;&#101;&#108;&#116;&#97;&#95;&#105;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#32;&#122;&#95;&#105;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#32;&#125;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#50;&#125;&#32;&#92;&#86;&#101;&#114;&#116;&#32;&#121;&#45;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#104;&#95;&#123;&#87;&#44;&#98;&#125;&#40;&#120;&#41;&#32;&#92;&#86;&#101;&#114;&#116;&#94;&#50;&#32;&#61;&#45;&#40;&#121;&#95;&#105;&#32;&#45;&#32;&#97;&#95;&#105;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#32;&#46;&#32;&#102;&#39;&#40;&#122;&#95;&#105;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#41;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li><li>For <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-cc9375488b4250c83fd6fe44365bb3de_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;&#61;&#110;&#95;&#108;&#45;&#49;&#44;&#110;&#95;&#108;&#45;&#50;&#44;&#110;&#95;&#108;&#45;&#51;&#44;&#32;&#8230;&#44;&#32;&#50;" title="Rendered by QuickLaTeX.com" height="17" width="208" style="vertical-align: -4px;"/><br>For each node <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-31318c5dcb226c69e0818e5f7d2422b5_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;" title="Rendered by QuickLaTeX.com" height="12" width="6" style="vertical-align: 0px;"/> in layer, set:<br><p class="ql-center-displayed-equation" style="line-height: 65px;"><span class="ql-right-eqno"> (17) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-42acfea7ad20b49fb2073db4d69a9eae_l3.png" height="65" width="241" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#100;&#101;&#108;&#116;&#97;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#61;&#32;&#92;&#108;&#101;&#102;&#116;&#40;&#32;&#32;&#32;&#32;&#92;&#115;&#117;&#109;&#95;&#123;&#106;&#61;&#49;&#125;&#94;&#123;&#115;&#95;&#123;&#108;&#43;&#49;&#125;&#125;&#32;&#87;&#95;&#123;&#106;&#105;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#92;&#100;&#101;&#108;&#116;&#97;&#95;&#106;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#32;&#102;&#39;&#40;&#122;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li><li>Compute the desired partial derivatives, which are given as:<br><p class="ql-center-displayed-equation" style="line-height: 104px;"><span class="ql-right-eqno"> (18) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-df8baac200962ac16a59afda70966e0e_l3.png" height="104" width="218" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#87;&#95;&#123;&#105;&#106;&#125;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#32;&#38;&#32;&#61;&#97;&#95;&#106;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#92;&#100;&#101;&#108;&#116;&#97;&#95;&#105;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#92;&#92;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#112;&#97;&#114;&#116;&#105;&#97;&#108;&#125;&#123;&#98;&#95;&#105;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#32;&#38;&#32;&#61;&#92;&#100;&#101;&#108;&#116;&#97;&#95;&#105;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li></ol>



<p> In the matrix-vectorial notation, the algorithm above can be re-written as:</p>



<ol><li>Perform a feedforward pass, computing the activations for layers <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-75206f65cdbe85990703ddcb6fea5f84_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#50;&#44;&#32;&#76;&#95;&#51;&#44;" title="Rendered by QuickLaTeX.com" height="16" width="51" style="vertical-align: -4px;"/> and so on up to the output layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e777d132d20d5bba887a66ec564b859_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#76;&#95;&#123;&#110;&#95;&#108;&#125;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: -5px;"/>.</li><li>For output layer (layer <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6e295917775cc894394ddc47bf84772b_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#110;&#95;&#108;" title="Rendered by QuickLaTeX.com" height="11" width="15" style="vertical-align: -3px;"/>) , set:<br><p class="ql-center-displayed-equation" style="line-height: 23px;"><span class="ql-right-eqno"> (19) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-e0712f724802bd50443bee3cf26af626_l3.png" height="23" width="209" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#100;&#101;&#108;&#116;&#97;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#61;&#32;&#45;&#40;&#121;&#32;&#45;&#32;&#97;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#32;&#46;&#32;&#102;&#39;&#40;&#122;&#94;&#123;&#40;&#110;&#95;&#108;&#41;&#125;&#41;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li><li>For <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-cc9375488b4250c83fd6fe44365bb3de_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#108;&#61;&#110;&#95;&#108;&#45;&#49;&#44;&#110;&#95;&#108;&#45;&#50;&#44;&#110;&#95;&#108;&#45;&#51;&#44;&#32;&#8230;&#44;&#32;&#50;" title="Rendered by QuickLaTeX.com" height="17" width="208" style="vertical-align: -4px;"/>, set:<br><p class="ql-center-displayed-equation" style="line-height: 33px;"><span class="ql-right-eqno"> (20) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7cc99eba41d401ae9bbfba68ada52139_l3.png" height="33" width="227" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#92;&#100;&#101;&#108;&#116;&#97;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#61;&#32;&#92;&#108;&#101;&#102;&#116;&#40;&#32;&#40;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#94;&#84;&#32;&#92;&#100;&#101;&#108;&#116;&#97;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#32;&#46;&#32;&#102;&#39;&#40;&#122;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li><li>Compute the desired partial derivatives, which are given as:<br><p class="ql-center-displayed-equation" style="line-height: 53px;"><span class="ql-right-eqno"> (21) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-6c9b9deb350c31d15d68731573a6dc56_l3.png" height="53" width="251" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#123;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#32;&#38;&#32;&#61;&#92;&#100;&#101;&#108;&#116;&#97;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#32;&#40;&#97;&#94;&#123;&#40;&#108;&#41;&#125;&#41;&#94;&#84;&#92;&#92;&#32;&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#123;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;&#32;&#38;&#32;&#61;&#92;&#100;&#101;&#108;&#116;&#97;&#94;&#123;&#40;&#108;&#43;&#49;&#41;&#125;&#32;&#92;&#92;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li></ol>



<p>Now, we are ready to derive the full Gradient Descent Algorithm.</p>



<ol><li>Set <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-0cb60938d64f0e50a7ee1289bca5bd83_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#58;&#61;&#32;&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#58;&#61;&#32;&#48;" title="Rendered by QuickLaTeX.com" height="17" width="155" style="vertical-align: 0px;"/></li><li>For <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-56e1388588b68a14b7d17f2d1ce331b0_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#105;&#61;&#49;" title="Rendered by QuickLaTeX.com" height="13" width="38" style="vertical-align: -1px;"/> to <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-fdc40b8ad1cdad0aab9d632215459d28_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#109;" title="Rendered by QuickLaTeX.com" height="8" width="15" style="vertical-align: 0px;"/>,<br>(a) Use back propagation to compute <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-a48d9256b634b82683a94355b7fb2f78_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;" title="Rendered by QuickLaTeX.com" height="26" width="122" style="vertical-align: -6px;"/>  and <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-aa1087408f0add75790f99f2b1e5ba97_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#123;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#125;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;" title="Rendered by QuickLaTeX.com" height="19" width="127" style="vertical-align: -5px;"/><br>(b) <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-635d525a832f0da0b89b7912e8f77dcb_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#58;&#61;&#32;&#32;&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#43;&#32;&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;" title="Rendered by QuickLaTeX.com" height="26" width="273" style="vertical-align: -6px;"/><br>(c) <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-b6cbd995490792386aa21d8e1011e92e_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#58;&#61;&#32;&#32;&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#43;&#32;&#92;&#110;&#97;&#98;&#108;&#97;&#95;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#32;&#74;&#40;&#87;&#44;&#98;&#59;&#120;&#44;&#121;&#41;" title="Rendered by QuickLaTeX.com" height="26" width="249" style="vertical-align: -6px;"/></li><li>Update the parameters:<br><p class="ql-center-displayed-equation" style="line-height: 92px;"><span class="ql-right-eqno"> (22) </span><span class="ql-left-eqno"> &nbsp; </span><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-7ee9691e5b48a015deb1ae341ccd73b8_l3.png" height="92" width="307" class="ql-img-displayed-equation quicklatex-auto-format" alt="&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;&#32;&#32;&#32;&#32;&#92;&#98;&#101;&#103;&#105;&#110;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#38;&#32;&#61;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#45;&#32;&#92;&#97;&#108;&#112;&#104;&#97;&#32;&#92;&#108;&#101;&#102;&#116;&#91;&#32;&#92;&#108;&#101;&#102;&#116;&#40;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#32;&#43;&#32;&#92;&#108;&#97;&#109;&#98;&#100;&#97;&#32;&#87;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#32;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#38;&#32;&#61;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#45;&#32;&#92;&#97;&#108;&#112;&#104;&#97;&#32;&#92;&#108;&#101;&#102;&#116;&#91;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#49;&#125;&#123;&#109;&#125;&#92;&#116;&#114;&#105;&#97;&#110;&#103;&#108;&#101;&#32;&#98;&#94;&#123;&#40;&#108;&#41;&#125;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#93;&#32;&#92;&#92;&#32;&#32;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#115;&#112;&#108;&#105;&#116;&#125;&#32;&#32;&#32;&#32;&#92;&#101;&#110;&#100;&#123;&#101;&#113;&#117;&#97;&#116;&#105;&#111;&#110;&#42;&#125;" title="Rendered by QuickLaTeX.com"/></p></li></ol>



<p>Finally, the NNs now can be trained by repeating the gradient descent steps to reduce the cost function <img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/ql-cache/quicklatex.com-271ede19559def352a0ad9c1dd2e03d9_l3.png" class="ql-img-inline-formula quicklatex-auto-format" alt="&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#92;&#116;&#101;&#120;&#116;&#105;&#116;&#123;&#74;&#125;&#125;&#40;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#92;&#116;&#101;&#120;&#116;&#105;&#116;&#123;&#87;&#125;&#125;&#44;&#92;&#116;&#101;&#120;&#116;&#98;&#102;&#123;&#92;&#116;&#101;&#120;&#116;&#105;&#116;&#123;&#98;&#125;&#125;&#41;" title="Rendered by QuickLaTeX.com" height="18" width="68" style="vertical-align: -4px;"/>.</p>



<h2>Reference</h2>



<p>Andrew Ng et.al, &#8216;Welcome to the Deep Learning Tutorial!: Multi-Layer Neural Network&#8217;, http://deeplearning.stanford.edu/tutorial/</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/introduction-to-arti%ef%ac%81cial-neural-networks-anns/">Introduction to Artiﬁcial Neural Networks (ANNs)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/introduction-to-arti%ef%ac%81cial-neural-networks-anns/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Sentiment Analysis Using Keras Embedding Layer in TensorFlow 2.0</title>
		<link>https://machinelearningspace.com/sentiment-analysis-tensorflow/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=sentiment-analysis-tensorflow</link>
					<comments>https://machinelearningspace.com/sentiment-analysis-tensorflow/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Sat, 25 Jan 2020 12:35:34 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Natural Language Processing]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[Keras Embedding]]></category>
		<category><![CDATA[NLP]]></category>
		<category><![CDATA[Sentiment Analysis]]></category>
		<category><![CDATA[TensorFlow 2]]></category>
		<category><![CDATA[TensorFlow 2.0]]></category>
		<category><![CDATA[Word Embeddings]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=935</guid>

					<description><![CDATA[<p>The sentiment analysis is a process of gaining an understanding of the people's or consumers' emotions or opinions about a product, service, person, or idea.  By understanding consumers' opinions, producers can enhance the quality of their products or services to meet the needs of their customers. </p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/sentiment-analysis-tensorflow/">Sentiment Analysis Using Keras Embedding Layer in TensorFlow 2.0</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p>Learn How to Solve Sentiment Analysis Problem With Keras Embedding Layer and Tensorflow</p>



<figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="2560" height="1554" src="https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-scaled.jpg" alt="NLP" class="wp-image-987" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-scaled.jpg 2560w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-300x182.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-1024x622.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-768x466.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-1536x933.jpg 1536w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-2048x1243.jpg 2048w, https://machinelearningspace.com/wp-content/uploads/2020/01/HeadImg-1-494x300.jpg 494w" sizes="(max-width: 2560px) 100vw, 2560px" /></figure>



<h2>Introduction</h2>



<p>Text classification, one of the fundamental tasks in Natural Language Processing, is a process of assigning predefined categories data to&nbsp;textual documents such as reviews, articles, tweets, blogs, etc. </p>



<p>One of the special cases of text classification is sentiment analysis.</p>



<p>The sentiment analysis is a process of gaining an understanding of the people&#8217;s or consumers&#8217; emotions or opinions about a product, service, person, or idea.  By understanding consumers&#8217; opinions, producers can enhance the quality of their products or services to meet the needs of their customers. </p>



<p>Sentiment can be classified into binary classification (positive or negative), and multi-class classification (3 or more classes, e.g., negative, neutral and positive).</p>



<p>In this tutorial, we are going to learn how to perform a simple sentiment analysis using TensorFlow by leveraging Keras Embedding layer. For the purpose of this tutorial, we&#8217;re going to use a case of Amazon&#8217;s reviews. </p>



<p>This is the list what we are going to do in this tutorial:</p>



<ul><li>Load the Amazon reviews data, then take randomly 20% of the data as our dataset. From this 20%, we&#8217;ll be dividing it again randomly to training data (70%) and validation data ( 30%). </li><li>Perform preprocessing including removing punctuation,  numbers, and single characters; and converting the upper cases to the lower cases,  so that the model can learn it easily.</li><li>Convert all text in corpus into sequences of words by using the Keras Tokenizer API.</li><li>Create and train a Deep Learning model to classify the sentiments using Keras Embedding layer.</li><li>Validate the model.</li></ul>



<p>Here is a straightforward guide to implementing it. Let&#8217;s get started!. </p>



<h2>Data preparation</h2>



<h3>Step1: Download the amazon reviews data from Kaggle</h3>



<p>For the purpose of this tutorial, we&#8217;re going to use the Kaggle&#8217;s dataset of amazon reviews that can be downloaded from this <a href="https://www.kaggle.com/bittlingmayer/amazonreviews">link.</a> If you want to work with google collab you can upload this dataset to your Google drive. </p>



<p>First of all, verify the installed TensorFlow 2.x in your colab notebook. If it exists, select it, otherwise upgrade TensorFlow.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">try:
    %tensorflow_version 2.x
except:    
    !pip install --upgrade tensorflow</pre>



<p>Then, mount your Google drive with the following code:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from google.colab import drive
drive.mount('/content/drive')</pre>



<p>Run the code and your output will be something like this: </p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="912" height="183" src="https://machinelearningspace.com/wp-content/uploads/2020/02/drive.png" alt="" class="wp-image-1193" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/drive.png 912w, https://machinelearningspace.com/wp-content/uploads/2020/02/drive-300x60.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/drive-768x154.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/drive-550x110.png 550w" sizes="(max-width: 912px) 100vw, 912px" /></figure></div>



<p>Click on the link provided as shown in the figure above, then authorize the connection, you will be given a code, copy and paste it to the box &#8220;<code>Enter your authorization code:</code>&#8220;, then press Enter. Now, you are normally in the Google drive directory.</p>



<p>Here is my Google drive, (just for example). I uploaded the file <code>amazonreviews.zip</code> to the NLP folder in my Google drive.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="442" height="300" src="https://machinelearningspace.com/wp-content/uploads/2020/01/drive2.png" alt="Sentiment Analysis: google drive" class="wp-image-1057" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/drive2.png 442w, https://machinelearningspace.com/wp-content/uploads/2020/01/drive2-300x204.png 300w" sizes="(max-width: 442px) 100vw, 442px" /></figure></div>



<p>Point to the path where your  <code>amazonreviews.zip</code> file is located. Mine is like in the following:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">%cd drive/My\ Drive/NLP</pre>



<p>Unzip the <code>amazonreviews.zip</code> file and decompress it.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">!unzip amazonreviews.zip</pre>



<p>The <code>amazonreviews.zip</code> file contains two compressed files, <code>train.ft.txt.bz2</code> and  <code>test.ft.txt.bz2</code>. </p>



<p>In this tutorial, we&#8217;re going to use only the <code>train.ft.txt.bz2</code> file.  So just  decompress this file using the following command, then you will have a <code>.txt</code> file, that is<code>train.ft.txt</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">!bzip2 -d train.ft.txt.bz2</pre>



<p>Now, we&#8217;re going to open the <code>train.ft.txt</code> file. To do so, use the following code:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">with open('train.ft.txt', 'r') as file:
    lines = file.readlines()</pre>



<p>First, let&#8217;s take a look at the contents of the <code>train.ft.txt</code> file.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1414" height="623" src="https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1.png" alt="Sentiment Analysis: Amazon Reviews data" class="wp-image-1061" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1.png 1414w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1-300x132.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1-1024x451.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1-768x338.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt0-1-550x242.png 550w" sizes="(max-width: 1414px) 100vw, 1414px" /></figure></div>



<p>As you can observe from the above figure, the beginnings of the lines are the labels followed by the reviews. The file contains only two review labels, <code>_label__2</code> and <code>__label_1</code> for the <em>positive </em>and <em>negative</em>, respectively. </p>



<p>So far, we&#8217;re doing good. Let&#8217;s go ahead.</p>



<h3>Step2: Data Preprocessing</h3>



<p>Since our data source is data with <code>.txt</code> format,  I prefer to convert it to a Pandas&#8217; data frame.  So, the first step of this data preparation is to convert the <code>.txt</code> data to the Pandas&#8217; data frame format. </p>



<h4>Converting Data to Pandas Data Frame</h4>



<p>To do so, I will start it by importing Pandas and creating a Pandas&#8217; data frame <code>DF_text_data</code> as follows:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># create a dataframe
import pandas as pd
DF_text_data = pd.DataFrame()
</pre>



<p>Now, we&#8217;re going to loop over the <code>lines</code> using the variable <code>line</code>. Then, we&#8217;ll separate the labels and the reviews from the <code>line</code> and store them to the Pandas&#8217; data frame <code>DF_text_data</code> with different columns.</p>



<p>Anytime we loop over the <code>lines</code>,  we convert text labels to numerical labels. Since this review is a binary case problem, i.e., <em>negative </em>and <em>positive </em>reviews, so we can easily convert these labels by replacing all the labels  <code>__label__2</code> to <code>1s</code> and all the labels  <code>__label__1</code> to <code>0s</code>. </p>



<p>Here is the code for doing this:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">texts=[]
labels=[]
for line in lines:
    line=line.split()
    labels.append(1) if line[0] =="__label__2" else labels.append(0)
    texts.append(" ".join(line[1:]))

DF_text_data['reviews'] = texts
DF_text_data['labels'] = labels</pre>



<p>If we print <code>DF_text_data</code>, you will see something like in the following figure. The data consists of 3 columns, they are indexes, reviews and labels.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt3.png" alt="Sentiment Analysis: Amazon reviews" class="wp-image-1064" width="465" height="355" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt3.png 769w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt3-300x229.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/print_txt3-392x300.png 392w" sizes="(max-width: 465px) 100vw, 465px" /></figure></div>



<p>As you can see, the index is started from 0 to 3.599.999, meaning this dataset contains 3.6M reviews and labels. This is a big dataset, by the way. If you have a good computer resource, you could just use them all, otherwise, we&#8217;ll be using a small part of it, let&#8217;s say 2 percent of it. To do so, check this code: </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from sklearn import model_selection

_, X_data,_, y_data = \
    model_selection.train_test_split(DF_text_data['reviews'], 
                                     DF_text_data['labels'], test_size=0.02)
</pre>



<p>The <code>X_data</code> now only contains 72K reviews and labels.</p>



<h3>Data cleaning</h3>



<p>Before we can go deeper into analyzing, we need to do data cleaning, including removing punctuation, numbers, and single characters; and converting the upper cases to the lower cases, so that the model can learn the data easily.  </p>



<p>The following is the function for this purpose:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">def preprocess(in_text):
    # If we have html tags, remove them by this way:
    #out_text = remove_tags(in_text)

    # Remove punctuations and numbers
    out_text = re.sub('[^a-zA-Z]', ' ', in_text)
    # Convert upper case to lower case
    out_text="".join(list(map(lambda x:x.lower(),out_text)))
    # Remove single character
    out_text = re.sub(r"\s+[a-zA-Z]\s+", ' ', out_text)
    return out_text</pre>



<p>Now, perform the preprocessing by calling the <code>preprocess </code>function.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#Performing preprocessing
import re

text_data=[]
for review in list(X_data):
    text_data.append(preprocess(review))</pre>



<p>Create a new data frame to store a small part of the data that has been performed preprocessing.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">DF_text= pd.DataFrame()
DF_text['reviews'] = text_data
DF_text['labels'] = list(y_data)</pre>



<p>Now, we plot the data distribution for both classes. From the plot figure, we can see that the distribution of the data is almost the same portion for both negative and positive sentiments.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#Plot data distribution
import seaborn as sns
sns.countplot(x='labels', data=DF_text)

"""
If you use Anaconda with PyCharm uncomment these lines to show the figure.
"""
#import matplot.pyplot as as plt
#plt.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/data_dist.png" alt="Sentiment Analysis: Plot data " class="wp-image-1097" width="472" height="232" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/data_dist.png 783w, https://machinelearningspace.com/wp-content/uploads/2020/01/data_dist-300x148.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/data_dist-768x378.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/data_dist-550x270.png 550w" sizes="(max-width: 472px) 100vw, 472px" /></figure></div>



<p>Now we&#8217;re going to divide our dataset into 70% as training and 30% as testing data. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">X_train, X_test, y_train, y_test = \
    model_selection.train_test_split(DF_text['reviews'], 
                                     DF_text['labels'], test_size=0.30)

</pre>



<p>Convert them to the list array.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import numpy as np
X_train=np.array(X_train.values.tolist())
X_test=np.array(X_test.values.tolist())
y_train=np.array(y_train.values.tolist())
y_test=np.array(y_test.values.tolist())</pre>



<h2>Word Embeddings</h2>



<p>In order to train our data, Deep learning model requires the numerical data as its input. Since we&#8217;re working on text classification, we need to translate our text data into numerical vectors. To do so, we&#8217;re going to use a method called word embeddings. This method encodes every word into an n-dimensional dense vector in which similar words will have similar encoding. </p>



<p>For this purpose, we’re going to use a Keras Embedding layer.  Embedding layer can be used to learn both custom word embeddings and predefined word embeddings like GloVe and Word2Vec.   </p>



<p>In this NLP tutorial, we&#8217;re going to use a Keras embedding layer to train our own custom word embedding model.  The layer is initialized with random weights and is defined as the first hidden layer of a network.   </p>



<p>The Embedding layer has 3 important arguments:</p>



<ul><li><strong>input_dim</strong>: Size of the vocabulary in the text data. </li><li><strong>output_dim</strong>: Size of the vector space in which words will be embedded. This is a parameter that can be experimented for having a better performance. (ex: 32, 100, &#8230;)</li><li><strong>input_length</strong>: Length of input sequences</li></ul>



<h3>Tokenizer</h3>



<p>Before the data text can be fed to the Keras embedding layer, it must be encoded first, so that each word can be represented by a unique integer as required by the Embedding layer. To do this, Keras also provides a Tokenizer API that allows us to vectorize a text corpus into a sequence of integers.  </p>



<p>The following is the code to do the tokenization.  First, we create a Keras tokenizer object. Then, with this object, we can call the <code>fit_on_texts</code>  function to fit the Keras tokenizer to the dataset.   </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from keras_preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
word_index=tokenizer.word_index
vocab_size = len(word_index)+1
print(vocab_size)</pre>



<p>After fitting the tokenizer to the dataset, now we&#8217;re ready to convert our text to sequences by passing our data text to <code>texts_to_sequences</code> function.</p>



<p>This function tokenizes the input corpus into tokens of words where each of the word token is associated with a unique integer value.  </p>



<p>We do it for both training and testing data. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)</pre>



<p>Finally, we add padding to make all the vectors to have the same length <code>maxlen</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

maxlen = 100

X_train_pad = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test_pad = pad_sequences(X_test, padding='post', maxlen=maxlen)</pre>



<p>Now, the data is ready to be feed to the model.   </p>



<h2>Create a Model</h2>



<p>We are now ready to create the NN model. For this tutorial, we use a simple network, you can try to use a deeper network, or with different configuration such as using LSTM layer, and perform a comparison.</p>



<p>We create a sequential model with the embedding layer is the first layer, then followed by a GRU layer with dropout=0.2 and recurrent_dropout=0.2.  and the last layer is a dense layer with the sigmoid activation function. We use sigmoid because we only have one output. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from tensorflow.keras.layers import Flatten, GRU, Dense, Flatten, Embedding
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Embedding(vocab_size, 20, input_length=maxlen))
model.add(GRU(units=32,dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))</pre>



<h2>Compile the Model</h2>



<p>To compile the model, we use <code>Adam </code>optimizer with <code>binary_crossentropy</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())</pre>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="917" height="359" src="https://machinelearningspace.com/wp-content/uploads/2020/02/summary.png" alt="Sentiment Analysis: Model Word embeddings" class="wp-image-1119" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/summary.png 917w, https://machinelearningspace.com/wp-content/uploads/2020/02/summary-300x117.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/summary-768x301.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/summary-550x215.png 550w" sizes="(max-width: 917px) 100vw, 917px" /></figure></div>



<h2>Train the Model</h2>



<p>Now, it&#8217;s time to train the model. </p>



<p>This code below is used to train the model. We validate the model while training process.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">model.fit(X_train_pad, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test_pad, y_test))</pre>



<p>After 10 epochs, the model achieves 86.66% of accuracy after epoch 10. Not bad.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1459" height="504" src="https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy.png" alt="Training Validation Sentiment Analysis" class="wp-image-1117" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy.png 1459w, https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy-300x104.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy-1024x354.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy-768x265.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/accuracy-550x190.png 550w" sizes="(max-width: 1459px) 100vw, 1459px" /></figure></div>



<h2>Conclusion</h2>



<p>In this article, we&#8217;ve built a simple model of sentiment analysis using custom word embeddings by leveraging the Keras API in TensorFlow 2.0.</p>



<p>Here are some remarks:</p>



<ul><li>To do text classification, we need to do some data preprocessing,  including removing punctuation, numbers, and single character and converting upper cases to lower cases, so that the computer can easily understand and enhance the accuracy rate.   </li><li>A Deep learning model requires numerical data as its input. Therefore we need to convert our text data into numerical vectors.  To do so, we use the word embeddings method.</li><li>Word embeddings are a way of representing words that can encode corpus text into numerical vector spaces in which similar words will have similar encoding.   </li><li>It is considered the best available representation of words in NLP. </li></ul>



<h2>Next&#8230;</h2>



<p>To explore further, in the next tutorial, we&#8217;re going to use two popular pre-trained word embeddings, GloVe and Word2Vec. So, see you in the next tutorial.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/sentiment-analysis-tensorflow/">Sentiment Analysis Using Keras Embedding Layer in TensorFlow 2.0</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/sentiment-analysis-tensorflow/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-4</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Sun, 29 Dec 2019 18:06:53 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[object detection yolov3]]></category>
		<category><![CDATA[yolov3 object detection]]></category>
		<category><![CDATA[Yolov3 TensorFlow 2.0]]></category>
		<category><![CDATA[yolov3 tensorflow 2.0 python]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=81</guid>

					<description><![CDATA[<p>In part 3, we&#8217;ve created a python code to convert the file yolov3.weights into the TensorFlow 2.0 weights format. Now, we&#8217;re already in part 4, and this is our last part of this tutorial. In this part, we&#8217;re going to work on 3 files, utils.py, image.py and video.py. The file utils.py contains useful functions for [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">In part 3</a>,  we&#8217;ve created a python code to convert the file <code>yolov3.weights</code> into the TensorFlow 2.0 weights format.  Now, we&#8217;re already in part 4, and this is our last part of this tutorial.  </p>



<p>In this part, we&#8217;re going to work on 3 files, <code>utils.py</code>, <code>image.py</code> and <code>video.py</code>. The file <code>utils.py</code> contains useful functions for the implementation of YOLOv3. The files <code>image.py</code> and  <code>video.py</code> are the files that will be used for testing an image and a video/camera, respectively.</p>



<p>So, let&#8217;s get into it&#8230;.</p>



<h3>utils.py</h3>



<p>The file <code>utils.py</code> will contain the useful functions that we&#8217;ll be creating soon, they are: <code>non_maximum_suppression()</code>,  <code>resize_image()</code>,  <code>output_boxes()</code>, and <code>draw_output()</code>.  </p>



<p>Open the file <code>utils.py</code> and import the necessary packages as the following:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="2" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
import numpy as np
import cv2</pre>



<h4>non_max_suppression() </h4>



<p>Now, we&#8217;re going to create the function <code>non_max_suppression()</code>. If you forget about what the non-maximum suppression is, just go back to our <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">first part</a> of this tutorial and read it carefully. </p>



<p>Here, we&#8217;re not going to develop NMS algorithm from scratch. Instead, we leverage the TensorFlow&#8217;s built-in NMS function, <code>tf.image.combined_non_max_suppression</code>.  </p>



<p>Here is the code for the <code>non_max_suppression()</code> function:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="7" data-enlighter-title="" data-enlighter-group="">def non_max_suppression(inputs, model_size, max_output_size, 
                        max_output_size_per_class, iou_threshold, confidence_threshold):
    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)
    bbox=bbox/model_size[0]

    scores = confs * class_probs
    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),
        scores=tf.reshape(scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),
        max_output_size_per_class=max_output_size_per_class,
        max_total_size=max_output_size,
        iou_threshold=iou_threshold,
        score_threshold=confidence_threshold
    )
    return boxes, scores, classes, valid_detections</pre>



<h4>resize_image()</h4>



<p>We resize the image to fit with the model’s size.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="26" data-enlighter-title="" data-enlighter-group="">def resize_image(inputs, modelsize):
    inputs= tf.image.resize(inputs, modelsize)
    return inputs</pre>



<h4>load_class_names()</h4>



<p>The following is the code for function load_class_names().</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="32" data-enlighter-title="" data-enlighter-group="">def load_class_names(file_name):
    with open(file_name, 'r') as f:
        class_names = f.read().splitlines()
    return class_names</pre>



<h4>output_boxes() </h4>



<p>This function is used to convert the boxes into the format of (<code>top-left-corner</code>, <code>bottom-right-corner</code>), following by applying the NMS function and returning the proper bounding boxes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="38" data-enlighter-title="" data-enlighter-group="">def output_boxes(inputs,model_size, max_output_size, max_output_size_per_class, 
                 iou_threshold, confidence_threshold):

    center_x, center_y, width, height, confidence, classes = \
        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)

    top_left_x = center_x - width / 2.0
    top_left_y = center_y - height / 2.0
    bottom_right_x = center_x + width / 2.0
    bottom_right_y = center_y + height / 2.0

    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,
                        bottom_right_y, confidence, classes], axis=-1)

    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size, 
                                      max_output_size_per_class, iou_threshold, confidence_threshold)

    return boxes_dicts</pre>


<p><!--EndFragment--></p>


<h4>draw_outputs()</h4>



<p>Finally, we create a function to draw the output.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="57" data-enlighter-title="" data-enlighter-group="">def draw_outputs(img, boxes, objectness, classes, nums, class_names):
    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]
    boxes=np.array(boxes)

    for i in range(nums):
        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))
        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))

        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)

        img = cv2.putText(img, '{} {:.4f}'.format(
            class_names[int(classes[i])], objectness[i]),
                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)
        return img</pre>



<p>Here is the complete code of  <code>utils.py</code>:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
import numpy as np
import cv2
import time

def non_max_suppression(inputs, model_size, max_output_size,
                        max_output_size_per_class, iou_threshold,
                        confidence_threshold):
    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)
    bbox=bbox/model_size[0]

    scores = confs * class_probs
    boxes, scores, classes, valid_detections = \
        tf.image.combined_non_max_suppression(
        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),
        scores=tf.reshape(scores, (tf.shape(scores)[0], -1,
                                   tf.shape(scores)[-1])),
        max_output_size_per_class=max_output_size_per_class,
        max_total_size=max_output_size,
        iou_threshold=iou_threshold,
        score_threshold=confidence_threshold
    )
    return boxes, scores, classes, valid_detections


def resize_image(inputs, modelsize):
    inputs= tf.image.resize(inputs, modelsize)
    return inputs


def load_class_names(file_name):
    with open(file_name, 'r') as f:
        class_names = f.read().splitlines()
    return class_names


def output_boxes(inputs,model_size, max_output_size, max_output_size_per_class,
                 iou_threshold, confidence_threshold):

    center_x, center_y, width, height, confidence, classes = \
        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)

    top_left_x = center_x - width / 2.0
    top_left_y = center_y - height / 2.0
    bottom_right_x = center_x + width / 2.0
    bottom_right_y = center_y + height / 2.0

    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,
                        bottom_right_y, confidence, classes], axis=-1)

    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size,
                                      max_output_size_per_class, iou_threshold, confidence_threshold)

    return boxes_dicts

def draw_outputs(img, boxes, objectness, classes, nums, class_names):
    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]
    boxes=np.array(boxes)

    for i in range(nums):
        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))
        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))

        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)

        img = cv2.putText(img, '{} {:.4f}'.format(
            class_names[int(classes[i])], objectness[i]),
                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)
    return img</pre>



<p>That&#8217;s all for the file  <code>utils.py</code>. Now, let&#8217;s create the testing programs for image and video.  </p>



<h3>image.py</h3>



<p>Now, open <code>image.py</code> and write the following code: </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group=""># image.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
import cv2
import numpy as np
from yolov3 import YOLOv3Net

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 40
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'
img_path = "data/images/test.jpg"</pre>



<p>Put a testing image under the directory path: <code>data/image</code>.  Let&#8217;s say our image named <code>test.jpg</code>, then add the following line to <code>image.py</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="22" data-enlighter-title="" data-enlighter-group="">img_path= "data/images/test.jpg"</pre>



<p>You can download this image and save it as <code>test.jpg</code>.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1200" height="675" src="https://machinelearningspace.com/wp-content/uploads/2020/01/test-1.jpg" alt="" class="wp-image-643" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/test-1.jpg 1200w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-300x169.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-1024x576.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-768x432.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-1-533x300.jpg 533w" sizes="(max-width: 1200px) 100vw, 1200px" /><figcaption>Source: <a href="https://france3-regions.francetvinfo.fr/bourgogne-franche-comte/sites/regions_france3/files/styles/top_big/public/assets/images/2018/08/29/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE">https://france3-regions.francetvinfo.fr/bourgogne-franche-comte/sites/regions_france3/files/styles/top_big/public/assets/images/2018/08/29/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE</a></figcaption></figure></div>



<h4>Main Pipeline</h4>



<p>Here is the main pipeline function:  loading the model,  reading the input image, performing the detection, and drawing the outputs:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="24" data-enlighter-title="" data-enlighter-group="">def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)
    model.load_weights(weightfile)

    class_names = load_class_names(class_name)

    image = cv2.imread(img_path)
    image = np.array(image)
    image = tf.expand_dims(image, 0)

    resized_frame = resize_image(image, (model_size[0],model_size[1]))
    pred = model.predict(resized_frame)

    boxes, scores, classes, nums = output_boxes( \
        pred, model_size,
        max_output_size=max_output_size,
        max_output_size_per_class=max_output_size_per_class,
        iou_threshold=iou_threshold,
        confidence_threshold=confidence_threshold)

    image = np.squeeze(image)
    img = draw_outputs(image, boxes, scores, classes, nums, class_names)

    win_name = 'Image detection'
    cv2.imshow(win_name, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    #If you want to save the result, uncommnent the line below:
    #cv2.imwrite('test.jpg', img)</pre>



<p>Here is the complete code of the  <code>image.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># image.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
import cv2
import numpy as np
from yolov3 import YOLOv3Net

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 40
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'
img_path = "data/images/test.jpg"

def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)
    model.load_weights(weightfile)

    class_names = load_class_names(class_name)

    image = cv2.imread(img_path)
    image = np.array(image)
    image = tf.expand_dims(image, 0)

    resized_frame = resize_image(image, (model_size[0],model_size[1]))
    pred = model.predict(resized_frame)

    boxes, scores, classes, nums = output_boxes( \
        pred, model_size,
        max_output_size=max_output_size,
        max_output_size_per_class=max_output_size_per_class,
        iou_threshold=iou_threshold,
        confidence_threshold=confidence_threshold)

    image = np.squeeze(image)
    img = draw_outputs(image, boxes, scores, classes, nums, class_names)

    win_name = 'Image detection'
    cv2.imshow(win_name, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    #If you want to save the result, uncommnent the line below:
    #cv2.imwrite('test.jpg', img)


if __name__ == '__main__':
    main()</pre>



<h4>Testing image</h4>



<p>Finally, we&#8217;re now ready to execute our first implementation. </p>



<p>In Anaconda prompt or in PyCharm Terminal, type the following command and press Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python image.py</pre>



<p>Here is the result. Bravo..! Finally, you did it.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1200" height="675" src="https://machinelearningspace.com/wp-content/uploads/2020/01/test-2.jpg" alt="" class="wp-image-645" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/test-2.jpg 1200w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-300x169.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-1024x576.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-768x432.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/test-2-533x300.jpg 533w" sizes="(max-width: 1200px) 100vw, 1200px" /></figure></div>


<p><!--StartFragment--></p>


<h3>video.py</h3>



<p>Open the file <code>video.py</code> then import the necessary packages as follow:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#video.py
import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
from yolov3 import YOLOv3Net
import cv2
import time

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 100
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5

cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'</pre>



<p>Then create the main function.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)

    model.load_weights(weightfile)

    class_names = load_class_names(class_name)



    win_name = 'Yolov3 detection'
    cv2.namedWindow(win_name)

    #specify the vidoe input.
    # 0 means input from cam 0.
    # For vidio, just change the 0 to video path
    cap = cv2.VideoCapture(0)
    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),
                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    try:
        while True:
            start = time.time()
            ret, frame = cap.read()
            if not ret:
                break

            resized_frame = tf.expand_dims(frame, 0)
            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))

            pred = model.predict(resized_frame)

            boxes, scores, classes, nums = output_boxes( \
                pred, model_size,
                max_output_size=max_output_size,
                max_output_size_per_class=max_output_size_per_class,
                iou_threshold=iou_threshold,
                confidence_threshold=confidence_threshold)

            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)
            cv2.imshow(win_name, img)

            stop = time.time()

            seconds = stop - start
            # print("Time taken : {0} seconds".format(seconds))

            # Calculate frames per second
            fps = 1 / seconds
            print("Estimated frames per second : {0}".format(fps))

            key = cv2.waitKey(1) &amp; 0xFF

            if key == ord('q'):
                break</pre>



<p>The function above read the input from a camera. If you want to test with the video just change this code (line 17):</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">    cap = cv2.VideoCapture(0)</pre>



<p>to:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">    cap = cv2.VideoCapture(videopath)</pre>



<p>Where, <code>video path</code> is the path of your video.</p>



<p>Here is the complete code of the <code>video.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">import tensorflow as tf
from utils import load_class_names, output_boxes, draw_outputs, resize_image
from yolov3 import YOLOv3Net
import cv2
import time

physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)

model_size = (416, 416,3)
num_classes = 80
class_name = './data/coco.names'
max_output_size = 100
max_output_size_per_class= 20
iou_threshold = 0.5
confidence_threshold = 0.5


cfgfile = 'cfg/yolov3.cfg'
weightfile = 'weights/yolov3_weights.tf'

def main():

    model = YOLOv3Net(cfgfile,model_size,num_classes)

    model.load_weights(weightfile)

    class_names = load_class_names(class_name)



    win_name = 'Yolov3 detection'
    cv2.namedWindow(win_name)

    #specify the vidoe input.
    # 0 means input from cam 0.
    # For vidio, just change the 0 to video path
    cap = cv2.VideoCapture(0)
    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),
                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    try:
        while True:
            start = time.time()
            ret, frame = cap.read()
            if not ret:
                break

            resized_frame = tf.expand_dims(frame, 0)
            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))

            pred = model.predict(resized_frame)

            boxes, scores, classes, nums = output_boxes( \
                pred, model_size,
                max_output_size=max_output_size,
                max_output_size_per_class=max_output_size_per_class,
                iou_threshold=iou_threshold,
                confidence_threshold=confidence_threshold)

            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)
            cv2.imshow(win_name, img)

            stop = time.time()

            seconds = stop - start
            # print("Time taken : {0} seconds".format(seconds))

            # Calculate frames per second
            fps = 1 / seconds
            print("Estimated frames per second : {0}".format(fps))

            key = cv2.waitKey(1) &amp; 0xFF

            if key == ord('q'):
                break

    finally:
        cv2.destroyAllWindows()
        cap.release()
        print('Detections have been performed successfully.')

if __name__ == '__main__':
    main()</pre>



<p>Execute the code with this command:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python video.py</pre>



<p>This is the end of our tutorial series of &#8220;The beginner’s guide to implementing YOLO (v3) in TensorFlow 2.0&#8221;. I hope you enjoy it.</p>



<h2>Conclusion</h2>



<p>In this tutorial series, we have implemented the YOLOv3 object detection algorithm in TensorFlow 2.0 from scratch. I made this tutorial simple and presented the code in a simple way so that every beginner just getting started learning object detection algorithms can learn it easily.<br>I hope this tutorial series will help you and will be useful for your skills as a deep learning practitioner. Don&#8217;t forget to share it and see you in another tutorial.</p>



<h2>Parts</h2>



<ul><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights  into TensorFlow 2.0 weights format.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p> </li></ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-4/feed/</wfw:commentRss>
			<slash:comments>67</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-3</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Sun, 29 Dec 2019 18:05:45 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[yolov3 object detection]]></category>
		<category><![CDATA[Yolov3 Tensorflow]]></category>
		<category><![CDATA[YoloV3 TensorFlow 2]]></category>
		<category><![CDATA[Yolov3 TensorFlow 2.0]]></category>
		<category><![CDATA[Yolov3 tutorial]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=79</guid>

					<description><![CDATA[<p>In part 2, we&#8217;ve discovered how to construct the YOLOv3 network. In this part 3, we&#8217;ll focus on the file yolov3.weights. So, what we&#8217;re going to do in part is to load the weights parameters from the file yolov3.weights, then convert them into the TensorFlow 2.0 weights format. Just to remain you that, the file [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">In part 2</a>, we&#8217;ve discovered how to construct the YOLOv3 network.  In this part 3, we&#8217;ll focus on the file <code>yolov3.weights</code>.</p>



<p>So, what we&#8217;re going to do in part is to load the weights parameters from the file <code>yolov3.weights</code>, then convert them into the TensorFlow 2.0 weights format.  Just to remain you that, the file <code>yolov3.weights</code> contains the pre-trained CNN&#8217;s parameters of YOLOv3.</p>



<p>To begin with, let&#8217;s take a look at how the YOLOv3 weights are stored.</p>



<h3 id="understandingtheweightsfile">How YOLOv3&#8217;s weights are stored?</h3>



<p>The original YOLOv3 weights file <code>yolov3.weights</code>  is a binary file and the weights are stored in the float data type. </p>



<p>One thing that we need to know that the weights only belong to convolutional layers. As we know, in YOLOv3, there are 2  convolutional layer types, with and without a batch normalization layer. So,  the weights are applied differently for different types of convolutional layers.  Since we&#8217;re reading the only float data, there&#8217;s no clue which one belongs to which layer.  If we incorrectly associate these weights with their layers properly, we&#8217;ll screw up everything, and the weights won&#8217;t be converted. So, that&#8217;s why understanding how the weights are stored is crucially important.  </p>



<p>Here, I tried to make a simple flowchart in order to describe how the weights are stored. </p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/weights.jpg" alt="" class="wp-image-529" width="444" height="436" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/weights.jpg 562w, https://machinelearningspace.com/wp-content/uploads/2020/01/weights-300x295.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/weights-305x300.jpg 305w" sizes="(max-width: 444px) 100vw, 444px" /></figure></div>


<p>When we re-write these weights to TensorFlow&#8217;s format  for a convolutional with a batch normalization layer, we need to switch the position of <code>beta</code> and <code>gamma</code>. So, they&#8217;re ordered like this: <code>beta</code>, <code>gamma</code>, <code>means</code>, <code>variance </code>and <code>conv weights</code>.  However,  the weights&#8217; order remains the same for the convolutional without a batch normalization layer.</p>



<p>All right!!, Now we&#8217;re ready to code the weights converter. </p>



<p>Without further ado, let&#8217;s do it&#8230;</p>



<h3>Working with the file <code>convert_weights.py</code></h3>



<p>Open the file <code>convert_weights.py</code>, then copy and paste the following code to the top of it. Here, we import NumPy library and the two functions that we&#8217;ve created previously in part 2, <code>YOLOv3Net</code> and <code>parse_cfg</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group="">#convert_weights.py
import numpy as np
from yolov3 import YOLOv3Net
from yolov3 import parse_cfg</pre>



<p>Now, let&#8217;s create a function called <code>load_weights()</code>. This function has 3 parameters, <code>model</code>, <code>cfgfile</code>, and <code>weightfile</code>. The parameter <code>model</code> is a returning parameters of the network&#8217;s model after calling the function <code>YOLOv3Net</code>.  The<code>cfgfile</code> and <code>weightfile</code> are respectively refer to the files <code>yolov3.cfg</code> and <code>yolov3.weights</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="6" data-enlighter-title="" data-enlighter-group="">def load_weights(model,cfgfile,weightfile):</pre>



<p>Open the file  <code>yolov3.weights</code> and read the first 5 values. These values are the header information. So, we can skip them all.<br></p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="7" data-enlighter-title="" data-enlighter-group="">    # Open the weights file
    fp = open(weightfile, "rb")

    # The first 5 values are header information
    np.fromfile(fp, dtype=np.int32, count=5)</pre>



<p>Then call <code>parse_cfg()</code> function.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="14" data-enlighter-title="" data-enlighter-group="">    blocks = parse_cfg(cfgfile)</pre>



<p>As we did when building the YOLOv3 network, we need to loop over the <code>blocks</code> and search for the convolutional layer. Don&#8217;t forget to check whether the convolutional is with batch normalization or not. If it is true, go get the relevant values (<code>gamma</code>, <code>beta</code>, <code>means</code>, and <code>variance</code>), and re-arrange them to the TensorFlow weights order. Otherwise, take the bias values. After that take the convolutional weights and set these weights to the convolutional layer depending on the convolutional type.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="16" data-enlighter-title="" data-enlighter-group="">    for i, block in enumerate(blocks[1:]):

        if (block["type"] == "convolutional"):
            conv_layer = model.get_layer('conv_' + str(i))
            print("layer: ",i+1,conv_layer)

            filters = conv_layer.filters
            k_size = conv_layer.kernel_size[0]
            in_dim = conv_layer.input_shape[-1]

            if "batch_normalize" in block:

                norm_layer = model.get_layer('bnorm_' + str(i))
                print("layer: ",i+1,norm_layer)
                size = np.prod(norm_layer.get_weights()[0].shape)

                bn_weights = np.fromfile(fp, dtype=np.float32, count=4 * filters)
                # tf [gamma, beta, mean, variance]
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]

            else:
                conv_bias = np.fromfile(fp, dtype=np.float32, count=filters)

            # darknet shape (out_dim, in_dim, height, width)
            conv_shape = (filters, in_dim, k_size, k_size)
            conv_weights = np.fromfile(
                fp, dtype=np.float32, count=np.product(conv_shape))
            # tf shape (height, width, in_dim, out_dim)
            conv_weights = conv_weights.reshape(
                conv_shape).transpose([2, 3, 1, 0])

            if "batch_normalize" in block:
                norm_layer.set_weights(bn_weights)
                conv_layer.set_weights([conv_weights])
            else:
                conv_layer.set_weights([conv_weights, conv_bias])</pre>



<p>Alert if the reading has failed. Then, close the file whether the reading was successful or not.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="53" data-enlighter-title="" data-enlighter-group="">    assert len(fp.read()) == 0, 'failed to read all data'
    fp.close()</pre>



<p>The last part of this code is the main function. Copy and paste the following code of the main function just right after the function <code>load_weights()</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="57" data-enlighter-title="" data-enlighter-group="">def main():

    weightfile = "weights/yolov3.weights"
    cfgfile = "cfg/yolov3.cfg"

    model_size = (416, 416, 3)
    num_classes = 80

    model=YOLOv3Net(cfgfile,model_size,num_classes)
    load_weights(model,cfgfile,weightfile)

    try:
        model.save_weights('weights/yolov3_weights.tf')
        print('\nThe file \'yolov3_weights.tf\' has been saved successfully.')
    except IOError:
        print("Couldn't write the file \'yolov3_weights.tf\'.")</pre>



<p>Here is the complete code for the  <code>convert_weights.py</code> </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#convert_weights.py
import numpy as np
from yolov3 import YOLOv3Net
from yolov3 import parse_cfg

def load_weights(model,cfgfile,weightfile):
    # Open the weights file
    fp = open(weightfile, "rb")

    # Skip 5 header values
    np.fromfile(fp, dtype=np.int32, count=5)

    # The rest of the values are the weights
    blocks = parse_cfg(cfgfile)

    for i, block in enumerate(blocks[1:]):

        if (block["type"] == "convolutional"):
            conv_layer = model.get_layer('conv_' + str(i))
            print("layer: ",i+1,conv_layer)

            filters = conv_layer.filters
            k_size = conv_layer.kernel_size[0]
            in_dim = conv_layer.input_shape[-1]

            if "batch_normalize" in block:

                norm_layer = model.get_layer('bnorm_' + str(i))
                print("layer: ",i+1,norm_layer)
                size = np.prod(norm_layer.get_weights()[0].shape)

                bn_weights = np.fromfile(fp, dtype=np.float32, count=4 * filters)
                # tf [gamma, beta, mean, variance]
                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]

            else:
                conv_bias = np.fromfile(fp, dtype=np.float32, count=filters)

            # darknet shape (out_dim, in_dim, height, width)
            conv_shape = (filters, in_dim, k_size, k_size)
            conv_weights = np.fromfile(
                fp, dtype=np.float32, count=np.product(conv_shape))
            # tf shape (height, width, in_dim, out_dim)
            conv_weights = conv_weights.reshape(
                conv_shape).transpose([2, 3, 1, 0])

            if "batch_normalize" in block:
                norm_layer.set_weights(bn_weights)
                conv_layer.set_weights([conv_weights])
            else:
                conv_layer.set_weights([conv_weights, conv_bias])

    assert len(fp.read()) == 0, 'failed to read all data'
    fp.close()


def main():

    weightfile = "weights/yolov3.weights"
    cfgfile = "cfg/yolov3.cfg"

    model_size = (416, 416, 3)
    num_classes = 80

    model=YOLOv3Net(cfgfile,model_size,num_classes)
    load_weights(model,cfgfile,weightfile)

    try:
        model.save_weights('weights/yolov3_weights.tf')
        print('\nThe file \'yolov3_weights.tf\' has been saved successfully.')
    except IOError:
        print("Couldn't write the file \'yolov3_weights.tf\'.")


if __name__ == '__main__':
    main()</pre>



<p>Finally, we can now execute the  <code>convert_weights.py</code> file . Open  Anaconda prompt or terminal in Pycharm, type the following command and press Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">python convert_weights.py</pre>



<p>Here is the output, I printed all the convolutional layers just to make sure that the weights are loaded correctly until the last convolutional layer.</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1089" height="937" src="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3.png" alt="" class="wp-image-592" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3.png 1089w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-300x258.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-1024x881.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-768x661.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights-3-349x300.png 349w" sizes="(max-width: 1089px) 100vw, 1089px" /></figure></div>


<p>If you use PyCharm, look at the Project Navigation on the left side as I pointed with the red boxes, you have 4 new files:</p>



<ul><li>checkpoint</li><li>yolov3_weights.tf.data-00000-of-00002</li><li>yolov3_weights.tf.data-00001-of-00002</li><li>yolov3_weights.tf.index</li></ul>



<p>Those files are the TensorFlow 2.0 weights format. So, anytime we want to use them, just simply call them like the only one file,  <code>yolov3_weights.tf</code>. We&#8217;ll see how to do this in the last part of this tutorial. </p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" loading="lazy" width="1357" height="819" src="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2.png" alt="" class="wp-image-594" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2.png 1357w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-300x181.png 300w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-1024x618.png 1024w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-768x464.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/conv_weights_2-497x300.png 497w" sizes="(max-width: 1357px) 100vw, 1357px" /></figure></div>


<p>This is the end of part 3, and we still have several things to do, those are: </p>



<ul><li>creating a pipeline to read the input image or video/camera, </li><li>computing the prediction, </li><li>and drawing the bounding boxes prediction over the input image/video/camera. </li></ul>



<p>Those are what we will be doing soon in the <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">last part</a>. Let&#8217;s go into it&#8230;</p>



<h2>Parts</h2>



<ul>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part--2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p>
</li>
</ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-3)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-3/feed/</wfw:commentRss>
			<slash:comments>21</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-2</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 17:46:29 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[Belajar Deteksi Objek]]></category>
		<category><![CDATA[Belajar Yolo]]></category>
		<category><![CDATA[cara mudah Yolo]]></category>
		<category><![CDATA[Pendeteksi objek]]></category>
		<category><![CDATA[simple yolov3]]></category>
		<category><![CDATA[Simple Yolov3 tutorial]]></category>
		<category><![CDATA[TensorFlow 2 yolov3 tutorial]]></category>
		<category><![CDATA[tensorflow 2.0 yolov3]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=44</guid>

					<description><![CDATA[<p>In part 1, we've discussed the YOLOv3 algorithm. Now, it's time to dive into the technical details of the Yolov3 implementation in Tensorflow 2.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">In part 1</a>, we&#8217;ve discussed the YOLOv3 algorithm. Now, it&#8217;s time to dive into the technical details for the implementation of YOLOv3 in Tensorflow 2.</p>



<p>The code for this tutorial designed to run on Python 3.7 and TensorFlow 2.0 can be found in my <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/RahmadSadli/Deep-Learning/tree/master/YOLOv3_TF2" target="_blank">Github repo</a>.</p>



<p>This tutorial was inspired by Ayoosh Kathuria, from one of his great articles about the implementation of YOLOv3 in Pytorch published in paperspaces&#8217;s blog (credit link at the end of this tutorial). </p>



<p>YOLOv3 has 2 important files: <code>yolov3.cfg</code> and <code>yolov3.weights</code>.  The file <code>yolov3.cfg</code> contains all information related to the YOLOv3 architecture and its parameters, whereas the file <code>yolov3.weights</code> contains the convolutional neural network (CNN) parameters of the YOLOv3 pre-trained weights. </p>



<p>Specifically, in this part, we&#8217;ll focus only on the file <code>yolov3.cfg</code>, while the file <code>yolov3.weights</code> will be discussed in the next <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">part</a>. </p>



<p>So, what we&#8217;re going to do now is to parse the parameters from the file <code>yolov3.cfg</code>, read them all, and based on that we&#8217;ll construct the YOLOv3 network.<br></p>



<p>Take your hot drink and let&#8217;s get into it&#8230;</p>



<h2>Preparation</h2>



<h3>Creating a Project Directory and Files</h3>



<p>The first thing that we need to do is to create a project directory, I personally name it <code>PROJECTS</code> because I have several projects under it. However, feel free to give another name as you want, but I suggest you do the same thing as I did so that you can follow this tutorial easily. </p>



<p>Under <code>PROJECTS</code>, create a directory named <code>YOLOv3_TF2</code>.  This is the directory where we&#8217;ll be working. <br><br>Now, under the <code>YOLOv3_TF2</code> directory, let&#8217;s create 4 subdirectories, namely: <code><strong>img</strong></code>, <code><strong>cfg</strong></code>, <code><strong>data</strong></code>,and <code><strong>weights</strong></code>. </p>



<p>And still under <code>PROJECTS</code>, now create 5 python files, they are: </p>



<ul><li><code>yolov3.py</code>,</li><li><code>convert_weights.py</code>, </li><li><code>utils.py</code>, </li><li><code>image.py</code>, and </li><li><code>video.py</code>. </li></ul>



<p>Specifically, in this part, we&#8217;ll only work on the file <code>yolov3.py</code> and leave the others all empty for the moment.</p>



<h3>Downloading files <strong><code>yolov3.cfg</code></strong>, <code>yolov3.weights</code>, and <code>coco.names</code></h3>



<p>Here are the links to download the files <code>yolov3.cfg</code>, <code>yolov3.weights</code>, and <code>coco.names</code>:</p>



<ul><li><a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg">yolov3.cfg</a></li><li><a href="https://pjreddie.com/media/files/yolov3.weights">yolov3.weights</a></li><li><a href="https://github.com/pjreddie/darknet/blob/master/data/coco.names">coco.names</a></li></ul>



<p>Save the files <code>yolov3.cfg</code>, <code>yolov3.weights</code>, and <code>coco.names</code> to the subdirectories <code><strong>cfg</strong></code>, <code><strong>weights</strong></code>, and <code><strong>data</strong></code>, respectively.</p>



<h2>yolov3.py </h2>



<h3>Importing the necessary packages</h3>



<p>Open  the <code>yolov3.py</code> and import TensorFlow and Keras Model. We also import the layers from Keras, they are <code>Conv2D</code>, <code>Input</code>, <code>ZeroPadding2D</code>, <code>LeakyReLU</code>, and <code>UpSampling2D</code><em>. </em>We&#8217;ll use them all when we build the YOLOv3 network. </p>



<p>Copy the following lines to the top of the file <code>yolov3.py</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="1,2" data-enlighter-linenumbers="" data-enlighter-lineoffset="1" data-enlighter-title="" data-enlighter-group="">#yolov3.py
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import BatchNormalization, Conv2D, \
    Input, ZeroPadding2D, LeakyReLU, UpSampling2D</pre>



<h3>Parsing the configuration file</h3>



<p>The code below is a function  called <code>parse_cfg()</code> with a parameter named <code>cfgfile</code> used to parse the YOLOv3 configuration file<code>yolov3.cfg</code>.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="10" data-enlighter-title="" data-enlighter-group="">def parse_cfg(cfgfile):
    with open(cfgfile, 'r') as file:
        lines = [line.rstrip('\n') for line in file if line != '\n' and line[0] != '#']
    holder = {}
    blocks = []
    for line in lines:
        if line[0] == '[':
            line = 'type=' + line[1:-1].rstrip()
            if len(holder) != 0:
                blocks.append(holder)
                holder = {}
        key, value = line.split("=")
        holder[key.rstrip()] = value.lstrip()
    blocks.append(holder)
    return blocks</pre>



<p>Let&#8217;s explain this code. </p>



<p>Lines 11-12, we open the <code>cfgfile</code> and read it, then remove unnecessary characters like &#8216;\n&#8217; and &#8216;#&#8217;. </p>



<p>The variable <code>lines</code> in line 12 is now holding all the lines of the file <code>yolov3.cfg</code>. So, we need to loop over it in order to read every single line from it.</p>



<p>Lines 15-23,  loop over the variable <code>lines</code> and read every single attribute from it and store them all in the list <code>blocks</code>.  This process is performed by reading the attributes block per block.  The block&#8217;s attributes and their values are firstly stored as the key-value pairs in a dictionary <code>holder</code>. After reading each block, all attributes are then appended to the list <code>blocks</code> and the <code>holder</code> is then made empty and ready to read another block.  Loop until all blocks are read before returning the content of the list <code>blocks</code>. </p>



<p>All right!..we just finished a small piece of code. The next step is to create the YOLOv3 network function. Let&#8217;s do it..</p>



<h3>Building the YOLOv3 Network</h3>



<p>We&#8217;re still working on the file <code>yolov3.py</code>, the following is the code for the YOLOv3 network function, called the <code>YOLOv3Net</code>. We pass a parameter named <code>cfgfile</code>. So, Just copy and paste the following lines under the previous function <code>parse_cfg()</code>. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="27" data-enlighter-title="" data-enlighter-group="">def YOLOv3Net(cfgfile, model_size, num_classes):

    blocks = parse_cfg(cfgfile)

    outputs = {}
    output_filters = []
    filters = []
    out_pred = []
    scale = 0

    inputs = input_image = Input(shape=model_size)
    inputs = inputs / 255.0</pre>



<p>Let&#8217;s look at it&#8230;</p>



<p>Line 27, we first call the  function <code>parse_cfg()</code> and store all the return attributes in a variable <code>blocks</code>. Here, the variable <code>blocks</code> contains all the attributes read from the file <code>yolov3.cfg</code>. </p>



<p>Lines 37-38,  we define the input model using Keras function and divided by 255 to normalize it to the range of 0–1.</p>



<p>Next&#8230;  </p>



<p>YOLOv3 has 5 layers types in general, they are: &#8220;<em>convolutional</em> layer&#8221;, &#8220;<em>upsample</em> layer&#8221;, &#8220;<em>route</em> layer&#8221;, &#8220;<em>shortcut</em> layer&#8221;, and &#8220;<em>yolo</em> layer&#8221;. </p>



<p>The following code performs an iteration over the list <code>blocks</code>. For every iteration, we check the type of the block which corresponds to the type of layer.   </p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="40" data-enlighter-title="" data-enlighter-group="">    for i, block in enumerate(blocks[1:]):</pre>



<h4>Convolutional Layer</h4>



<p>In YOLOv3, there are 2 convolutional layer types, i.e with and without batch normalization layer. The convolutional layer followed by <em>a batch normalization layer</em> uses <em>a leaky ReLU activation layer</em>, otherwise, it uses the linear activation.  So, we must handle them for every single iteration we perform.  </p>



<p>This is the code to perform the convolutional layer.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="41" data-enlighter-title="" data-enlighter-group="">        # If it is a convolutional layer
        if (block["type"] == "convolutional"):

            activation = block["activation"]
            filters = int(block["filters"])
            kernel_size = int(block["size"])
            strides = int(block["stride"])

            if strides > 1:
                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)

            inputs = Conv2D(filters,
                            kernel_size,
                            strides=strides,
                            padding='valid' if strides > 1 else 'same',
                            name='conv_' + str(i),
                            use_bias=False if ("batch_normalize" in block) else True)(inputs)

            if "batch_normalize" in block:
                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)
                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)</pre>



<p>Line 42,  we check whether the type of the block is a convolutional block, if it is true then read the attributes associated with it, otherwise, go check for another type ( we&#8217;ll be explaining after this).  In the convolutional block, you&#8217;ll find the following attributes: <em>batch_normalize</em>, <em>activation</em>, <em>filters</em>, <em>pad</em>, <em>size</em>, and <em>stride</em>. For more details, what attributes are in the convolutional blocks, you can open the file <code>yolov3.cfg</code>. </p>



<p>Lines 49-50, verify whether the <code>stride</code>is greater than 1, if it is true, then downsampling is performed, so we need to adjust the padding.</p>



<p> Lines 59-61, if we find <code>batch_normalize</code>in a block,  then add layers <em>BatchNormalization </em>and  <em>LeakyReLU</em>,  otherwise, do nothing.  </p>



<h4>Upsample Layer</h4>



<p>Now, we&#8217;re going to continue <code>if..else</code> case above. Here, we&#8217;re going to check for the <code>upsample layer</code>.  The upsample layer performs upsampling of the previous feature map by a factor of <code>stride</code>. To do this, YOLOv3 uses bilinear upsampling method.<br>So, if we find upsample block, retrieve the <code>stride </code>value and add a layer <code>UpSampling2D</code> by specifying the stride value.</p>



<p>The following is the code for that.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="64" data-enlighter-title="" data-enlighter-group="">        elif (block["type"] == "upsample"):
            stride = int(block["stride"])
            inputs = UpSampling2D(stride)(inputs)</pre>



<h4>Route Layer</h4>



<p>The route block contains an attribute <code>layers</code> which holds one or two values.  For more details, please look at the file <code>yolov3.cfg</code> and point to lines 619-634. There, you will find the following lines.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="619" data-enlighter-title="" data-enlighter-group="">[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 61</pre>



<p>I&#8217;ll explain a little bit about the above lines of <code>yolov3.cfg</code>.</p>



<p>In the line 620 above, the attribute <code>layers</code> holds a value of -4 which means that if we are in this route block, we need to backward 4 layers and then output the feature map from that layer. However, for the case of the route block whose attribute <code>layers</code> has 2 values like in lines 633-634, <code>layers</code> contains -1 and 61, we need to concatenate the feature map from a previous layer (-1) and the feature map from layer 61. So, the following is the code for the <code>route</code> layer.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="68" data-enlighter-title="" data-enlighter-group="">        # If it is a route layer
        elif (block["type"] == "route"):
            block["layers"] = block["layers"].split(',')
            start = int(block["layers"][0])

            if len(block["layers"]) > 1:
                end = int(block["layers"][1]) - i
                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index
                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)
            else:
                filters = output_filters[i + start]
                inputs = outputs[i + start]</pre>



<h4>Shortcut Layer</h4>



<p>In this layer, we perform skip connection. If we look at the file <code>yolov3.cfg</code>, this block contains an attribute <code>from</code> as shown below.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="60" data-enlighter-title="" data-enlighter-group="">[shortcut]
from=-3
activation=linear</pre>



<p>What we&#8217;re going to do in this layer block is to backward  3 layers (-3) as indicated in <code>from</code> value, then take the feature map from that layer, and add it with the feature map from the previous layer. Here is the code for that.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="81" data-enlighter-title="" data-enlighter-group="">        elif block["type"] == "shortcut":
            from_ = int(block["from"])
            inputs = outputs[i - 1] + outputs[i + from_]</pre>



<h4>Yolo Layer</h4>



<p>Here, we perform our detection and do some refining to the bounding boxes. If you have any difficulty understanding or have a problem with this part, just check out my previous <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">post </a>(part-1 of this tutorial). </p>



<p>As we did to other layers, just check whether we&#8217;re in the yolo layer. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="85" data-enlighter-title="" data-enlighter-group="">        # Yolo detection layer
        elif block["type"] == "yolo":</pre>



<p>If it is true, then take all the necessary attributes associated with it. In this case, we just need <code>mask</code> and <code>anchors</code> attributes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="88" data-enlighter-title="" data-enlighter-group="">            mask = block["mask"].split(",")
            mask = [int(x) for x in mask]
            anchors = block["anchors"].split(",")
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]
            n_anchors = len(anchors)</pre>



<p>Then we need to reshape the YOLOv3 output to the form of  [<code>None</code>, B  * <code>grid size</code> * <code>grid size</code>, 5 + <code>C</code>]. The <em>B</em> is the number of anchors and <em>C</em> is the number of classes.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="96" data-enlighter-title="" data-enlighter-group="">            out_shape = inputs.get_shape().as_list()

            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \
										 5 + num_classes])</pre>



<p>Then access all boxes attributes by this way:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="101" data-enlighter-title="" data-enlighter-group="">            box_centers = inputs[:, :, 0:2]
            box_shapes = inputs[:, :, 2:4]
            confidence = inputs[:, :, 4:5]
            classes = inputs[:, :, 5:num_classes + 5]</pre>



<h4>Refine Bounding Boxes</h4>



<p>As I mentioned in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#unique-identifier2" target="_blank">part 1</a> that after the YOLOv3 network outputs the bounding boxes prediction, we need to refine them in order to the have the right positions and shapes. </p>



<p>Use the sigmoid function to convert <code>box_centers</code>, <code>confidence</code>, and <code>classes</code> values into range of 0 &#8211; 1.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="106" data-enlighter-title="" data-enlighter-group="">            box_centers = tf.sigmoid(box_centers)
            confidence = tf.sigmoid(confidence)
            classes = tf.sigmoid(classes)</pre>



<p>Then convert <code>box_shapes</code> as the following:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="110" data-enlighter-title="" data-enlighter-group="">            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])
            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)</pre>



<p>Use a <code>meshgrid </code>to convert the relative positions of the center boxes into the real positions.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="113" data-enlighter-title="" data-enlighter-group="">            x = tf.range(out_shape[1], dtype=tf.float32)
            y = tf.range(out_shape[2], dtype=tf.float32)

            cx, cy = tf.meshgrid(x, y)
            cx = tf.reshape(cx, (-1, 1))
            cy = tf.reshape(cy, (-1, 1))
            cxy = tf.concat([cx, cy], axis=-1)
            cxy = tf.tile(cxy, [1, n_anchors])
            cxy = tf.reshape(cxy, [1, -1, 2])

            strides = (input_image.shape[1] // out_shape[1], \
                       input_image.shape[2] // out_shape[2])
            box_centers = (box_centers + cxy) * strides</pre>



<p>Then, concatenate them all together.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="127" data-enlighter-title="" data-enlighter-group="">            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)</pre>



<p><strong><code>Big note:</code> </strong>Just to remain you that YOLOv3 does 3 predictions across the scale. We do as it is. </p>



<p>Take the prediction result for each scale and concatenate it with the others.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="129" data-enlighter-title="" data-enlighter-group="">            if scale:
                out_pred = tf.concat([out_pred, prediction], axis=1)
            else:
                out_pred = prediction
                scale = 1</pre>



<p>Since the <em>route</em> and <em>shortcut</em> layers need output feature maps from previous layers, so for every iteration, we always keep the track of the feature maps and output filters.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="135" data-enlighter-title="" data-enlighter-group="">        outputs[i] = inputs
        output_filters.append(filters)</pre>



<p>Finally, we can return our model.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="138" data-enlighter-title="" data-enlighter-group="">    model = Model(input_image, out_pred)
    model.summary()
    return model</pre>



<h3>The Complete Code of the <code>yolov3.py</code></h3>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="atomic" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#yolov3.py
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import BatchNormalization, Conv2D, \
    Input, ZeroPadding2D, LeakyReLU, UpSampling2D




def parse_cfg(cfgfile):
    with open(cfgfile, 'r') as file:
        lines = [line.rstrip('\n') for line in file if line != '\n' and line[0] != '#']
    holder = {}
    blocks = []
    for line in lines:
        if line[0] == '[':
            line = 'type=' + line[1:-1].rstrip()
            if len(holder) != 0:
                blocks.append(holder)
                holder = {}
        key, value = line.split("=")
        holder[key.rstrip()] = value.lstrip()
    blocks.append(holder)
    return blocks


def YOLOv3Net(cfgfile, model_size, num_classes):

    blocks = parse_cfg(cfgfile)

    outputs = {}
    output_filters = []
    filters = []
    out_pred = []
    scale = 0

    inputs = input_image = Input(shape=model_size)
    inputs = inputs / 255.0

    for i, block in enumerate(blocks[1:]):
        # If it is a convolutional layer
        if (block["type"] == "convolutional"):

            activation = block["activation"]
            filters = int(block["filters"])
            kernel_size = int(block["size"])
            strides = int(block["stride"])

            if strides > 1:
                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)

            inputs = Conv2D(filters,
                            kernel_size,
                            strides=strides,
                            padding='valid' if strides > 1 else 'same',
                            name='conv_' + str(i),
                            use_bias=False if ("batch_normalize" in block) else True)(inputs)

            if "batch_normalize" in block:
                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)
            #if activation == "leaky":
                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)

        elif (block["type"] == "upsample"):
            stride = int(block["stride"])
            inputs = UpSampling2D(stride)(inputs)

        # If it is a route layer
        elif (block["type"] == "route"):
            block["layers"] = block["layers"].split(',')
            start = int(block["layers"][0])

            if len(block["layers"]) > 1:
                end = int(block["layers"][1]) - i
                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index
                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)
            else:
                filters = output_filters[i + start]
                inputs = outputs[i + start]

        elif block["type"] == "shortcut":
            from_ = int(block["from"])
            inputs = outputs[i - 1] + outputs[i + from_]

        # Yolo detection layer
        elif block["type"] == "yolo":

            mask = block["mask"].split(",")
            mask = [int(x) for x in mask]
            anchors = block["anchors"].split(",")
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]
            n_anchors = len(anchors)

            out_shape = inputs.get_shape().as_list()

            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \
										 5 + num_classes])

            box_centers = inputs[:, :, 0:2]
            box_shapes = inputs[:, :, 2:4]
            confidence = inputs[:, :, 4:5]
            classes = inputs[:, :, 5:num_classes + 5]

            box_centers = tf.sigmoid(box_centers)
            confidence = tf.sigmoid(confidence)
            classes = tf.sigmoid(classes)

            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])
            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)

            x = tf.range(out_shape[1], dtype=tf.float32)
            y = tf.range(out_shape[2], dtype=tf.float32)

            cx, cy = tf.meshgrid(x, y)
            cx = tf.reshape(cx, (-1, 1))
            cy = tf.reshape(cy, (-1, 1))
            cxy = tf.concat([cx, cy], axis=-1)
            cxy = tf.tile(cxy, [1, n_anchors])
            cxy = tf.reshape(cxy, [1, -1, 2])

            strides = (input_image.shape[1] // out_shape[1], \
                       input_image.shape[2] // out_shape[2])
            box_centers = (box_centers + cxy) * strides

            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)

            if scale:
                out_pred = tf.concat([out_pred, prediction], axis=1)
            else:
                out_pred = prediction
                scale = 1

        outputs[i] = inputs
        output_filters.append(filters)

    model = Model(input_image, out_pred)
    model.summary()
    return model</pre>



<p>That&#8217;s it for part 2 and see you in <a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">part 3</a>.</p>



<h2>Parts:</h2>



<ul>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part--2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p>
</li>
<li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></p>
</li>
</ul>



<p>Credit link:<br><a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</a><br></p>



<p> </p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">The beginner’s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-2/feed/</wfw:commentRss>
			<slash:comments>31</slash:comments>
		
		
			</item>
		<item>
		<title>The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</title>
		<link>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=yolov3-tensorflow-2-part-1</link>
					<comments>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 13:18:14 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[implementation yolov3]]></category>
		<category><![CDATA[implementation yolov3 python]]></category>
		<category><![CDATA[Yolo TensorFlow]]></category>
		<category><![CDATA[Yolo TensorFlow 2]]></category>
		<category><![CDATA[Yolo TensorFlow 2.0]]></category>
		<category><![CDATA[yolov3 implementation]]></category>
		<category><![CDATA[yolov3 python]]></category>
		<category><![CDATA[yolov3 python tensorflow 2]]></category>
		<category><![CDATA[yolov3 tutorial python]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=5</guid>

					<description><![CDATA[<p>In this tutorial, I'll be sharing how to implement the YOLOv3 object detector using TensorFlow 2 in the simplest way. Without over complicating things, you will discover how easy is to build a YOLOv3 object detector in TensorFlow 2.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><div class="responsive-embed-container">
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/ZOt1q3aCIIA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></p>



<h2>Tutorial Overview</h2>



<p></p>



<h3>What is this post about? </h3>



<p>Over the past few years in Machine learning, we&#8217;ve seen dramatic progress in the field of object detection. Although there are several different models of object detection, in this post, we&#8217;re going to discuss specifically one model called &#8220;You Only Look Once&#8221; or in short YOLO.  </p>



<p>Invented by  Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi (2015), YOLO has already 3 different versions so far. But in this post, we&#8217;are going to focus on the latest version only, that is YOLOv3. So here, you&#8217;ll be discovering how to implement the YOLOv3 algorithm in TensorFlow 2.0 in the simplest way. </p>



<p>For more details about how to install TensorFlow 2.0, you can follow my previous tutorial <a href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">here</a>.</p>



<h3>Who is this tutorial for?</h3>



<p>When I got started learning YOLO a few years ago, I found that it was really difficult for me to understand both the concept and implementation.  Even though there are tons of blog posts and GitHub repos about it, most of them are presented in complex architectures.</p>



<p>I pushed myself to learn them one after another and it ended me up to debug every single code, step by step, in order to grasp the core of the YOLO concept. Fortunately, I didn’t give up. After spending a lot of time, I finally made it works. </p>



<p>Based on that experience, I tried to make this tutorial easy and useful for many beginners who just got started learning object detection.  Without over-complicating things, this tutorial can be a simple explanation of  YOLOv3’s implementation in TensorFlow 2.0.</p>



<h3 id="prerequisites">Prerequisites</h3>



<ul><li>Familiar with Python 3</li><li>Understand object detection and Convolutional Neural Networks (CNNs).  </li><li>Basic TensorFlow usage.</li></ul>



<h3>What will you get after completing this tutorial?</h3>



<p>After completing this tutorial, you will understand the principle of YOLOv3 and know how to implement it in TensorFlow 2.0. I believe this tutorial will be useful for a beginner who just got started learning object detection.</p>



<p>This tutorial is broken into 4 parts, they are: </p>



<ol><li><a href="https://machinelearningspace.com//yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></li><li><a rel="noreferrer noopener" href="https://medium.com/@rahmadsadli/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2-eb2551eef3d6" target="_blank">P</a><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">art-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></li><li><a rel="noreferrer noopener" href="https://medium.com/@rahmadsadli/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-3-2a48f6a06f0a" target="_blank">Pa</a><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">rt-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></li><li><a href="https://machinelearningspace.com/yolo-v3-in-tensorflow-2-0-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos. </a></li></ol>



<p>Now, it’s time to get started on this tutorial with a brief overview of everything that we’ll be seeing in this post.</p>



<h2>YOLO: YOLOv3</h2>



<p>Initially, for those of you who don’t have a lot of prior experience with this topic, I’m going to do a brief introduction about YOLOv3 and how the algorithm actually works.</p>



<p>As its name suggests, YOLO – You Only Look Once, it applies a single forward pass neural network to the whole image and predicts the bounding boxes and their class probabilities as well. This technique makes YOLO a super-fast real-time object detection algorithm. </p>



<p>As mentioned in the original paper (<em>the link is provided at the end of this part</em>), YOLOv3 has 53 convolutional layers called Darknet-53 as you can see in the following figure. </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2.png" alt="YoloV3 Darknet 53" class="wp-image-191" width="393" height="480" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2.png 685w, https://machinelearningspace.com/wp-content/uploads/2020/01/yolo_structure2-246x300.png 246w" sizes="(max-width: 393px) 100vw, 393px" /></figure></div>



<h3>How YOLOv3 works?</h3>



<p>The YOLOv3 network divides an input image into <em>S</em> x <em>S</em> grid of cells and predicts bounding boxes as well as class probabilities for each grid. Each grid cell is responsible for predicting <em>B</em> bounding boxes and <em>C</em> class probabilities of objects whose centers fall inside the grid cell. Bounding boxes are the regions of interest (ROI) of the candidate objects. The &#8220;<em>B</em>&#8221; is associated with the number of using anchors. Each bounding box has (<em>5</em> + <em>C</em>) attributes. The value of &#8220;<em>5</em>&#8221; is related to <em>5</em> bounding box attributes, those are<em> center coordinates</em> (b<sub>x</sub>, b<sub>y</sub>) and <em>shape </em>(b<sub>h</sub>, b<sub>w</sub>) of the bounding box, and one <em>confidence score</em>.  The &#8220;<em>C</em>&#8221; is the number of classes.   The confidence score reflects how confidence a box contains an object. The confidence score is in the range of 0 &#8211; 1. We&#8217;ll be talking this confidence score in more detail in the section <a href="#nms-unique"><em>Non-Maximum Suppression</em> (<em>NMS</em>)</a>. </p>



<p>Since we have <em>S</em> x <em>S</em> grid of cells, after running a single forward pass convolutional neural network to the whole image, YOLOv3 produces a 3-D tensor with the shape of [<em>S</em>, <em>S</em>, <em>B</em> * (5 + <em>C</em>]. </p>



<p>The following figure illustrates the basic principle of YOLOv3  where the input image is divided into the 13 x 13 grid of cells (<code>13 x 13 grid of cells is used for the first scale,  whereas YOLOv3 actually uses 3 different scales and we're going to discuss it in the section <a href="#unique-identifier"><em>prediction across scale</em></a></code>).  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" loading="lazy" width="868" height="1024" src="https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png" alt="yolov3 tensorflow 2" class="wp-image-177" srcset="https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png 868w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-254x300.png 254w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-768x906.png 768w, https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2.png 1233w" sizes="(max-width: 868px) 100vw, 868px" /></figure></div>



<p>YOLOv3 was trained on the COCO dataset with <em>C</em>=80 and <em>B</em>=3.   So, for the first prediction scale, after a single forward pass of CNN, the YOLOv3 outputs a tensor with the shape of [(13, 13, 3 * (5 + 80)].   </p>



<h3>Anchor Box Algorithm </h3>



<p>Basically, one grid cell can detect only one object whose mid-point of the object falls inside the cell, but what about if a grid cell contains more than one mid-point of the objects?.  That means there are multiple objects overlapping. In order to overcome this condition, YOLOv3 uses 3 different anchor boxes for every detection scale. </p>



<p>The anchor boxes are a set of pre-defined bounding boxes of a certain height and width that are used to capture the scale and different aspect ratio of specific object classes that we want to detect.  </p>



<p>While there are 3 predictions across scale, so the total anchor boxes are 9, they are: (10×13), (16×30), (33×23) for the first scale, (30×61), (62×45), (59×119)  for the second scale, and (116×90), (156×198), (373×326)  for the third scale. </p>



<p>A clear explanation of the anchor box&#8217;s concept can be found in Andrew NG&#8217;s video <a href="https://www.youtube.com/watch?v=RTlwl2bv0Tg">here</a>.</p>



<h3 id="unique-identifier">Prediction Across Scale</h3>



<p>YOLOv3 makes detection in 3 different scales in order to accommodate different objects size by using strides of 32, 16 and 8. This means, if we feed an input image of size 416 x 416, YOLOv3 will make detection on the scale of 13 x 13, 26 x 26, and 52 x 52.  </p>



<p>For the first scale, YOLOv3 downsamples the input image into 13 x 13 and makes a prediction at the 82nd layer.  The 1st detection scale yields a 3-D tensor of size 13 x 13 x 255. </p>



<p>After that, YOLOv3 takes the feature map from layer 79 and applies one convolutional layer before upsampling it by a factor of 2 to have a size of 26 x 26. This upsampled feature map is then concatenated with the feature map from layer 61. The concatenated feature map is then subjected to a few more convolutional layers until the 2nd detection scale is performed at layer 94.  The second prediction scale produces a 3-D tensor of size 26 x 26 x 255.</p>



<p>The same design is again performed one more time to predict the 3rd scale.   The feature map from layer 91 is added one convolutional layer and is then concatenated with a feature map from layer 36.   The final prediction layer is done at layer 106 yielding a 3-D tensor of size 52 x 52 x 255. </p>



<figure class="wp-block-image"><img decoding="async" loading="lazy" width="1024" height="569" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1024x569.png" alt="yolov3 tensorflow 2" class="wp-image-27" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1024x569.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-300x167.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-768x427.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-1536x854.png 1536w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct-540x300.png 540w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_struct.png 1901w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption> Source:<a rel="noreferrer noopener" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" target="_blank"> https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b</a> </figcaption></figure>



<p>Once again, YOLOv3 predicts over 3 different scales detection, so if we feed an image of size 416x 416, it produces 3  different output shape tensor,  13 x 13 x 255,  26 x 26 x 255, and  52 x 52 x 255. </p>



<h3 id="unique-identifier2">Bounding box Prediction </h3>



<p>For each bounding box, YOLO predicts 4 coordinates, <em>t</em><sub>x</sub>, <em>t</em><sub>y</sub>, <em>t</em><sub>w</sub>, <em>t</em><sub>h</sub>. The <em>t</em><sub>x</sub> and <em>t</em><sub>y</sub> are the bounding box&#8217;s center coordinate relative to the grid cell whose center falls inside, and the <em>t</em><sub>w</sub> and <em>t</em><sub>h</sub> are the bounding box&#8217;s shape, width and height, respectively. </p>



<p id="unique-identifier3">The final output of the bounding box predictions need to be refined based on this formula: </p>



<div class="wp-block-image"><figure class="aligncenter size-medium is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-300x246.png" alt="yolov3 tensorflow 2" class="wp-image-29" width="211" height="173" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-300x246.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1-366x300.png 366w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo_eq1.png 582w" sizes="(max-width: 211px) 100vw, 211px" /></figure></div>



<p>The <em>p</em><sub>w</sub> and <em>p</em><sub>h</sub> are the anchor&#8217;s width and height, respectively. The figure below describes this transformation in more detail. </p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-300x225.png" alt="yolov3 tensorflow 2" class="wp-image-31" width="468" height="351" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-300x225.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression-400x300.png 400w, https://machinelearningspace.com/wp-content/uploads/2019/12/yolo-regression.png 720w" sizes="(max-width: 468px) 100vw, 468px" /><figcaption> Source:<a rel="noreferrer noopener" href="https://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html" target="_blank"> https://christopher5106.github.io [Bounding box object detectors: understanding YOLO, You Look Only Once]</a> </figcaption></figure></div>



<p>The YOLO algorithm returns bounding boxes in the form of (b<sub>x</sub>, b<sub>y</sub>, b<sub>w</sub>, b<sub>h</sub>). The b<sub>x</sub> and b<sub>y</sub> are the center coordinates of the boxes and b<sub>w</sub> and b<sub>h</sub> are the box shape (width and height). Generally, to draw boxes, we use the top-left coordinate (x<sub>1</sub>, y<sub>1</sub>) and the box shape (width and height). To do this just simply convert them using this simple relation: </p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox_formula2.png" alt="" class="wp-image-62" width="140" height="134"/></figure></div>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1.png" alt="yolov3 tensorflow 2" class="wp-image-57" width="289" height="182" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1.png 694w, https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1-300x189.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/bbox-1-475x300.png 475w" sizes="(max-width: 289px) 100vw, 289px" /></figure></div>



<h3>Total Class Prediction </h3>



<p>Using the COCO dataset, YOLOv3 predicts 80 different classes. YOLO outputs bounding boxes and class prediction as well. If we split an image into a 13 x 13 grid of cells and use 3 anchors box, the total output prediction is 13 x 13 x 3 or 169 x 3. However, YOLOv3 uses 3 different prediction scales which splits an image into (13 x 13), (26 x 26) and (52 x 52) grid of cells and with 3 anchors for each scale. So, the total output prediction will be ([(13 x13) + (26&#215;26)+(52&#215;52)] x3) =10,647. </p>



<h3 id="nms-unique">Non-Maximum Suppression</h3>



<p>Actually, after single forward pass CNN, what&#8217;s going to happen is the YOLO network is trying to suggest multiple bounding boxes for the same detected object. The problem is how do we decide which one of these bounding boxes is the right one. </p>



<p>Fortunately, to overcome this problem, a method called non-maximum suppression (NMS) can be applied.  Basically, what NMS does is to clean up these detections.  The first step of NMS is to suppress all the predictions boxes where the confidence score is under a certain threshold value. Let&#8217;s say the confidence threshold is set to 0.5, so every bounding box where the confidence score is less than or equal to 0.5 will be discarded. </p>



<p>Yet, this method is still not sufficient to choose the proper bounding boxes, because not all unnecessary bounding boxes can be eliminated by this step, so then the second step of NMS is applied. The rest of the higher confidence scores are sorted from the highest to the lowest one, then highlight the bounding box with the highest score as the proper bounding box, and after that find all the other bounding boxes that have a high IOU (<em>intersection over union</em>) with this highlighted box.  Let&#8217;s say we&#8217;ve set the IOU threshold to 0.5, so every bounding box that has IOU greater than 0.5 must be removed because it has a high IOU that corresponds to the same highlighted object. This method allows us to output only one proper bounding box for a detected object. Repeat this process for the remaining bounding boxes and always highlight the highest score as an appropriate bounding box. Do the same step until all bounding boxes are selected properly.</p>



<h2>End Notes</h2>



<p> Here’s a brief summary of what we have covered in this part: </p>



<ul><li>YOLO applies a single neural network to the whole image and predicts the bounding boxes and class probabilities as well. This makes YOLO a super-fast real-time object detection algorithm.</li><li>YOLO divides an image into SxS grid cells. Every cell is responsible for detecting an object whose center falls inside.</li><li>To overcome the overlapping objects whose centers fall in the same grid cell, YOLOv3 uses anchor boxes.</li><li>To facilitate the prediction across scale, YOLOv3 uses three different numbers of grid cell sizes (13&#215;13), (26&#215;26), and (52&#215;52).</li><li>A Non-Max Suppression is used to eliminate the overlapping boxes and keep only the accurate one.</li></ul>



<p>If I missed something or you have any questions, please don&#8217;t hesitate to let me know in the comments section. </p>



<p>After a brief introduction, now it&#8217;s time for us to jump into the technical details. So, let&#8217;s go get <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2/"><strong>part-2</strong></a>. </p>



<h2>Parts</h2>



<ul><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">Part-1, An introduction of the YOLOv3 algorithm.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-2/">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-3/">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.</a></p></li><li><p><a href="https://machinelearningspace.com/yolov3-tensorflow-2-part-4/">Part-4, Encoding bounding boxes and testing this implementation with images and videos.</a></p></li></ul>



<h3>Links to the original YOLO&#8217;s papers:</h3>



<ul><li>v1, You Only Look Once: Unified, Real-Time Object Detection <a rel="noreferrer noopener" href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></li><li>v2, YOLO9000: Better, Faster, Stronger <a rel="noreferrer noopener" href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a> </li><li>v3, YOLOv3: An Incremental Improvement <a rel="noreferrer noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li></ul>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/">The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/yolov3-tensorflow-2-part-1/feed/</wfw:commentRss>
			<slash:comments>29</slash:comments>
		
		
			</item>
		<item>
		<title>Installing TensorFlow 2.0 in Anaconda Environment</title>
		<link>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/?utm_source=rss&#038;utm_medium=rss&#038;utm_campaign=installing-tensorflow-2-0-in-anaconda-environment</link>
					<comments>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/#comments</comments>
		
		<dc:creator><![CDATA[rahmadsadli]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 07:49:02 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Object Detection]]></category>
		<category><![CDATA[Python Programming]]></category>
		<category><![CDATA[Anaconda]]></category>
		<category><![CDATA[Easy Install Anaconda]]></category>
		<category><![CDATA[How to Install Anacoda]]></category>
		<category><![CDATA[Python]]></category>
		<category><![CDATA[TensorFlow 2]]></category>
		<category><![CDATA[TensorFlow 2.0]]></category>
		<guid isPermaLink="false">https://machinelearningspace.com/?p=1324</guid>

					<description><![CDATA[<p>TensorFlow is still one of the popular Deep learning frameworks. It has been used in many different fields of applications including handwritten digit classification, image recognition, object detection, word embeddings, and natural language processing (NLP). In September last year, 2019, Google finally announced the availability of the final release of TensorFlow 2.0. With eager execution [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">Installing TensorFlow 2.0 in Anaconda Environment</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="640" src="https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-1024x640.jpg" alt="TensorFlow 2 in Anaconda" class="wp-image-1326" srcset="https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-1024x640.jpg 1024w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-300x188.jpg 300w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-768x480.jpg 768w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image-480x300.jpg 480w, https://machinelearningspace.com/wp-content/uploads/2020/02/TF_image.jpg 1080w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>TensorFlow is still one of the popular Deep learning frameworks. It has been used in many different fields of applications including handwritten digit classification, image recognition, object detection, word embeddings, and natural language processing (NLP).</p>



<p>In September last year, 2019, Google finally announced the availability of the final release of TensorFlow 2.0. With eager execution by default and tight integration with Keras, now TensorFlow 2.0 makes the development of machine learning applications much easier than before. </p>



<p>We can now easily debug TensorFlow&#8217;s variables and print their values just like in the standard Python. That&#8217;s way, TensorFlow 2.0 is more friendly than the older version 1.x. </p>



<p>For those of you who don&#8217;t have prior experience with this topic, this post is special for you. Here, I&#8217;m going to show you how to install TensorFlow 2.0 in Anaconda.</p>



<h2>What is Anaconda and why I recommend it?   </h2>



<p>Anaconda is a Python-based data processing built for data science. It comes with many useful built-in third-party libraries.  Installing Anaconda meaning installing Python with some commonly used libraries such as Numpy, Pandas, Scrip, and Matplotlib.  </p>



<p>For a Python developer or a data science researcher, using Anaconda has a lot of advantages, such as independently installing/updating packages without ruining the system.  So, we no need to worry about the system library or anything like that. This can save time and energy for other things. </p>



<p>Anaconda can be used across different platforms, Windows, macOS, and Linux. If we want to use a different Python version or package libraries, just create a different environment and play around without any risk of crashing the system library. </p>



<p>Now, let&#8217;s install Anaconda first.</p>



<h2><strong>Installing Anaconda</strong></h2>



<p>Anaconda is available for Windows, Mac OS X, and Linux, you can find the installation file in <a href="https://www.anaconda.com/distribution/">the anaconda official site</a>.  I suggest you choose the Python version 3.7 64-bit installer if you have a 64-bit machine, otherwise choose the 32-bit installer, instead. If you need, you can easily install Python 2.7 versions later.  </p>



<p>In case you have already installed Python on your computer, don&#8217;t worry, it won&#8217;t ruin anything. Instead, the default Python used by your programs will be the one that comes with Anaconda. Go ahead and choose the appropriate version,  follow the instructions and install it. </p>



<p>I will let you explore it, but anyhow, if you have any problem, you can simply post a comment in the comment section and I will try to do my best for you.<br>(Note: For more details on how to use Anaconda, you can visit the Anaconda user guide <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands">here</a>).</p>



<p>Now, we&#8217;re going to create our first environment, but be sure that you&#8217;ve installed Anaconda on your computer.</p>



<h3>Creating an Environment</h3>



<p>Open Anaconda prompt, and create a new environment called <strong>yolov3_tf2</strong> ( I gave this name because it relates to my next article about <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-1/">the implementation of YOLOv3 in TensorFlow 2.0</a>).  You can name it whatever you want. Just type or copy the following command to your Anaconda prompt and hit Enter.</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda create -n yolov3_tf2 python=3.7</pre>



<p>After that, you will be prompted something like this, just type &#8216;<strong>y</strong>&#8216; and then hit the Enter. <br>Note: you might be prompted a bit different to this, it doesn&#8217;t matter just hit Enter, Anaconda will do the best for you. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="769" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-1024x769.png" alt="Installing Anaconda" class="wp-image-11" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-1024x769.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-300x225.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-768x577.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1-399x300.png 399w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_1.png 1085w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>Wait until all packages installed successfully, and then you can activate your new Anaconda environment. <br>Copy and paste this command to your Anaconda prompt and hit Enter. </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda activate yolov3_tf2</pre>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="452" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-1024x452.png" alt="Activating Anaconda environment" class="wp-image-12" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-1024x452.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-300x132.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-768x339.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2-550x243.png 550w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_2.png 1085w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p> Now, your Conda&#8217;s environment is ready to use. Let&#8217;s install TensorFlow 2.0.   </p>



<h2>Installing TensorFlow 2.0</h2>



<p>When you are in the <strong>yolov3_tf2</strong> environment, now you can install any package you want. To install TensorFlow 2.0, type this command and hit Enter. </p>



<p>GPU: </p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda install -c conda-forge tensorflow-gpu=2.0</pre>



<p>CPU:</p>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="false" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">conda install -c conda-forge tensorflow=2.0</pre>



<p>Verify the Cuda toolkit and <code>cudnn</code> that will be installed, it must come with Cudatoolkit 10 and <code>cudnn 7.6</code>. If everything goes right, just type &#8216;y&#8217; and hit Enter. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="805" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-1024x805.png" alt="TensorFlow GPU" class="wp-image-13" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-1024x805.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-300x236.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-768x603.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3-382x300.png 382w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_3.png 1083w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>Basically, your TensorFlow has been installed now.  Let&#8217;s check whether it&#8217;s installed correctly or not.<br>Type <code><strong>python</strong></code> in Anaconda command prompt and hit Enter, your Python must be version <strong>3.7</strong>, then type <strong><code>import tensorflow as tf</code></strong>  and hit Enter, followed by typing <strong><code>tf.__version__</code></strong> and hit Enter. If you have TensorFlow installed on your environment, you&#8217;ll get no errors, otherwise, you&#8217;ll need to re-install it. </p>



<p>If everything has been installed correctly, you&#8217;ll get the result as shown in the figure below. Your TF version must be &#8216;2.0.0&#8217;. </p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" loading="lazy" width="1024" height="203" src="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-1024x203.png" alt="Verify TensorFlow installation" class="wp-image-14" srcset="https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-1024x203.png 1024w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-300x60.png 300w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-768x152.png 768w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4-550x109.png 550w, https://machinelearningspace.com/wp-content/uploads/2019/12/conda_4.png 1084w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>



<p>See you and check <a href="https://machinelearningspace.com/the-beginners-guide-to-implementing-yolov3-in-tensorflow-2-0-part-1/">this out</a>, my tutorial about  YOLOv3 object detection.</p>
<p>The post <a rel="nofollow" href="https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/">Installing TensorFlow 2.0 in Anaconda Environment</a> appeared first on <a rel="nofollow" href="https://machinelearningspace.com">Machine Learning Space</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://machinelearningspace.com/installing-tensorflow-2-0-in-anaconda-environment/feed/</wfw:commentRss>
			<slash:comments>71</slash:comments>
		
		
			</item>
	</channel>
</rss>
