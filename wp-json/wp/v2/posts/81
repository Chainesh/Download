{"id":81,"date":"2019-12-29T19:06:53","date_gmt":"2019-12-29T18:06:53","guid":{"rendered":"https:\/\/machinelearningspace.com\/?p=81"},"modified":"2020-03-10T15:47:49","modified_gmt":"2020-03-10T14:47:49","slug":"yolov3-tensorflow-2-part-4","status":"publish","type":"post","link":"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-4\/","title":{"rendered":"The beginner\u2019s guide to implementing YOLOv3 in TensorFlow 2.0 (part-4)"},"content":{"rendered":"\n<p><div class=\"responsive-embed-container\">\n<iframe loading=\"lazy\" width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ZOt1q3aCIIA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n<\/div><\/p>\n\n\n\n<p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">In part 3<\/a>,  we&#8217;ve created a python code to convert the file <code>yolov3.weights<\/code> into the TensorFlow 2.0 weights format.  Now, we&#8217;re already in part 4, and this is our last part of this tutorial.  <\/p>\n\n\n\n<p>In this part, we&#8217;re going to work on 3 files, <code>utils.py<\/code>, <code>image.py<\/code> and <code>video.py<\/code>. The file <code>utils.py<\/code> contains useful functions for the implementation of YOLOv3. The files <code>image.py<\/code> and  <code>video.py<\/code> are the files that will be used for testing an image and a video\/camera, respectively.<\/p>\n\n\n\n<p>So, let&#8217;s get into it&#8230;.<\/p>\n\n\n\n<h3>utils.py<\/h3>\n\n\n\n<p>The file <code>utils.py<\/code> will contain the useful functions that we&#8217;ll be creating soon, they are: <code>non_maximum_suppression()<\/code>,  <code>resize_image()<\/code>,  <code>output_boxes()<\/code>, and <code>draw_output()<\/code>.  <\/p>\n\n\n\n<p>Open the file <code>utils.py<\/code> and import the necessary packages as the following:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"2\" data-enlighter-title=\"\" data-enlighter-group=\"\">import tensorflow as tf\nimport numpy as np\nimport cv2<\/pre>\n\n\n\n<h4>non_max_suppression() <\/h4>\n\n\n\n<p>Now, we&#8217;re going to create the function <code>non_max_suppression()<\/code>. If you forget about what the non-maximum suppression is, just go back to our <a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">first part<\/a> of this tutorial and read it carefully. <\/p>\n\n\n\n<p>Here, we&#8217;re not going to develop NMS algorithm from scratch. Instead, we leverage the TensorFlow&#8217;s built-in NMS function, <code>tf.image.combined_non_max_suppression<\/code>.  <\/p>\n\n\n\n<p>Here is the code for the <code>non_max_suppression()<\/code> function:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"7\" data-enlighter-title=\"\" data-enlighter-group=\"\">def non_max_suppression(inputs, model_size, max_output_size, \n                        max_output_size_per_class, iou_threshold, confidence_threshold):\n    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)\n    bbox=bbox\/model_size[0]\n\n    scores = confs * class_probs\n    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n        scores=tf.reshape(scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),\n        max_output_size_per_class=max_output_size_per_class,\n        max_total_size=max_output_size,\n        iou_threshold=iou_threshold,\n        score_threshold=confidence_threshold\n    )\n    return boxes, scores, classes, valid_detections<\/pre>\n\n\n\n<h4>resize_image()<\/h4>\n\n\n\n<p>We resize the image to fit with the model\u2019s size.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"26\" data-enlighter-title=\"\" data-enlighter-group=\"\">def resize_image(inputs, modelsize):\n    inputs= tf.image.resize(inputs, modelsize)\n    return inputs<\/pre>\n\n\n\n<h4>load_class_names()<\/h4>\n\n\n\n<p>The following is the code for function load_class_names().<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"32\" data-enlighter-title=\"\" data-enlighter-group=\"\">def load_class_names(file_name):\n    with open(file_name, 'r') as f:\n        class_names = f.read().splitlines()\n    return class_names<\/pre>\n\n\n\n<h4>output_boxes() <\/h4>\n\n\n\n<p>This function is used to convert the boxes into the format of (<code>top-left-corner<\/code>, <code>bottom-right-corner<\/code>), following by applying the NMS function and returning the proper bounding boxes.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"38\" data-enlighter-title=\"\" data-enlighter-group=\"\">def output_boxes(inputs,model_size, max_output_size, max_output_size_per_class, \n                 iou_threshold, confidence_threshold):\n\n    center_x, center_y, width, height, confidence, classes = \\\n        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n\n    top_left_x = center_x - width \/ 2.0\n    top_left_y = center_y - height \/ 2.0\n    bottom_right_x = center_x + width \/ 2.0\n    bottom_right_y = center_y + height \/ 2.0\n\n    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,\n                        bottom_right_y, confidence, classes], axis=-1)\n\n    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size, \n                                      max_output_size_per_class, iou_threshold, confidence_threshold)\n\n    return boxes_dicts<\/pre>\n\n\n<p><!--EndFragment--><\/p>\n\n\n<h4>draw_outputs()<\/h4>\n\n\n\n<p>Finally, we create a function to draw the output.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"57\" data-enlighter-title=\"\" data-enlighter-group=\"\">def draw_outputs(img, boxes, objectness, classes, nums, class_names):\n    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]\n    boxes=np.array(boxes)\n\n    for i in range(nums):\n        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))\n        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))\n\n        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)\n\n        img = cv2.putText(img, '{} {:.4f}'.format(\n            class_names[int(classes[i])], objectness[i]),\n                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)\n        return img<\/pre>\n\n\n\n<p>Here is the complete code of  <code>utils.py<\/code>:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">import tensorflow as tf\nimport numpy as np\nimport cv2\nimport time\n\ndef non_max_suppression(inputs, model_size, max_output_size,\n                        max_output_size_per_class, iou_threshold,\n                        confidence_threshold):\n    bbox, confs, class_probs = tf.split(inputs, [4, 1, -1], axis=-1)\n    bbox=bbox\/model_size[0]\n\n    scores = confs * class_probs\n    boxes, scores, classes, valid_detections = \\\n        tf.image.combined_non_max_suppression(\n        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n        scores=tf.reshape(scores, (tf.shape(scores)[0], -1,\n                                   tf.shape(scores)[-1])),\n        max_output_size_per_class=max_output_size_per_class,\n        max_total_size=max_output_size,\n        iou_threshold=iou_threshold,\n        score_threshold=confidence_threshold\n    )\n    return boxes, scores, classes, valid_detections\n\n\ndef resize_image(inputs, modelsize):\n    inputs= tf.image.resize(inputs, modelsize)\n    return inputs\n\n\ndef load_class_names(file_name):\n    with open(file_name, 'r') as f:\n        class_names = f.read().splitlines()\n    return class_names\n\n\ndef output_boxes(inputs,model_size, max_output_size, max_output_size_per_class,\n                 iou_threshold, confidence_threshold):\n\n    center_x, center_y, width, height, confidence, classes = \\\n        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n\n    top_left_x = center_x - width \/ 2.0\n    top_left_y = center_y - height \/ 2.0\n    bottom_right_x = center_x + width \/ 2.0\n    bottom_right_y = center_y + height \/ 2.0\n\n    inputs = tf.concat([top_left_x, top_left_y, bottom_right_x,\n                        bottom_right_y, confidence, classes], axis=-1)\n\n    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size,\n                                      max_output_size_per_class, iou_threshold, confidence_threshold)\n\n    return boxes_dicts\n\ndef draw_outputs(img, boxes, objectness, classes, nums, class_names):\n    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]\n    boxes=np.array(boxes)\n\n    for i in range(nums):\n        x1y1 = tuple((boxes[i,0:2] * [img.shape[1],img.shape[0]]).astype(np.int32))\n        x2y2 = tuple((boxes[i,2:4] * [img.shape[1],img.shape[0]]).astype(np.int32))\n\n        img = cv2.rectangle(img, (x1y1), (x2y2), (255,0,0), 2)\n\n        img = cv2.putText(img, '{} {:.4f}'.format(\n            class_names[int(classes[i])], objectness[i]),\n                          (x1y1), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)\n    return img<\/pre>\n\n\n\n<p>That&#8217;s all for the file  <code>utils.py<\/code>. Now, let&#8217;s create the testing programs for image and video.  <\/p>\n\n\n\n<h3>image.py<\/h3>\n\n\n\n<p>Now, open <code>image.py<\/code> and write the following code: <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"1\" data-enlighter-title=\"\" data-enlighter-group=\"\"># image.py\nimport tensorflow as tf\nfrom utils import load_class_names, output_boxes, draw_outputs, resize_image\nimport cv2\nimport numpy as np\nfrom yolov3 import YOLOv3Net\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nmodel_size = (416, 416,3)\nnum_classes = 80\nclass_name = '.\/data\/coco.names'\nmax_output_size = 40\nmax_output_size_per_class= 20\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\ncfgfile = 'cfg\/yolov3.cfg'\nweightfile = 'weights\/yolov3_weights.tf'\nimg_path = \"data\/images\/test.jpg\"<\/pre>\n\n\n\n<p>Put a testing image under the directory path: <code>data\/image<\/code>.  Let&#8217;s say our image named <code>test.jpg<\/code>, then add the following line to <code>image.py<\/code>. <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"22\" data-enlighter-title=\"\" data-enlighter-group=\"\">img_path= \"data\/images\/test.jpg\"<\/pre>\n\n\n\n<p>You can download this image and save it as <code>test.jpg<\/code>.<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"675\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1.jpg\" alt=\"\" class=\"wp-image-643\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1.jpg 1200w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1-300x169.jpg 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1-1024x576.jpg 1024w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1-768x432.jpg 768w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-1-533x300.jpg 533w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" \/><figcaption>Source: <a href=\"https:\/\/france3-regions.francetvinfo.fr\/bourgogne-franche-comte\/sites\/regions_france3\/files\/styles\/top_big\/public\/assets\/images\/2018\/08\/29\/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE\">https:\/\/france3-regions.francetvinfo.fr\/bourgogne-franche-comte\/sites\/regions_france3\/files\/styles\/top_big\/public\/assets\/images\/2018\/08\/29\/embouteillage_bouchon_autoroute_a7_-_maxppp-3819544.jpg?itok=3Iqd4eOE<\/a><\/figcaption><\/figure><\/div>\n\n\n\n<h4>Main Pipeline<\/h4>\n\n\n\n<p>Here is the main pipeline function:  loading the model,  reading the input image, performing the detection, and drawing the outputs:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"24\" data-enlighter-title=\"\" data-enlighter-group=\"\">def main():\n\n    model = YOLOv3Net(cfgfile,model_size,num_classes)\n    model.load_weights(weightfile)\n\n    class_names = load_class_names(class_name)\n\n    image = cv2.imread(img_path)\n    image = np.array(image)\n    image = tf.expand_dims(image, 0)\n\n    resized_frame = resize_image(image, (model_size[0],model_size[1]))\n    pred = model.predict(resized_frame)\n\n    boxes, scores, classes, nums = output_boxes( \\\n        pred, model_size,\n        max_output_size=max_output_size,\n        max_output_size_per_class=max_output_size_per_class,\n        iou_threshold=iou_threshold,\n        confidence_threshold=confidence_threshold)\n\n    image = np.squeeze(image)\n    img = draw_outputs(image, boxes, scores, classes, nums, class_names)\n\n    win_name = 'Image detection'\n    cv2.imshow(win_name, img)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    #If you want to save the result, uncommnent the line below:\n    #cv2.imwrite('test.jpg', img)<\/pre>\n\n\n\n<p>Here is the complete code of the  <code>image.py<\/code> <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\"># image.py\nimport tensorflow as tf\nfrom utils import load_class_names, output_boxes, draw_outputs, resize_image\nimport cv2\nimport numpy as np\nfrom yolov3 import YOLOv3Net\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nmodel_size = (416, 416,3)\nnum_classes = 80\nclass_name = '.\/data\/coco.names'\nmax_output_size = 40\nmax_output_size_per_class= 20\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\ncfgfile = 'cfg\/yolov3.cfg'\nweightfile = 'weights\/yolov3_weights.tf'\nimg_path = \"data\/images\/test.jpg\"\n\ndef main():\n\n    model = YOLOv3Net(cfgfile,model_size,num_classes)\n    model.load_weights(weightfile)\n\n    class_names = load_class_names(class_name)\n\n    image = cv2.imread(img_path)\n    image = np.array(image)\n    image = tf.expand_dims(image, 0)\n\n    resized_frame = resize_image(image, (model_size[0],model_size[1]))\n    pred = model.predict(resized_frame)\n\n    boxes, scores, classes, nums = output_boxes( \\\n        pred, model_size,\n        max_output_size=max_output_size,\n        max_output_size_per_class=max_output_size_per_class,\n        iou_threshold=iou_threshold,\n        confidence_threshold=confidence_threshold)\n\n    image = np.squeeze(image)\n    img = draw_outputs(image, boxes, scores, classes, nums, class_names)\n\n    win_name = 'Image detection'\n    cv2.imshow(win_name, img)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    #If you want to save the result, uncommnent the line below:\n    #cv2.imwrite('test.jpg', img)\n\n\nif __name__ == '__main__':\n    main()<\/pre>\n\n\n\n<h4>Testing image<\/h4>\n\n\n\n<p>Finally, we&#8217;re now ready to execute our first implementation. <\/p>\n\n\n\n<p>In Anaconda prompt or in PyCharm Terminal, type the following command and press Enter.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"false\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">python image.py<\/pre>\n\n\n\n<p>Here is the result. Bravo..! Finally, you did it.<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"675\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2.jpg\" alt=\"\" class=\"wp-image-645\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2.jpg 1200w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2-300x169.jpg 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2-1024x576.jpg 1024w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2-768x432.jpg 768w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/test-2-533x300.jpg 533w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" \/><\/figure><\/div>\n\n\n<p><!--StartFragment--><\/p>\n\n\n<h3>video.py<\/h3>\n\n\n\n<p>Open the file <code>video.py<\/code> then import the necessary packages as follow:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">#video.py\nimport tensorflow as tf\nfrom utils import load_class_names, output_boxes, draw_outputs, resize_image\nfrom yolov3 import YOLOv3Net\nimport cv2\nimport time\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nmodel_size = (416, 416,3)\nnum_classes = 80\nclass_name = '.\/data\/coco.names'\nmax_output_size = 100\nmax_output_size_per_class= 20\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\ncfgfile = 'cfg\/yolov3.cfg'\nweightfile = 'weights\/yolov3_weights.tf'<\/pre>\n\n\n\n<p>Then create the main function.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">def main():\n\n    model = YOLOv3Net(cfgfile,model_size,num_classes)\n\n    model.load_weights(weightfile)\n\n    class_names = load_class_names(class_name)\n\n\n\n    win_name = 'Yolov3 detection'\n    cv2.namedWindow(win_name)\n\n    #specify the vidoe input.\n    # 0 means input from cam 0.\n    # For vidio, just change the 0 to video path\n    cap = cv2.VideoCapture(0)\n    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),\n                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    try:\n        while True:\n            start = time.time()\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            resized_frame = tf.expand_dims(frame, 0)\n            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))\n\n            pred = model.predict(resized_frame)\n\n            boxes, scores, classes, nums = output_boxes( \\\n                pred, model_size,\n                max_output_size=max_output_size,\n                max_output_size_per_class=max_output_size_per_class,\n                iou_threshold=iou_threshold,\n                confidence_threshold=confidence_threshold)\n\n            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)\n            cv2.imshow(win_name, img)\n\n            stop = time.time()\n\n            seconds = stop - start\n            # print(\"Time taken : {0} seconds\".format(seconds))\n\n            # Calculate frames per second\n            fps = 1 \/ seconds\n            print(\"Estimated frames per second : {0}\".format(fps))\n\n            key = cv2.waitKey(1) &amp; 0xFF\n\n            if key == ord('q'):\n                break<\/pre>\n\n\n\n<p>The function above read the input from a camera. If you want to test with the video just change this code (line 17):<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">    cap = cv2.VideoCapture(0)<\/pre>\n\n\n\n<p>to:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">    cap = cv2.VideoCapture(videopath)<\/pre>\n\n\n\n<p>Where, <code>video path<\/code> is the path of your video.<\/p>\n\n\n\n<p>Here is the complete code of the <code>video.py<\/code> <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">import tensorflow as tf\nfrom utils import load_class_names, output_boxes, draw_outputs, resize_image\nfrom yolov3 import YOLOv3Net\nimport cv2\nimport time\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nmodel_size = (416, 416,3)\nnum_classes = 80\nclass_name = '.\/data\/coco.names'\nmax_output_size = 100\nmax_output_size_per_class= 20\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\n\ncfgfile = 'cfg\/yolov3.cfg'\nweightfile = 'weights\/yolov3_weights.tf'\n\ndef main():\n\n    model = YOLOv3Net(cfgfile,model_size,num_classes)\n\n    model.load_weights(weightfile)\n\n    class_names = load_class_names(class_name)\n\n\n\n    win_name = 'Yolov3 detection'\n    cv2.namedWindow(win_name)\n\n    #specify the vidoe input.\n    # 0 means input from cam 0.\n    # For vidio, just change the 0 to video path\n    cap = cv2.VideoCapture(0)\n    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),\n                  cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    try:\n        while True:\n            start = time.time()\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            resized_frame = tf.expand_dims(frame, 0)\n            resized_frame = resize_image(resized_frame, (model_size[0],model_size[1]))\n\n            pred = model.predict(resized_frame)\n\n            boxes, scores, classes, nums = output_boxes( \\\n                pred, model_size,\n                max_output_size=max_output_size,\n                max_output_size_per_class=max_output_size_per_class,\n                iou_threshold=iou_threshold,\n                confidence_threshold=confidence_threshold)\n\n            img = draw_outputs(frame, boxes, scores, classes, nums, class_names)\n            cv2.imshow(win_name, img)\n\n            stop = time.time()\n\n            seconds = stop - start\n            # print(\"Time taken : {0} seconds\".format(seconds))\n\n            # Calculate frames per second\n            fps = 1 \/ seconds\n            print(\"Estimated frames per second : {0}\".format(fps))\n\n            key = cv2.waitKey(1) &amp; 0xFF\n\n            if key == ord('q'):\n                break\n\n    finally:\n        cv2.destroyAllWindows()\n        cap.release()\n        print('Detections have been performed successfully.')\n\nif __name__ == '__main__':\n    main()<\/pre>\n\n\n\n<p>Execute the code with this command:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"false\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">python video.py<\/pre>\n\n\n\n<p>This is the end of our tutorial series of &#8220;The beginner\u2019s guide to implementing YOLO (v3) in TensorFlow 2.0&#8221;. I hope you enjoy it.<\/p>\n\n\n\n<h2>Conclusion<\/h2>\n\n\n\n<p>In this tutorial series, we have implemented the YOLOv3 object detection algorithm in TensorFlow 2.0 from scratch. I made this tutorial simple and presented the code in a simple way so that every beginner just getting started learning object detection algorithms can learn it easily.<br>I hope this tutorial series will help you and will be useful for your skills as a deep learning practitioner. Don&#8217;t forget to share it and see you in another tutorial.<\/p>\n\n\n\n<h2>Parts<\/h2>\n\n\n\n<ul><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">Part-1, An introduction of the YOLOv3 algorithm.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-2\/\">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">Part-3, Converting the YOLOv3 pre-trained weights  into TensorFlow 2.0 weights format.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-4\/\">Part-4, Encoding bounding boxes and testing this implementation with images and videos. <\/a><\/p> <\/li><\/ul>\n","protected":false},"excerpt":{"rendered":"<p>In part 3, we&#8217;ve created a python code to convert the file yolov3.weights into the TensorFlow 2.0 weights format. Now, we&#8217;re already in part 4, and this is our last part of this tutorial. In this part, we&#8217;re going to work on 3 files, utils.py, image.py and video.py. The file utils.py contains useful functions for [&hellip;]<\/p>\n","protected":false},"author":2,"featured_media":569,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"_mi_skip_tracking":false},"categories":[2,4,6,3],"tags":[88,87,25,82],"_links":{"self":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/81"}],"collection":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/comments?post=81"}],"version-history":[{"count":5,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/81\/revisions"}],"predecessor-version":[{"id":2475,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/81\/revisions\/2475"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media\/569"}],"wp:attachment":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media?parent=81"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/categories?post=81"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/tags?post=81"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}