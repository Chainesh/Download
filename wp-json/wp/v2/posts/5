{"id":5,"date":"2019-12-27T14:18:14","date_gmt":"2019-12-27T13:18:14","guid":{"rendered":"https:\/\/machinelearningspace.com\/?p=5"},"modified":"2021-12-14T03:57:10","modified_gmt":"2021-12-14T02:57:10","slug":"yolov3-tensorflow-2-part-1","status":"publish","type":"post","link":"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/","title":{"rendered":"The beginner&#8217;s guide to implementing YOLOv3 in TensorFlow 2.0 (part-1)"},"content":{"rendered":"\n<p><div class=\"responsive-embed-container\">\n<iframe loading=\"lazy\" width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ZOt1q3aCIIA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n<\/div><\/p>\n\n\n\n<h2>Tutorial Overview<\/h2>\n\n\n\n<p><\/p>\n\n\n\n<h3>What is this post about? <\/h3>\n\n\n\n<p>Over the past few years in Machine learning, we&#8217;ve seen dramatic progress in the field of object detection. Although there are several different models of object detection, in this post, we&#8217;re going to discuss specifically one model called &#8220;You Only Look Once&#8221; or in short YOLO.  <\/p>\n\n\n\n<p>Invented by  Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi (2015), YOLO has already 3 different versions so far. But in this post, we&#8217;are going to focus on the latest version only, that is YOLOv3. So here, you&#8217;ll be discovering how to implement the YOLOv3 algorithm in TensorFlow 2.0 in the simplest way. <\/p>\n\n\n\n<p>For more details about how to install TensorFlow 2.0, you can follow my previous tutorial <a href=\"https:\/\/machinelearningspace.com\/installing-tensorflow-2-0-in-anaconda-environment\/\">here<\/a>.<\/p>\n\n\n\n<h3>Who is this tutorial for?<\/h3>\n\n\n\n<p>When I got started learning YOLO a few years ago, I found that it was really difficult for me to understand both the concept and implementation.  Even though there are tons of blog posts and GitHub repos about it, most of them are presented in complex architectures.<\/p>\n\n\n\n<p>I pushed myself to learn them one after another and it ended me up to debug every single code, step by step, in order to grasp the core of the YOLO concept. Fortunately, I didn\u2019t give up. After spending a lot of time, I finally made it works. <\/p>\n\n\n\n<p>Based on that experience, I tried to make this tutorial easy and useful for many beginners who just got started learning object detection.  Without over-complicating things, this tutorial can be a simple explanation of  YOLOv3\u2019s implementation in TensorFlow 2.0.<\/p>\n\n\n\n<h3 id=\"prerequisites\">Prerequisites<\/h3>\n\n\n\n<ul><li>Familiar with Python 3<\/li><li>Understand object detection and Convolutional Neural Networks (CNNs).  <\/li><li>Basic TensorFlow usage.<\/li><\/ul>\n\n\n\n<h3>What will you get after completing this tutorial?<\/h3>\n\n\n\n<p>After completing this tutorial, you will understand the principle of YOLOv3 and know how to implement it in TensorFlow 2.0. I believe this tutorial will be useful for a beginner who just got started learning object detection.<\/p>\n\n\n\n<p>This tutorial is broken into 4 parts, they are: <\/p>\n\n\n\n<ol><li><a href=\"https:\/\/machinelearningspace.com\/\/yolov3-tensorflow-2-part-1\/\">Part-1, An introduction of the YOLOv3 algorithm.<\/a><\/li><li><a rel=\"noreferrer noopener\" href=\"https:\/\/medium.com\/@rahmadsadli\/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2-eb2551eef3d6\" target=\"_blank\">P<\/a><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-2\/\">art-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.<\/a><\/li><li><a rel=\"noreferrer noopener\" href=\"https:\/\/medium.com\/@rahmadsadli\/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-3-2a48f6a06f0a\" target=\"_blank\">Pa<\/a><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">rt-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.<\/a><\/li><li><a href=\"https:\/\/machinelearningspace.com\/yolo-v3-in-tensorflow-2-0-part-4\/\">Part-4, Encoding bounding boxes and testing this implementation with images and videos. <\/a><\/li><\/ol>\n\n\n\n<p>Now, it\u2019s time to get started on this tutorial with a brief overview of everything that we\u2019ll be seeing in this post.<\/p>\n\n\n\n<h2>YOLO: YOLOv3<\/h2>\n\n\n\n<p>Initially, for those of you who don\u2019t have a lot of prior experience with this topic, I\u2019m going to do a brief introduction about YOLOv3 and how the algorithm actually works.<\/p>\n\n\n\n<p>As its name suggests, YOLO \u2013 You Only Look Once, it applies a single forward pass neural network to the whole image and predicts the bounding boxes and their class probabilities as well. This technique makes YOLO a super-fast real-time object detection algorithm. <\/p>\n\n\n\n<p>As mentioned in the original paper (<em>the link is provided at the end of this part<\/em>), YOLOv3 has 53 convolutional layers called Darknet-53 as you can see in the following figure. <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-full is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/yolo_structure2.png\" alt=\"YoloV3 Darknet 53\" class=\"wp-image-191\" width=\"393\" height=\"480\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/yolo_structure2.png 685w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/yolo_structure2-246x300.png 246w\" sizes=\"(max-width: 393px) 100vw, 393px\" \/><\/figure><\/div>\n\n\n\n<h3>How YOLOv3 works?<\/h3>\n\n\n\n<p>The YOLOv3 network divides an input image into <em>S<\/em> x <em>S<\/em> grid of cells and predicts bounding boxes as well as class probabilities for each grid. Each grid cell is responsible for predicting <em>B<\/em> bounding boxes and <em>C<\/em> class probabilities of objects whose centers fall inside the grid cell. Bounding boxes are the regions of interest (ROI) of the candidate objects. The &#8220;<em>B<\/em>&#8221; is associated with the number of using anchors. Each bounding box has (<em>5<\/em> + <em>C<\/em>) attributes. The value of &#8220;<em>5<\/em>&#8221; is related to <em>5<\/em> bounding box attributes, those are<em> center coordinates<\/em> (b<sub>x<\/sub>, b<sub>y<\/sub>) and <em>shape <\/em>(b<sub>h<\/sub>, b<sub>w<\/sub>) of the bounding box, and one <em>confidence score<\/em>.  The &#8220;<em>C<\/em>&#8221; is the number of classes.   The confidence score reflects how confidence a box contains an object. The confidence score is in the range of 0 &#8211; 1. We&#8217;ll be talking this confidence score in more detail in the section <a href=\"#nms-unique\"><em>Non-Maximum Suppression<\/em> (<em>NMS<\/em>)<\/a>. <\/p>\n\n\n\n<p>Since we have <em>S<\/em> x <em>S<\/em> grid of cells, after running a single forward pass convolutional neural network to the whole image, YOLOv3 produces a 3-D tensor with the shape of [<em>S<\/em>, <em>S<\/em>, <em>B<\/em> * (5 + <em>C<\/em>]. <\/p>\n\n\n\n<p>The following figure illustrates the basic principle of YOLOv3  where the input image is divided into the 13 x 13 grid of cells (<code>13 x 13 grid of cells is used for the first scale,  whereas YOLOv3 actually uses 3 different scales and we're going to discuss it in the section <a href=\"#unique-identifier\"><em>prediction across scale<\/em><\/a><\/code>).  <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"868\" height=\"1024\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2-868x1024.png\" alt=\"yolov3 tensorflow 2\" class=\"wp-image-177\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2-868x1024.png 868w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2-254x300.png 254w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2-768x906.png 768w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2.png 1233w\" sizes=\"(max-width: 868px) 100vw, 868px\" \/><\/figure><\/div>\n\n\n\n<p>YOLOv3 was trained on the COCO dataset with <em>C<\/em>=80 and <em>B<\/em>=3.   So, for the first prediction scale, after a single forward pass of CNN, the YOLOv3 outputs a tensor with the shape of [(13, 13, 3 * (5 + 80)].   <\/p>\n\n\n\n<h3>Anchor Box Algorithm <\/h3>\n\n\n\n<p>Basically, one grid cell can detect only one object whose mid-point of the object falls inside the cell, but what about if a grid cell contains more than one mid-point of the objects?.  That means there are multiple objects overlapping. In order to overcome this condition, YOLOv3 uses 3 different anchor boxes for every detection scale. <\/p>\n\n\n\n<p>The anchor boxes are a set of pre-defined bounding boxes of a certain height and width that are used to capture the scale and different aspect ratio of specific object classes that we want to detect.  <\/p>\n\n\n\n<p>While there are 3 predictions across scale, so the total anchor boxes are 9, they are: (10\u00d713), (16\u00d730), (33\u00d723) for the first scale, (30\u00d761), (62\u00d745), (59\u00d7119)  for the second scale, and (116\u00d790), (156\u00d7198), (373\u00d7326)  for the third scale. <\/p>\n\n\n\n<p>A clear explanation of the anchor box&#8217;s concept can be found in Andrew NG&#8217;s video <a href=\"https:\/\/www.youtube.com\/watch?v=RTlwl2bv0Tg\">here<\/a>.<\/p>\n\n\n\n<h3 id=\"unique-identifier\">Prediction Across Scale<\/h3>\n\n\n\n<p>YOLOv3 makes detection in 3 different scales in order to accommodate different objects size by using strides of 32, 16 and 8. This means, if we feed an input image of size 416 x 416, YOLOv3 will make detection on the scale of 13 x 13, 26 x 26, and 52 x 52.  <\/p>\n\n\n\n<p>For the first scale, YOLOv3 downsamples the input image into 13 x 13 and makes a prediction at the 82nd layer.  The 1st detection scale yields a 3-D tensor of size 13 x 13 x 255. <\/p>\n\n\n\n<p>After that, YOLOv3 takes the feature map from layer 79 and applies one convolutional layer before upsampling it by a factor of 2 to have a size of 26 x 26. This upsampled feature map is then concatenated with the feature map from layer 61. The concatenated feature map is then subjected to a few more convolutional layers until the 2nd detection scale is performed at layer 94.  The second prediction scale produces a 3-D tensor of size 26 x 26 x 255.<\/p>\n\n\n\n<p>The same design is again performed one more time to predict the 3rd scale.   The feature map from layer 91 is added one convolutional layer and is then concatenated with a feature map from layer 36.   The final prediction layer is done at layer 106 yielding a 3-D tensor of size 52 x 52 x 255. <\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" loading=\"lazy\" width=\"1024\" height=\"569\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-1024x569.png\" alt=\"yolov3 tensorflow 2\" class=\"wp-image-27\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-1024x569.png 1024w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-300x167.png 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-768x427.png 768w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-1536x854.png 1536w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct-540x300.png 540w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_struct.png 1901w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" \/><figcaption> Source:<a rel=\"noreferrer noopener\" href=\"https:\/\/towardsdatascience.com\/yolo-v3-object-detection-53fb7d3bfe6b\" target=\"_blank\"> https:\/\/towardsdatascience.com\/yolo-v3-object-detection-53fb7d3bfe6b<\/a> <\/figcaption><\/figure>\n\n\n\n<p>Once again, YOLOv3 predicts over 3 different scales detection, so if we feed an image of size 416x 416, it produces 3  different output shape tensor,  13 x 13 x 255,  26 x 26 x 255, and  52 x 52 x 255. <\/p>\n\n\n\n<h3 id=\"unique-identifier2\">Bounding box Prediction <\/h3>\n\n\n\n<p>For each bounding box, YOLO predicts 4 coordinates, <em>t<\/em><sub>x<\/sub>, <em>t<\/em><sub>y<\/sub>, <em>t<\/em><sub>w<\/sub>, <em>t<\/em><sub>h<\/sub>. The <em>t<\/em><sub>x<\/sub> and <em>t<\/em><sub>y<\/sub> are the bounding box&#8217;s center coordinate relative to the grid cell whose center falls inside, and the <em>t<\/em><sub>w<\/sub> and <em>t<\/em><sub>h<\/sub> are the bounding box&#8217;s shape, width and height, respectively. <\/p>\n\n\n\n<p id=\"unique-identifier3\">The final output of the bounding box predictions need to be refined based on this formula: <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-medium is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_eq1-300x246.png\" alt=\"yolov3 tensorflow 2\" class=\"wp-image-29\" width=\"211\" height=\"173\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_eq1-300x246.png 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_eq1-366x300.png 366w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo_eq1.png 582w\" sizes=\"(max-width: 211px) 100vw, 211px\" \/><\/figure><\/div>\n\n\n\n<p>The <em>p<\/em><sub>w<\/sub> and <em>p<\/em><sub>h<\/sub> are the anchor&#8217;s width and height, respectively. The figure below describes this transformation in more detail. <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo-regression-300x225.png\" alt=\"yolov3 tensorflow 2\" class=\"wp-image-31\" width=\"468\" height=\"351\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo-regression-300x225.png 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo-regression-400x300.png 400w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/yolo-regression.png 720w\" sizes=\"(max-width: 468px) 100vw, 468px\" \/><figcaption> Source:<a rel=\"noreferrer noopener\" href=\"https:\/\/christopher5106.github.io\/object\/detectors\/2017\/08\/10\/bounding-box-object-detectors-understanding-yolo.html\" target=\"_blank\"> https:\/\/christopher5106.github.io [Bounding box object detectors: understanding YOLO, You Look Only Once]<\/a> <\/figcaption><\/figure><\/div>\n\n\n\n<p>The YOLO algorithm returns bounding boxes in the form of (b<sub>x<\/sub>, b<sub>y<\/sub>, b<sub>w<\/sub>, b<sub>h<\/sub>). The b<sub>x<\/sub> and b<sub>y<\/sub> are the center coordinates of the boxes and b<sub>w<\/sub> and b<sub>h<\/sub> are the box shape (width and height). Generally, to draw boxes, we use the top-left coordinate (x<sub>1<\/sub>, y<sub>1<\/sub>) and the box shape (width and height). To do this just simply convert them using this simple relation: <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/bbox_formula2.png\" alt=\"\" class=\"wp-image-62\" width=\"140\" height=\"134\"\/><\/figure><\/div>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/bbox-1.png\" alt=\"yolov3 tensorflow 2\" class=\"wp-image-57\" width=\"289\" height=\"182\" srcset=\"https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/bbox-1.png 694w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/bbox-1-300x189.png 300w, https:\/\/machinelearningspace.com\/wp-content\/uploads\/2019\/12\/bbox-1-475x300.png 475w\" sizes=\"(max-width: 289px) 100vw, 289px\" \/><\/figure><\/div>\n\n\n\n<h3>Total Class Prediction <\/h3>\n\n\n\n<p>Using the COCO dataset, YOLOv3 predicts 80 different classes. YOLO outputs bounding boxes and class prediction as well. If we split an image into a 13 x 13 grid of cells and use 3 anchors box, the total output prediction is 13 x 13 x 3 or 169 x 3. However, YOLOv3 uses 3 different prediction scales which splits an image into (13 x 13), (26 x 26) and (52 x 52) grid of cells and with 3 anchors for each scale. So, the total output prediction will be ([(13 x13) + (26&#215;26)+(52&#215;52)] x3) =10,647. <\/p>\n\n\n\n<h3 id=\"nms-unique\">Non-Maximum Suppression<\/h3>\n\n\n\n<p>Actually, after single forward pass CNN, what&#8217;s going to happen is the YOLO network is trying to suggest multiple bounding boxes for the same detected object. The problem is how do we decide which one of these bounding boxes is the right one. <\/p>\n\n\n\n<p>Fortunately, to overcome this problem, a method called non-maximum suppression (NMS) can be applied.  Basically, what NMS does is to clean up these detections.  The first step of NMS is to suppress all the predictions boxes where the confidence score is under a certain threshold value. Let&#8217;s say the confidence threshold is set to 0.5, so every bounding box where the confidence score is less than or equal to 0.5 will be discarded. <\/p>\n\n\n\n<p>Yet, this method is still not sufficient to choose the proper bounding boxes, because not all unnecessary bounding boxes can be eliminated by this step, so then the second step of NMS is applied. The rest of the higher confidence scores are sorted from the highest to the lowest one, then highlight the bounding box with the highest score as the proper bounding box, and after that find all the other bounding boxes that have a high IOU (<em>intersection over union<\/em>) with this highlighted box.  Let&#8217;s say we&#8217;ve set the IOU threshold to 0.5, so every bounding box that has IOU greater than 0.5 must be removed because it has a high IOU that corresponds to the same highlighted object. This method allows us to output only one proper bounding box for a detected object. Repeat this process for the remaining bounding boxes and always highlight the highest score as an appropriate bounding box. Do the same step until all bounding boxes are selected properly.<\/p>\n\n\n\n<h2>End Notes<\/h2>\n\n\n\n<p> Here\u2019s a brief summary of what we have covered in this part: <\/p>\n\n\n\n<ul><li>YOLO applies a single neural network to the whole image and predicts the bounding boxes and class probabilities as well. This makes YOLO a super-fast real-time object detection algorithm.<\/li><li>YOLO divides an image into SxS grid cells. Every cell is responsible for detecting an object whose center falls inside.<\/li><li>To overcome the overlapping objects whose centers fall in the same grid cell, YOLOv3 uses anchor boxes.<\/li><li>To facilitate the prediction across scale, YOLOv3 uses three different numbers of grid cell sizes (13&#215;13), (26&#215;26), and (52&#215;52).<\/li><li>A Non-Max Suppression is used to eliminate the overlapping boxes and keep only the accurate one.<\/li><\/ul>\n\n\n\n<p>If I missed something or you have any questions, please don&#8217;t hesitate to let me know in the comments section. <\/p>\n\n\n\n<p>After a brief introduction, now it&#8217;s time for us to jump into the technical details. So, let&#8217;s go get <a href=\"https:\/\/machinelearningspace.com\/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-2\/\"><strong>part-2<\/strong><\/a>. <\/p>\n\n\n\n<h2>Parts<\/h2>\n\n\n\n<ul><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">Part-1, An introduction of the YOLOv3 algorithm.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-2\/\">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.<\/a><\/p><\/li><li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-4\/\">Part-4, Encoding bounding boxes and testing this implementation with images and videos.<\/a><\/p><\/li><\/ul>\n\n\n\n<h3>Links to the original YOLO&#8217;s papers:<\/h3>\n\n\n\n<ul><li>v1, You Only Look Once: Unified, Real-Time Object Detection <a rel=\"noreferrer noopener\" href=\"https:\/\/arxiv.org\/pdf\/1506.02640.pdf\" target=\"_blank\">https:\/\/arxiv.org\/pdf\/1506.02640.pdf<\/a><\/li><li>v2, YOLO9000: Better, Faster, Stronger <a rel=\"noreferrer noopener\" href=\"https:\/\/arxiv.org\/pdf\/1612.08242.pdf\" target=\"_blank\">https:\/\/arxiv.org\/pdf\/1612.08242.pdf<\/a> <\/li><li>v3, YOLOv3: An Incremental Improvement <a rel=\"noreferrer noopener\" href=\"https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf\" target=\"_blank\">https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf<\/a><\/li><\/ul>\n","protected":false},"excerpt":{"rendered":"<p>In this tutorial, I&#8217;ll be sharing how to implement the YOLOv3 object detector using TensorFlow 2 in the simplest way. Without over complicating things, you will discover how easy is to build a YOLOv3 object detector in TensorFlow 2.<\/p>\n","protected":false},"author":2,"featured_media":564,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"_mi_skip_tracking":false},"categories":[2,4,6,3],"tags":[92,93,48,49,89,91,86,90,73],"_links":{"self":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/5"}],"collection":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/comments?post=5"}],"version-history":[{"count":5,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/5\/revisions"}],"predecessor-version":[{"id":2869,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/5\/revisions\/2869"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media\/564"}],"wp:attachment":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media?parent=5"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/categories?post=5"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/tags?post=5"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}