{"id":44,"date":"2019-12-27T18:46:29","date_gmt":"2019-12-27T17:46:29","guid":{"rendered":"https:\/\/machinelearningspace.com\/?p=44"},"modified":"2020-03-19T17:51:43","modified_gmt":"2020-03-19T16:51:43","slug":"yolov3-tensorflow-2-part-2","status":"publish","type":"post","link":"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-2\/","title":{"rendered":"The beginner\u2019s guide to implementing YOLOv3 in TensorFlow 2.0 (part-2)"},"content":{"rendered":"\n<p><div class=\"responsive-embed-container\">\n<iframe loading=\"lazy\" width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ZOt1q3aCIIA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n<\/div><\/p>\n\n\n\n<p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">In part 1<\/a>, we&#8217;ve discussed the YOLOv3 algorithm. Now, it&#8217;s time to dive into the technical details for the implementation of YOLOv3 in Tensorflow 2.<\/p>\n\n\n\n<p>The code for this tutorial designed to run on Python 3.7 and TensorFlow 2.0 can be found in my <a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https:\/\/github.com\/RahmadSadli\/Deep-Learning\/tree\/master\/YOLOv3_TF2\" target=\"_blank\">Github repo<\/a>.<\/p>\n\n\n\n<p>This tutorial was inspired by Ayoosh Kathuria, from one of his great articles about the implementation of YOLOv3 in Pytorch published in paperspaces&#8217;s blog (credit link at the end of this tutorial). <\/p>\n\n\n\n<p>YOLOv3 has 2 important files: <code>yolov3.cfg<\/code> and <code>yolov3.weights<\/code>.  The file <code>yolov3.cfg<\/code> contains all information related to the YOLOv3 architecture and its parameters, whereas the file <code>yolov3.weights<\/code> contains the convolutional neural network (CNN) parameters of the YOLOv3 pre-trained weights. <\/p>\n\n\n\n<p>Specifically, in this part, we&#8217;ll focus only on the file <code>yolov3.cfg<\/code>, while the file <code>yolov3.weights<\/code> will be discussed in the next <a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">part<\/a>. <\/p>\n\n\n\n<p>So, what we&#8217;re going to do now is to parse the parameters from the file <code>yolov3.cfg<\/code>, read them all, and based on that we&#8217;ll construct the YOLOv3 network.<br><\/p>\n\n\n\n<p>Take your hot drink and let&#8217;s get into it&#8230;<\/p>\n\n\n\n<h2>Preparation<\/h2>\n\n\n\n<h3>Creating a Project Directory and Files<\/h3>\n\n\n\n<p>The first thing that we need to do is to create a project directory, I personally name it <code>PROJECTS<\/code> because I have several projects under it. However, feel free to give another name as you want, but I suggest you do the same thing as I did so that you can follow this tutorial easily. <\/p>\n\n\n\n<p>Under <code>PROJECTS<\/code>, create a directory named <code>YOLOv3_TF2<\/code>.  This is the directory where we&#8217;ll be working. <br><br>Now, under the <code>YOLOv3_TF2<\/code> directory, let&#8217;s create 4 subdirectories, namely: <code><strong>img<\/strong><\/code>, <code><strong>cfg<\/strong><\/code>, <code><strong>data<\/strong><\/code>,and <code><strong>weights<\/strong><\/code>. <\/p>\n\n\n\n<p>And still under <code>PROJECTS<\/code>, now create 5 python files, they are: <\/p>\n\n\n\n<ul><li><code>yolov3.py<\/code>,<\/li><li><code>convert_weights.py<\/code>, <\/li><li><code>utils.py<\/code>, <\/li><li><code>image.py<\/code>, and <\/li><li><code>video.py<\/code>. <\/li><\/ul>\n\n\n\n<p>Specifically, in this part, we&#8217;ll only work on the file <code>yolov3.py<\/code> and leave the others all empty for the moment.<\/p>\n\n\n\n<h3>Downloading files <strong><code>yolov3.cfg<\/code><\/strong>, <code>yolov3.weights<\/code>, and <code>coco.names<\/code><\/h3>\n\n\n\n<p>Here are the links to download the files <code>yolov3.cfg<\/code>, <code>yolov3.weights<\/code>, and <code>coco.names<\/code>:<\/p>\n\n\n\n<ul><li><a href=\"https:\/\/github.com\/pjreddie\/darknet\/blob\/master\/cfg\/yolov3.cfg\">yolov3.cfg<\/a><\/li><li><a href=\"https:\/\/pjreddie.com\/media\/files\/yolov3.weights\">yolov3.weights<\/a><\/li><li><a href=\"https:\/\/github.com\/pjreddie\/darknet\/blob\/master\/data\/coco.names\">coco.names<\/a><\/li><\/ul>\n\n\n\n<p>Save the files <code>yolov3.cfg<\/code>, <code>yolov3.weights<\/code>, and <code>coco.names<\/code> to the subdirectories <code><strong>cfg<\/strong><\/code>, <code><strong>weights<\/strong><\/code>, and <code><strong>data<\/strong><\/code>, respectively.<\/p>\n\n\n\n<h2>yolov3.py <\/h2>\n\n\n\n<h3>Importing the necessary packages<\/h3>\n\n\n\n<p>Open  the <code>yolov3.py<\/code> and import TensorFlow and Keras Model. We also import the layers from Keras, they are <code>Conv2D<\/code>, <code>Input<\/code>, <code>ZeroPadding2D<\/code>, <code>LeakyReLU<\/code>, and <code>UpSampling2D<\/code><em>. <\/em>We&#8217;ll use them all when we build the YOLOv3 network. <\/p>\n\n\n\n<p>Copy the following lines to the top of the file <code>yolov3.py<\/code>.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"1,2\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"1\" data-enlighter-title=\"\" data-enlighter-group=\"\">#yolov3.py\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, \\\n    Input, ZeroPadding2D, LeakyReLU, UpSampling2D<\/pre>\n\n\n\n<h3>Parsing the configuration file<\/h3>\n\n\n\n<p>The code below is a function  called <code>parse_cfg()<\/code> with a parameter named <code>cfgfile<\/code> used to parse the YOLOv3 configuration file<code>yolov3.cfg<\/code>.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"10\" data-enlighter-title=\"\" data-enlighter-group=\"\">def parse_cfg(cfgfile):\n    with open(cfgfile, 'r') as file:\n        lines = [line.rstrip('\\n') for line in file if line != '\\n' and line[0] != '#']\n    holder = {}\n    blocks = []\n    for line in lines:\n        if line[0] == '[':\n            line = 'type=' + line[1:-1].rstrip()\n            if len(holder) != 0:\n                blocks.append(holder)\n                holder = {}\n        key, value = line.split(\"=\")\n        holder[key.rstrip()] = value.lstrip()\n    blocks.append(holder)\n    return blocks<\/pre>\n\n\n\n<p>Let&#8217;s explain this code. <\/p>\n\n\n\n<p>Lines 11-12, we open the <code>cfgfile<\/code> and read it, then remove unnecessary characters like &#8216;\\n&#8217; and &#8216;#&#8217;. <\/p>\n\n\n\n<p>The variable <code>lines<\/code> in line 12 is now holding all the lines of the file <code>yolov3.cfg<\/code>. So, we need to loop over it in order to read every single line from it.<\/p>\n\n\n\n<p>Lines 15-23,  loop over the variable <code>lines<\/code> and read every single attribute from it and store them all in the list <code>blocks<\/code>.  This process is performed by reading the attributes block per block.  The block&#8217;s attributes and their values are firstly stored as the key-value pairs in a dictionary <code>holder<\/code>. After reading each block, all attributes are then appended to the list <code>blocks<\/code> and the <code>holder<\/code> is then made empty and ready to read another block.  Loop until all blocks are read before returning the content of the list <code>blocks<\/code>. <\/p>\n\n\n\n<p>All right!..we just finished a small piece of code. The next step is to create the YOLOv3 network function. Let&#8217;s do it..<\/p>\n\n\n\n<h3>Building the YOLOv3 Network<\/h3>\n\n\n\n<p>We&#8217;re still working on the file <code>yolov3.py<\/code>, the following is the code for the YOLOv3 network function, called the <code>YOLOv3Net<\/code>. We pass a parameter named <code>cfgfile<\/code>. So, Just copy and paste the following lines under the previous function <code>parse_cfg()<\/code>. <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"27\" data-enlighter-title=\"\" data-enlighter-group=\"\">def YOLOv3Net(cfgfile, model_size, num_classes):\n\n    blocks = parse_cfg(cfgfile)\n\n    outputs = {}\n    output_filters = []\n    filters = []\n    out_pred = []\n    scale = 0\n\n    inputs = input_image = Input(shape=model_size)\n    inputs = inputs \/ 255.0<\/pre>\n\n\n\n<p>Let&#8217;s look at it&#8230;<\/p>\n\n\n\n<p>Line 27, we first call the  function <code>parse_cfg()<\/code> and store all the return attributes in a variable <code>blocks<\/code>. Here, the variable <code>blocks<\/code> contains all the attributes read from the file <code>yolov3.cfg<\/code>. <\/p>\n\n\n\n<p>Lines 37-38,  we define the input model using Keras function and divided by 255 to normalize it to the range of 0\u20131.<\/p>\n\n\n\n<p>Next&#8230;  <\/p>\n\n\n\n<p>YOLOv3 has 5 layers types in general, they are: &#8220;<em>convolutional<\/em> layer&#8221;, &#8220;<em>upsample<\/em> layer&#8221;, &#8220;<em>route<\/em> layer&#8221;, &#8220;<em>shortcut<\/em> layer&#8221;, and &#8220;<em>yolo<\/em> layer&#8221;. <\/p>\n\n\n\n<p>The following code performs an iteration over the list <code>blocks<\/code>. For every iteration, we check the type of the block which corresponds to the type of layer.   <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"40\" data-enlighter-title=\"\" data-enlighter-group=\"\">    for i, block in enumerate(blocks[1:]):<\/pre>\n\n\n\n<h4>Convolutional Layer<\/h4>\n\n\n\n<p>In YOLOv3, there are 2 convolutional layer types, i.e with and without batch normalization layer. The convolutional layer followed by <em>a batch normalization layer<\/em> uses <em>a leaky ReLU activation layer<\/em>, otherwise, it uses the linear activation.  So, we must handle them for every single iteration we perform.  <\/p>\n\n\n\n<p>This is the code to perform the convolutional layer.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"41\" data-enlighter-title=\"\" data-enlighter-group=\"\">        # If it is a convolutional layer\n        if (block[\"type\"] == \"convolutional\"):\n\n            activation = block[\"activation\"]\n            filters = int(block[\"filters\"])\n            kernel_size = int(block[\"size\"])\n            strides = int(block[\"stride\"])\n\n            if strides > 1:\n                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)\n\n            inputs = Conv2D(filters,\n                            kernel_size,\n                            strides=strides,\n                            padding='valid' if strides > 1 else 'same',\n                            name='conv_' + str(i),\n                            use_bias=False if (\"batch_normalize\" in block) else True)(inputs)\n\n            if \"batch_normalize\" in block:\n                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)\n                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)<\/pre>\n\n\n\n<p>Line 42,  we check whether the type of the block is a convolutional block, if it is true then read the attributes associated with it, otherwise, go check for another type ( we&#8217;ll be explaining after this).  In the convolutional block, you&#8217;ll find the following attributes: <em>batch_normalize<\/em>, <em>activation<\/em>, <em>filters<\/em>, <em>pad<\/em>, <em>size<\/em>, and <em>stride<\/em>. For more details, what attributes are in the convolutional blocks, you can open the file <code>yolov3.cfg<\/code>. <\/p>\n\n\n\n<p>Lines 49-50, verify whether the <code>stride<\/code>is greater than 1, if it is true, then downsampling is performed, so we need to adjust the padding.<\/p>\n\n\n\n<p> Lines 59-61, if we find <code>batch_normalize<\/code>in a block,  then add layers <em>BatchNormalization <\/em>and  <em>LeakyReLU<\/em>,  otherwise, do nothing.  <\/p>\n\n\n\n<h4>Upsample Layer<\/h4>\n\n\n\n<p>Now, we&#8217;re going to continue <code>if..else<\/code> case above. Here, we&#8217;re going to check for the <code>upsample layer<\/code>.  The upsample layer performs upsampling of the previous feature map by a factor of <code>stride<\/code>. To do this, YOLOv3 uses bilinear upsampling method.<br>So, if we find upsample block, retrieve the <code>stride <\/code>value and add a layer <code>UpSampling2D<\/code> by specifying the stride value.<\/p>\n\n\n\n<p>The following is the code for that.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"64\" data-enlighter-title=\"\" data-enlighter-group=\"\">        elif (block[\"type\"] == \"upsample\"):\n            stride = int(block[\"stride\"])\n            inputs = UpSampling2D(stride)(inputs)<\/pre>\n\n\n\n<h4>Route Layer<\/h4>\n\n\n\n<p>The route block contains an attribute <code>layers<\/code> which holds one or two values.  For more details, please look at the file <code>yolov3.cfg<\/code> and point to lines 619-634. There, you will find the following lines.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"619\" data-enlighter-title=\"\" data-enlighter-group=\"\">[route]\nlayers = -4\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[upsample]\nstride=2\n\n[route]\nlayers = -1, 61<\/pre>\n\n\n\n<p>I&#8217;ll explain a little bit about the above lines of <code>yolov3.cfg<\/code>.<\/p>\n\n\n\n<p>In the line 620 above, the attribute <code>layers<\/code> holds a value of -4 which means that if we are in this route block, we need to backward 4 layers and then output the feature map from that layer. However, for the case of the route block whose attribute <code>layers<\/code> has 2 values like in lines 633-634, <code>layers<\/code> contains -1 and 61, we need to concatenate the feature map from a previous layer (-1) and the feature map from layer 61. So, the following is the code for the <code>route<\/code> layer.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"68\" data-enlighter-title=\"\" data-enlighter-group=\"\">        # If it is a route layer\n        elif (block[\"type\"] == \"route\"):\n            block[\"layers\"] = block[\"layers\"].split(',')\n            start = int(block[\"layers\"][0])\n\n            if len(block[\"layers\"]) > 1:\n                end = int(block[\"layers\"][1]) - i\n                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index\n                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)\n            else:\n                filters = output_filters[i + start]\n                inputs = outputs[i + start]<\/pre>\n\n\n\n<h4>Shortcut Layer<\/h4>\n\n\n\n<p>In this layer, we perform skip connection. If we look at the file <code>yolov3.cfg<\/code>, this block contains an attribute <code>from<\/code> as shown below.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"60\" data-enlighter-title=\"\" data-enlighter-group=\"\">[shortcut]\nfrom=-3\nactivation=linear<\/pre>\n\n\n\n<p>What we&#8217;re going to do in this layer block is to backward  3 layers (-3) as indicated in <code>from<\/code> value, then take the feature map from that layer, and add it with the feature map from the previous layer. Here is the code for that.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"81\" data-enlighter-title=\"\" data-enlighter-group=\"\">        elif block[\"type\"] == \"shortcut\":\n            from_ = int(block[\"from\"])\n            inputs = outputs[i - 1] + outputs[i + from_]<\/pre>\n\n\n\n<h4>Yolo Layer<\/h4>\n\n\n\n<p>Here, we perform our detection and do some refining to the bounding boxes. If you have any difficulty understanding or have a problem with this part, just check out my previous <a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">post <\/a>(part-1 of this tutorial). <\/p>\n\n\n\n<p>As we did to other layers, just check whether we&#8217;re in the yolo layer. <\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"85\" data-enlighter-title=\"\" data-enlighter-group=\"\">        # Yolo detection layer\n        elif block[\"type\"] == \"yolo\":<\/pre>\n\n\n\n<p>If it is true, then take all the necessary attributes associated with it. In this case, we just need <code>mask<\/code> and <code>anchors<\/code> attributes.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"88\" data-enlighter-title=\"\" data-enlighter-group=\"\">            mask = block[\"mask\"].split(\",\")\n            mask = [int(x) for x in mask]\n            anchors = block[\"anchors\"].split(\",\")\n            anchors = [int(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in mask]\n            n_anchors = len(anchors)<\/pre>\n\n\n\n<p>Then we need to reshape the YOLOv3 output to the form of  [<code>None<\/code>, B  * <code>grid size<\/code> * <code>grid size<\/code>, 5 + <code>C<\/code>]. The <em>B<\/em> is the number of anchors and <em>C<\/em> is the number of classes.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"96\" data-enlighter-title=\"\" data-enlighter-group=\"\">            out_shape = inputs.get_shape().as_list()\n\n            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \\\n\t\t\t\t\t\t\t\t\t\t 5 + num_classes])<\/pre>\n\n\n\n<p>Then access all boxes attributes by this way:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"101\" data-enlighter-title=\"\" data-enlighter-group=\"\">            box_centers = inputs[:, :, 0:2]\n            box_shapes = inputs[:, :, 2:4]\n            confidence = inputs[:, :, 4:5]\n            classes = inputs[:, :, 5:num_classes + 5]<\/pre>\n\n\n\n<h4>Refine Bounding Boxes<\/h4>\n\n\n\n<p>As I mentioned in <a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/#unique-identifier2\" target=\"_blank\">part 1<\/a> that after the YOLOv3 network outputs the bounding boxes prediction, we need to refine them in order to the have the right positions and shapes. <\/p>\n\n\n\n<p>Use the sigmoid function to convert <code>box_centers<\/code>, <code>confidence<\/code>, and <code>classes<\/code> values into range of 0 &#8211; 1.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"106\" data-enlighter-title=\"\" data-enlighter-group=\"\">            box_centers = tf.sigmoid(box_centers)\n            confidence = tf.sigmoid(confidence)\n            classes = tf.sigmoid(classes)<\/pre>\n\n\n\n<p>Then convert <code>box_shapes<\/code> as the following:<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"110\" data-enlighter-title=\"\" data-enlighter-group=\"\">            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])\n            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)<\/pre>\n\n\n\n<p>Use a <code>meshgrid <\/code>to convert the relative positions of the center boxes into the real positions.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"113\" data-enlighter-title=\"\" data-enlighter-group=\"\">            x = tf.range(out_shape[1], dtype=tf.float32)\n            y = tf.range(out_shape[2], dtype=tf.float32)\n\n            cx, cy = tf.meshgrid(x, y)\n            cx = tf.reshape(cx, (-1, 1))\n            cy = tf.reshape(cy, (-1, 1))\n            cxy = tf.concat([cx, cy], axis=-1)\n            cxy = tf.tile(cxy, [1, n_anchors])\n            cxy = tf.reshape(cxy, [1, -1, 2])\n\n            strides = (input_image.shape[1] \/\/ out_shape[1], \\\n                       input_image.shape[2] \/\/ out_shape[2])\n            box_centers = (box_centers + cxy) * strides<\/pre>\n\n\n\n<p>Then, concatenate them all together.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"127\" data-enlighter-title=\"\" data-enlighter-group=\"\">            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)<\/pre>\n\n\n\n<p><strong><code>Big note:<\/code> <\/strong>Just to remain you that YOLOv3 does 3 predictions across the scale. We do as it is. <\/p>\n\n\n\n<p>Take the prediction result for each scale and concatenate it with the others.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"129\" data-enlighter-title=\"\" data-enlighter-group=\"\">            if scale:\n                out_pred = tf.concat([out_pred, prediction], axis=1)\n            else:\n                out_pred = prediction\n                scale = 1<\/pre>\n\n\n\n<p>Since the <em>route<\/em> and <em>shortcut<\/em> layers need output feature maps from previous layers, so for every iteration, we always keep the track of the feature maps and output filters.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"135\" data-enlighter-title=\"\" data-enlighter-group=\"\">        outputs[i] = inputs\n        output_filters.append(filters)<\/pre>\n\n\n\n<p>Finally, we can return our model.<\/p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"138\" data-enlighter-title=\"\" data-enlighter-group=\"\">    model = Model(input_image, out_pred)\n    model.summary()\n    return model<\/pre>\n\n\n\n<h3>The Complete Code of the <code>yolov3.py<\/code><\/h3>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"python\" data-enlighter-theme=\"atomic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">#yolov3.py\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, \\\n    Input, ZeroPadding2D, LeakyReLU, UpSampling2D\n\n\n\n\ndef parse_cfg(cfgfile):\n    with open(cfgfile, 'r') as file:\n        lines = [line.rstrip('\\n') for line in file if line != '\\n' and line[0] != '#']\n    holder = {}\n    blocks = []\n    for line in lines:\n        if line[0] == '[':\n            line = 'type=' + line[1:-1].rstrip()\n            if len(holder) != 0:\n                blocks.append(holder)\n                holder = {}\n        key, value = line.split(\"=\")\n        holder[key.rstrip()] = value.lstrip()\n    blocks.append(holder)\n    return blocks\n\n\ndef YOLOv3Net(cfgfile, model_size, num_classes):\n\n    blocks = parse_cfg(cfgfile)\n\n    outputs = {}\n    output_filters = []\n    filters = []\n    out_pred = []\n    scale = 0\n\n    inputs = input_image = Input(shape=model_size)\n    inputs = inputs \/ 255.0\n\n    for i, block in enumerate(blocks[1:]):\n        # If it is a convolutional layer\n        if (block[\"type\"] == \"convolutional\"):\n\n            activation = block[\"activation\"]\n            filters = int(block[\"filters\"])\n            kernel_size = int(block[\"size\"])\n            strides = int(block[\"stride\"])\n\n            if strides > 1:\n                inputs = ZeroPadding2D(((1, 0), (1, 0)))(inputs)\n\n            inputs = Conv2D(filters,\n                            kernel_size,\n                            strides=strides,\n                            padding='valid' if strides > 1 else 'same',\n                            name='conv_' + str(i),\n                            use_bias=False if (\"batch_normalize\" in block) else True)(inputs)\n\n            if \"batch_normalize\" in block:\n                inputs = BatchNormalization(name='bnorm_' + str(i))(inputs)\n            #if activation == \"leaky\":\n                inputs = LeakyReLU(alpha=0.1, name='leaky_' + str(i))(inputs)\n\n        elif (block[\"type\"] == \"upsample\"):\n            stride = int(block[\"stride\"])\n            inputs = UpSampling2D(stride)(inputs)\n\n        # If it is a route layer\n        elif (block[\"type\"] == \"route\"):\n            block[\"layers\"] = block[\"layers\"].split(',')\n            start = int(block[\"layers\"][0])\n\n            if len(block[\"layers\"]) > 1:\n                end = int(block[\"layers\"][1]) - i\n                filters = output_filters[i + start] + output_filters[end]  # Index negatif :end - index\n                inputs = tf.concat([outputs[i + start], outputs[i + end]], axis=-1)\n            else:\n                filters = output_filters[i + start]\n                inputs = outputs[i + start]\n\n        elif block[\"type\"] == \"shortcut\":\n            from_ = int(block[\"from\"])\n            inputs = outputs[i - 1] + outputs[i + from_]\n\n        # Yolo detection layer\n        elif block[\"type\"] == \"yolo\":\n\n            mask = block[\"mask\"].split(\",\")\n            mask = [int(x) for x in mask]\n            anchors = block[\"anchors\"].split(\",\")\n            anchors = [int(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in mask]\n            n_anchors = len(anchors)\n\n            out_shape = inputs.get_shape().as_list()\n\n            inputs = tf.reshape(inputs, [-1, n_anchors * out_shape[1] * out_shape[2], \\\n\t\t\t\t\t\t\t\t\t\t 5 + num_classes])\n\n            box_centers = inputs[:, :, 0:2]\n            box_shapes = inputs[:, :, 2:4]\n            confidence = inputs[:, :, 4:5]\n            classes = inputs[:, :, 5:num_classes + 5]\n\n            box_centers = tf.sigmoid(box_centers)\n            confidence = tf.sigmoid(confidence)\n            classes = tf.sigmoid(classes)\n\n            anchors = tf.tile(anchors, [out_shape[1] * out_shape[2], 1])\n            box_shapes = tf.exp(box_shapes) * tf.cast(anchors, dtype=tf.float32)\n\n            x = tf.range(out_shape[1], dtype=tf.float32)\n            y = tf.range(out_shape[2], dtype=tf.float32)\n\n            cx, cy = tf.meshgrid(x, y)\n            cx = tf.reshape(cx, (-1, 1))\n            cy = tf.reshape(cy, (-1, 1))\n            cxy = tf.concat([cx, cy], axis=-1)\n            cxy = tf.tile(cxy, [1, n_anchors])\n            cxy = tf.reshape(cxy, [1, -1, 2])\n\n            strides = (input_image.shape[1] \/\/ out_shape[1], \\\n                       input_image.shape[2] \/\/ out_shape[2])\n            box_centers = (box_centers + cxy) * strides\n\n            prediction = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)\n\n            if scale:\n                out_pred = tf.concat([out_pred, prediction], axis=1)\n            else:\n                out_pred = prediction\n                scale = 1\n\n        outputs[i] = inputs\n        output_filters.append(filters)\n\n    model = Model(input_image, out_pred)\n    model.summary()\n    return model<\/pre>\n\n\n\n<p>That&#8217;s it for part 2 and see you in <a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">part 3<\/a>.<\/p>\n\n\n\n<h2>Parts:<\/h2>\n\n\n\n<ul>\n<li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-1\/\">Part-1, An introduction of the YOLOv3 algorithm.<\/a><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part--2\/\">Part-2, Parsing the YOLOv3 configuration file and creating the YOLOv3 network.<\/a><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-3\/\">Part-3, Converting the YOLOv3 pre-trained weights into the TensorFlow 2.0 weights format.<\/a><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/machinelearningspace.com\/yolov3-tensorflow-2-part-4\/\">Part-4, Encoding bounding boxes and testing this implementation with images and videos. <\/a><\/p>\n<\/li>\n<\/ul>\n\n\n\n<p>Credit link:<br><a href=\"https:\/\/blog.paperspace.com\/how-to-implement-a-yolo-object-detector-in-pytorch\/\">https:\/\/blog.paperspace.com\/how-to-implement-a-yolo-object-detector-in-pytorch\/<\/a><br><\/p>\n\n\n\n<p> <\/p>\n","protected":false},"excerpt":{"rendered":"<p>In part 1, we&#8217;ve discussed the YOLOv3 algorithm. Now, it&#8217;s time to dive into the technical details of the Yolov3 implementation in Tensorflow 2.<\/p>\n","protected":false},"author":2,"featured_media":569,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"_mi_skip_tracking":false},"categories":[2,4,6,3],"tags":[59,58,33,29,41,56,94,95],"_links":{"self":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/44"}],"collection":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/comments?post=44"}],"version-history":[{"count":5,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/44\/revisions"}],"predecessor-version":[{"id":2558,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/posts\/44\/revisions\/2558"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media\/569"}],"wp:attachment":[{"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/media?parent=44"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/categories?post=44"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/machinelearningspace.com\/wp-json\/wp\/v2\/tags?post=44"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}